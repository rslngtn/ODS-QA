{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_EiHka9RP6B",
        "colab_type": "code",
        "outputId": "a09b2521-13c2-4595-e8e2-cfae56ef3c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAgFaHDJ-wC7",
        "colab_type": "code",
        "outputId": "d20e7a44-c592-46ea-9d21-7a089da48a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=adf034963f88bf00d84aa154813b345a29c7a2f72a92142dd0847377916919c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 282.1 MB\n",
            "GPU RAM Free: 16270MB | Used: 10MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "554LXdS6zGeF",
        "colab_type": "code",
        "outputId": "7c0e148d-5462-419b-9e0c-aaef0bbd2d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "!unzip drive/My\\ Drive/Colab\\ Notebooks/DeepPavlov_ru-SBERT.zip "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/Colab Notebooks/DeepPavlov_ru-SBERT.zip\n",
            "   creating: DeepPavlov_ru-SBERT/\n",
            "   creating: DeepPavlov_ru-SBERT/data/\n",
            "  inflating: DeepPavlov_ru-SBERT/data/label_weights.pkl  \n",
            "  inflating: DeepPavlov_ru-SBERT/data/train.parquet  \n",
            "  inflating: DeepPavlov_ru-SBERT/data/test.parquet  \n",
            "  inflating: DeepPavlov_ru-SBERT/data/val.parquet  \n",
            "   creating: DeepPavlov_ru-SBERT/source/\n",
            "  inflating: DeepPavlov_ru-SBERT/source/model.py  \n",
            "  inflating: DeepPavlov_ru-SBERT/source/eval.py  \n",
            "  inflating: DeepPavlov_ru-SBERT/source/config.py  \n",
            "  inflating: DeepPavlov_ru-SBERT/source/train.py  \n",
            "  inflating: DeepPavlov_ru-SBERT/run_colab_training.py  \n",
            " extracting: DeepPavlov_ru-SBERT/requirements.in  \n",
            "  inflating: DeepPavlov_ru-SBERT/requirements.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ7_XQqjzXt0",
        "colab_type": "code",
        "outputId": "34f23492-3501-4de8-e29b-4bfea196bf26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r DeepPavlov_ru-SBERT/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 7)) (0.9.0)\n",
            "Collecting boto3==1.13.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/76/b488ad549d3652c299e1a1f2425a0a53c55857d59fefd4598ddcd067886a/boto3-1.13.18-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hCollecting botocore==1.16.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/12/2b993a5a67148454404e4e50238a23df67e28eaa7d9701580b5c9ed6ad1b/botocore-1.16.18-py2.py3-none-any.whl (6.2MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2MB 8.3MB/s \n",
            "\u001b[?25hCollecting cachetools==4.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/59/524ffb454d05001e2be74c14745b485681c6ed5f2e625f71d135704c0909/cachetools-4.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: certifi==2020.4.5.1 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 11)) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 12)) (3.0.4)\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 13)) (7.1.2)\n",
            "Collecting configparser==5.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Collecting docker-pycreds==0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docutils==0.15.2 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 16)) (0.15.2)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 17)) (3.0.12)\n",
            "Collecting future==0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 47.9MB/s \n",
            "\u001b[?25hCollecting gitdb==4.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25hCollecting gitpython==3.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/33/917e6fde1cad13daa7053f39b7c8af3be287314f75f1b1ea8d3fe37a8571/GitPython-3.1.2-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 57.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 21)) (0.4.1)\n",
            "Collecting google-auth==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/a3/2695fdcda305ea85c43ebd2a4d1429f3c7e897c7cf9045a8c378e1115a15/google_auth-1.15.0-py2.py3-none-any.whl (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.7MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Collecting graphql-core==1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio==1.29.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 25)) (1.29.0)\n",
            "Requirement already satisfied: idna==2.9 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 26)) (2.9)\n",
            "Requirement already satisfied: importlib-metadata==1.6.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 27)) (1.6.0)\n",
            "Requirement already satisfied: jmespath==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 28)) (0.10.0)\n",
            "Requirement already satisfied: joblib==0.15.1 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 29)) (0.15.1)\n",
            "Requirement already satisfied: markdown==3.2.2 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 30)) (3.2.2)\n",
            "Collecting numpy==1.18.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl (20.2MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2MB 103kB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3==7.352.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 32)) (7.352.0)\n",
            "Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 33)) (3.1.0)\n",
            "Collecting pathtools==0.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 35)) (2.3)\n",
            "Collecting protobuf==3.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/05/9867ef8eafd12265267bee138fa2c46ebf34a276ea4cbe184cba4c606e8b/protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 48.6MB/s \n",
            "\u001b[?25hCollecting psutil==5.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/b8/3512f0e93e0db23a71d82485ba256071ebef99b227351f0f5540f744af41/psutil-5.7.0.tar.gz (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 38)) (0.2.8)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 39)) (0.4.8)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 40)) (2.8.1)\n",
            "Collecting pytorch-lightning==0.7.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/ab/561d1fa6e5af30b2fd7cb4001f93eb08531e1b72976f13eebf7f7cdc021c/pytorch_lightning-0.7.6-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 51.1MB/s \n",
            "\u001b[?25hCollecting pyyaml==5.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 49.9MB/s \n",
            "\u001b[?25hCollecting regex==2020.5.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/7c/0d46b10a87b3087e8e303fac923beb19ec839d7c5ea34971a12fafb22b52/regex-2020.5.14-cp36-cp36m-manylinux2010_x86_64.whl (675kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 44)) (1.3.0)\n",
            "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 45)) (2.23.0)\n",
            "Requirement already satisfied: rsa==4.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 46)) (4.0)\n",
            "Requirement already satisfied: s3transfer==0.3.3 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 47)) (0.3.3)\n",
            "Collecting sacremoses==0.0.43\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 59.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 51.3MB/s \n",
            "\u001b[?25hCollecting sentry-sdk==0.14.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/95/9a20eebcedab2c1c63fad59fe19a0469edfc2a25b8576497e8084629c2ff/sentry_sdk-0.14.4-py2.py3-none-any.whl (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 59.6MB/s \n",
            "\u001b[?25hCollecting shortuuid==1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting six==1.15.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Collecting smmap==3.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Collecting subprocess32==3.5.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit==1.6.0.post3 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 55)) (1.6.0.post3)\n",
            "Collecting tensorboard==2.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/fd/4f3ca1516cbb3713259ef229abd9314bba0077ef6070285dde0dd1ed21b2/tensorboard-2.2.1-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 47.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 58)) (1.5.0+cu101)\n",
            "Collecting tqdm==4.46.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/40/058b12e8ba10e35f89c9b1fdfc2d4c7f8c05947df2d5eb3c7b258019fda0/tqdm-4.46.0-py2.py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
            "\u001b[?25hCollecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 54.7MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/e5/df302e8017440f111c11cc41a6b432838672f5a70aa29227bf58149dc72f/urllib3-1.25.9-py2.py3-none-any.whl (126kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 63.2MB/s \n",
            "\u001b[?25hCollecting wandb==0.8.36\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/c7/8bf2c62c3f133f45e135a8a116e4e0f162043248e3db54de30996eaf1a8a/wandb-0.8.36-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 55.5MB/s \n",
            "\u001b[?25hCollecting watchdog==0.10.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 64)) (1.0.1)\n",
            "Requirement already satisfied: wheel==0.34.2 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 65)) (0.34.2)\n",
            "Requirement already satisfied: zipp==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r DeepPavlov_ru-SBERT/requirements.txt (line 66)) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth==1.15.0->-r DeepPavlov_ru-SBERT/requirements.txt (line 22)) (47.1.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r DeepPavlov_ru-SBERT/requirements.txt (line 60)) (0.7)\n",
            "Building wheels for collected packages: future, gql, graphql-core, pathtools, psutil, pyyaml, sacremoses, subprocess32, watchdog\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=650a7f8cd6631aa9cbb8c667f0c66eab6f790ce80a2669b2abdf210b6f4b65eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=5e181e45c94aefb728130cd995d1a64342c1f3fedf3134d6f930d7d935d30c4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=774c204c4ba2579794433f05a207d6515ccc5616d288cca3b38a53bbe76ac1cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=13963341c1b42eabc651ff8d750752049b97f85007a2f5f8b37fa8306efe67ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.7.0-cp36-cp36m-linux_x86_64.whl size=272675 sha256=72699a3280f1d941793ff3a87450f1f726a0fec50ae2832dd0f2d4bda8807fd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/69/b4/3200b95828d1f0ddb3cb5699083717f4fdbd9b4223d0644c57\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=3fdcdf5c1fe7d08a3c1346abd12f9e5c453bcfde1323bdc7ce37729fcf64c6d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=b75e98d27d5add8db70c67cdd0bdc54124993e82655a1268b4c7097be8c89027\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=d6dbc8b3edb4292b4d90e89793059bee9b2218d55e1cab5cf2e169b53e1ea20f\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=9291f5cf2db72847407650390f51434dba8cbd18f99b7cf888f10f5110a51f07\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "Successfully built future gql graphql-core pathtools psutil pyyaml sacremoses subprocess32 watchdog\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.7.2, but you'll have google-auth 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3, botocore, boto3, cachetools, configparser, six, docker-pycreds, future, smmap, gitdb, gitpython, google-auth, graphql-core, gql, numpy, pathtools, protobuf, psutil, pyyaml, tensorboard, tqdm, pytorch-lightning, regex, sacremoses, sentencepiece, sentry-sdk, shortuuid, subprocess32, tokenizers, transformers, watchdog, wandb\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: botocore 1.16.23\n",
            "    Uninstalling botocore-1.16.23:\n",
            "      Successfully uninstalled botocore-1.16.23\n",
            "  Found existing installation: boto3 1.13.23\n",
            "    Uninstalling boto3-1.13.23:\n",
            "      Successfully uninstalled boto3-1.13.23\n",
            "  Found existing installation: cachetools 3.1.1\n",
            "    Uninstalling cachetools-3.1.1:\n",
            "      Successfully uninstalled cachetools-3.1.1\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: google-auth 1.7.2\n",
            "    Uninstalling google-auth-1.7.2:\n",
            "      Successfully uninstalled google-auth-1.7.2\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "  Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed boto3-1.13.18 botocore-1.16.18 cachetools-4.1.0 configparser-5.0.0 docker-pycreds-0.4.0 future-0.18.2 gitdb-4.0.5 gitpython-3.1.2 google-auth-1.15.0 gql-0.2.0 graphql-core-1.1 numpy-1.18.4 pathtools-0.1.2 protobuf-3.12.2 psutil-5.7.0 pytorch-lightning-0.7.6 pyyaml-5.3.1 regex-2020.5.14 sacremoses-0.0.43 sentencepiece-0.1.91 sentry-sdk-0.14.4 shortuuid-1.0.1 six-1.15.0 smmap-3.0.4 subprocess32-3.5.4 tensorboard-2.2.1 tokenizers-0.5.2 tqdm-4.46.0 transformers-2.8.0 urllib3-1.25.9 wandb-0.8.36 watchdog-0.10.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cachetools",
                  "google",
                  "numpy",
                  "psutil",
                  "six",
                  "tqdm",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZpPrvZ_z3w7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python DeepPavlov_ru-SBERT/run_colab_training.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LFaP_Ma6AYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r DeepPavlov_ru-SBERT/source .\n",
        "!cp -r DeepPavlov_ru-SBERT/data ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzbOwzX3-dAR",
        "colab_type": "code",
        "outputId": "56a1b6d5-aa34-410f-9faa-ccd2944ce5d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "def memory_cleanup():\n",
        "    \"\"\"\n",
        "    Cleans up GPU memory \n",
        "    https://github.com/huggingface/transformers/issues/1742\n",
        "    \"\"\"\n",
        "    for obj in gc.get_objects():\n",
        "        if torch.is_tensor(obj):\n",
        "            del obj\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "memory_cleanup()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py:102: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
            "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvzVWbWi5a_p",
        "colab_type": "code",
        "outputId": "276884fd-38f6-454f-c004-02ee4af09f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "from source import config, train, eval\n",
        "from source.model import ModelClass, DatasetClass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkbaK3aq5vr_",
        "colab_type": "code",
        "outputId": "d8518d31-5c5b-48c4-d0d7-e89aa73bd5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5a65abc0e3874504af428145b47963c3",
            "36f3d2472a0d4d8681ea23d14a3d7515",
            "5dcafa844227443db88656decab26bce",
            "32d9627354e0496ca24a7e52d83d3ede",
            "4bfc264a66bb4d0e858f32a1545256e8",
            "033563de9068407ca4538fef8e23315a",
            "5d522901073f49838f9314128850f539",
            "c846078b6a964e4cb7c5cfb5d6e52ca7",
            "442785f9d13e48b9b57e49ea78a4b168",
            "f208d43f54fd48cfb1fd4b4907e48e63",
            "2e32d59a7b144c18b82e0c3e76ad62a0",
            "91ce13a7504f4605bd3e26b8a42f9a28",
            "f750af402de74981bf6be47c8f6ff9e3",
            "206c60976dc3468e91a20b9400965894",
            "643fa155a9c74c2c80fc84d060d91b02",
            "83f067e18361484c80e14ece3b7a02b2",
            "57148228d9d346e5b58b043a1219bdf4",
            "2a75a71c32e84333a59f9a1f20bce099",
            "f0813054671f4752a1093ac948950722",
            "74f391a7c9f44ca9ab55455c66bb52dd",
            "0e6d5026403e4c2a98b82831f31ff5e9",
            "40bd8002bc6044909a7a735063154218",
            "f286acfe806b41aca5551d87f02be9f5",
            "33601a45a8ba4025917619ad51e99ff3",
            "60ce3d8580bc4260b8dcff62d1820436",
            "664037ce464f4cc3898c3957db2c3cdc",
            "55957f7adf174ddaa1aff26d8d80f69e",
            "bb985fc58fa04695a8330590ca69fcff",
            "14f1c71998014f80a9b9f223e099d89c",
            "3374b90ab0964148817cfb577f086263",
            "3c9e863b4e3148d1935aecc191d23e29",
            "6147f256b38740f3aff087b1ec32bef0",
            "d3de46a999f2458f85554e28e66d994a",
            "ea3bcf44e2824e299de99da26fcb4bed",
            "1fb07fb45a2c49079fd3be448a172ed5",
            "6ba2ffa17750412c81c01f883469c8d9",
            "368e9cca0e5c410daae755e8f536e7d2",
            "a388f24434444bb89e03fe7cac9014ca",
            "62fa6b6f0fda4695bf373ebd632495b5",
            "5127d4b878954ca7b1435ed1318575c3",
            "d0bb2b0321d74be0add961f514e2d600",
            "7aaa48983292445fac3f7a165faad398",
            "a273640579c84530acd499db36149916",
            "350cafaee4e44f93b2e4e25b1e1e45a6",
            "de9d20ac9c84493db931c6dfffc95227",
            "ed7d2e1c2a6140e69c05fadbe6a0a365",
            "3f2b90dd33464e7e83c830753965692e",
            "26c983211884477584072e28d0feae0b",
            "aaef8d2268ed45b8bc8ed16767756819",
            "0c05455f4e284385b2f2047a8c3dd293",
            "7a96f0b57ab44502a63569b02f60c418",
            "07e9ed013dd3422099d1acb70cf3967a",
            "3e6d6d3e7a2442bcae5c3fba511026a3",
            "23ce2bcd8f4245b5aeb535cb9ced5853",
            "f7012a051ef143d3bdd3bed60901df93",
            "e352ae5cc0e543ccb824a27d053c4498",
            "74ccf3d4c8c843ed86cb134f2375bece",
            "69d8d096e4594a3ba438caa352eadbdc",
            "5ddf9337a8794f6cb7845b707a76e5f5",
            "3f6a82b2de4049c6b63b3d44c7650a01",
            "e4edd9bd234c463b8bcd0c3b7475e4c7",
            "e4dd73c7d148418f8ba58276158a0c8f",
            "5f365cbf159e4e5ea5038eefd99443c0",
            "29ec6f2599bb48ccb36c3e40a9f5ca4c",
            "5bfff87f567846ff804f8181d40c55f5",
            "fc5d6fb32df74e6a886f763c43369c28",
            "88d69c268fcc4deca22c0430fff04531",
            "45f388592b0f4acba9525af889f876f2",
            "7886887deb5a4486acd0a07ffc489768",
            "edbcc7479cbb4794a2b857fa3f2b7011",
            "df05e933b5e748f1b8dbe8066dcc98f3",
            "6e14a9b884c14d7a9f73be6fac342ea9",
            "e96879e04b9540898bfe4b0cd32aca6e",
            "ac29453ebe1d4b1e84bf28c4a4ad3e5d",
            "086c5c69a17943dab5bc85ef964b1cda",
            "1efef3d8d44f4a3d902fc1f228001480",
            "99a62e8d748d48faa3a1ee96c8095762",
            "472eec65e672492492764f61b2b10304",
            "6b8b5354eb274ddaa0d2d7d599043075",
            "dbcdf2b93e984f569713e405e81c3376",
            "a147bc9ed6c5424a87c49f040795e7a5",
            "3eaa92e7d8d044369259472e02a88f74",
            "c0c7b3b5a27542cdacd2e89f707df900",
            "8b0645d6dd254618aa101bffcbab68dd",
            "58cfc7a7e2654052b0dbea4ac1c36db0",
            "9309306fe8954a38859ac4604d721839",
            "7742761c63c84d578cd35e76bde8108d",
            "5cae93abb2e942bb8d4bb15150ca5cb5",
            "dfc8ebd9a37c4bec9304fc2338d670c2",
            "92b9303f96674d3fb0d872a88ffa4b36",
            "567666c030894f39bfe03732446ff9f3",
            "e3e6736818664dee916735a18513446a",
            "a5417c5d8875407281336bbd1c230886",
            "d06f0763948849e880804d53105d1ab8",
            "d0fb4e16339a41ffadabba0b143d10a7",
            "500400ea7cb640b9ba5bbe0e4ec4a84d"
          ]
        }
      },
      "source": [
        "RUN_NAME = 'DeepPavlov_ru-SBERT'\n",
        "with open('data/label_weights.pkl', 'rb') as f:\n",
        "        class_weights = pickle.load(f)\n",
        "config = config.get_config('DeepPavlov_ru-SBERT/', num_classes=len(class_weights), weigths=list(class_weights.values()), batch_size=12)\n",
        "model = train.run_training(ModelClass, config, run_name=RUN_NAME)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a65abc0e3874504af428145b47963c3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=642.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "442785f9d13e48b9b57e49ea78a4b168",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=711456784.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "No environment variable for node rank defined. Set as 0.\n",
            "CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57148228d9d346e5b58b043a1219bdf4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1649718.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60ce3d8580bc4260b8dcff62d1820436",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3de46a999f2458f85554e28e66d994a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=24.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rTokenizing...:   0%|          | 0/215054 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokenizing...: 100%|██████████| 215054/215054 [03:15<00:00, 1099.19it/s]\n",
            "Tokenizing...: 100%|██████████| 71685/71685 [01:10<00:00, 1021.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/sirily/ODS_QA\" target=\"_blank\">https://app.wandb.ai/sirily/ODS_QA</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/sirily/ODS_QA/runs/1pp96apw\" target=\"_blank\">https://app.wandb.ai/sirily/ODS_QA/runs/1pp96apw</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.1 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\n",
            "    | Name                                             | Type              | Params\n",
            "-----------------------------------------------------------------------------------\n",
            "0   | bert                                             | BertModel         | 177 M \n",
            "1   | bert.embeddings                                  | BertEmbeddings    | 92 M  \n",
            "2   | bert.embeddings.word_embeddings                  | Embedding         | 91 M  \n",
            "3   | bert.embeddings.position_embeddings              | Embedding         | 393 K \n",
            "4   | bert.embeddings.token_type_embeddings            | Embedding         | 1 K   \n",
            "5   | bert.embeddings.LayerNorm                        | LayerNorm         | 1 K   \n",
            "6   | bert.embeddings.dropout                          | Dropout           | 0     \n",
            "7   | bert.encoder                                     | BertEncoder       | 85 M  \n",
            "8   | bert.encoder.layer                               | ModuleList        | 85 M  \n",
            "9   | bert.encoder.layer.0                             | BertLayer         | 7 M   \n",
            "10  | bert.encoder.layer.0.attention                   | BertAttention     | 2 M   \n",
            "11  | bert.encoder.layer.0.attention.self              | BertSelfAttention | 1 M   \n",
            "12  | bert.encoder.layer.0.attention.self.query        | Linear            | 590 K \n",
            "13  | bert.encoder.layer.0.attention.self.key          | Linear            | 590 K \n",
            "14  | bert.encoder.layer.0.attention.self.value        | Linear            | 590 K \n",
            "15  | bert.encoder.layer.0.attention.self.dropout      | Dropout           | 0     \n",
            "16  | bert.encoder.layer.0.attention.output            | BertSelfOutput    | 592 K \n",
            "17  | bert.encoder.layer.0.attention.output.dense      | Linear            | 590 K \n",
            "18  | bert.encoder.layer.0.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
            "19  | bert.encoder.layer.0.attention.output.dropout    | Dropout           | 0     \n",
            "20  | bert.encoder.layer.0.intermediate                | BertIntermediate  | 2 M   \n",
            "21  | bert.encoder.layer.0.intermediate.dense          | Linear            | 2 M   \n",
            "22  | bert.encoder.layer.0.output                      | BertOutput        | 2 M   \n",
            "23  | bert.encoder.layer.0.output.dense                | Linear            | 2 M   \n",
            "24  | bert.encoder.layer.0.output.LayerNorm            | LayerNorm         | 1 K   \n",
            "25  | bert.encoder.layer.0.output.dropout              | Dropout           | 0     \n",
            "26  | bert.encoder.layer.1                             | BertLayer         | 7 M   \n",
            "27  | bert.encoder.layer.1.attention                   | BertAttention     | 2 M   \n",
            "28  | bert.encoder.layer.1.attention.self              | BertSelfAttention | 1 M   \n",
            "29  | bert.encoder.layer.1.attention.self.query        | Linear            | 590 K \n",
            "30  | bert.encoder.layer.1.attention.self.key          | Linear            | 590 K \n",
            "31  | bert.encoder.layer.1.attention.self.value        | Linear            | 590 K \n",
            "32  | bert.encoder.layer.1.attention.self.dropout      | Dropout           | 0     \n",
            "33  | bert.encoder.layer.1.attention.output            | BertSelfOutput    | 592 K \n",
            "34  | bert.encoder.layer.1.attention.output.dense      | Linear            | 590 K \n",
            "35  | bert.encoder.layer.1.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
            "36  | bert.encoder.layer.1.attention.output.dropout    | Dropout           | 0     \n",
            "37  | bert.encoder.layer.1.intermediate                | BertIntermediate  | 2 M   \n",
            "38  | bert.encoder.layer.1.intermediate.dense          | Linear            | 2 M   \n",
            "39  | bert.encoder.layer.1.output                      | BertOutput        | 2 M   \n",
            "40  | bert.encoder.layer.1.output.dense                | Linear            | 2 M   \n",
            "41  | bert.encoder.layer.1.output.LayerNorm            | LayerNorm         | 1 K   \n",
            "42  | bert.encoder.layer.1.output.dropout              | Dropout           | 0     \n",
            "43  | bert.encoder.layer.2                             | BertLayer         | 7 M   \n",
            "44  | bert.encoder.layer.2.attention                   | BertAttention     | 2 M   \n",
            "45  | bert.encoder.layer.2.attention.self              | BertSelfAttention | 1 M   \n",
            "46  | bert.encoder.layer.2.attention.self.query        | Linear            | 590 K \n",
            "47  | bert.encoder.layer.2.attention.self.key          | Linear            | 590 K \n",
            "48  | bert.encoder.layer.2.attention.self.value        | Linear            | 590 K \n",
            "49  | bert.encoder.layer.2.attention.self.dropout      | Dropout           | 0     \n",
            "50  | bert.encoder.layer.2.attention.output            | BertSelfOutput    | 592 K \n",
            "51  | bert.encoder.layer.2.attention.output.dense      | Linear            | 590 K \n",
            "52  | bert.encoder.layer.2.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
            "53  | bert.encoder.layer.2.attention.output.dropout    | Dropout           | 0     \n",
            "54  | bert.encoder.layer.2.intermediate                | BertIntermediate  | 2 M   \n",
            "55  | bert.encoder.layer.2.intermediate.dense          | Linear            | 2 M   \n",
            "56  | bert.encoder.layer.2.output                      | BertOutput        | 2 M   \n",
            "57  | bert.encoder.layer.2.output.dense                | Linear            | 2 M   \n",
            "58  | bert.encoder.layer.2.output.LayerNorm            | LayerNorm         | 1 K   \n",
            "59  | bert.encoder.layer.2.output.dropout              | Dropout           | 0     \n",
            "60  | bert.encoder.layer.3                             | BertLayer         | 7 M   \n",
            "61  | bert.encoder.layer.3.attention                   | BertAttention     | 2 M   \n",
            "62  | bert.encoder.layer.3.attention.self              | BertSelfAttention | 1 M   \n",
            "63  | bert.encoder.layer.3.attention.self.query        | Linear            | 590 K \n",
            "64  | bert.encoder.layer.3.attention.self.key          | Linear            | 590 K \n",
            "65  | bert.encoder.layer.3.attention.self.value        | Linear            | 590 K \n",
            "66  | bert.encoder.layer.3.attention.self.dropout      | Dropout           | 0     \n",
            "67  | bert.encoder.layer.3.attention.output            | BertSelfOutput    | 592 K \n",
            "68  | bert.encoder.layer.3.attention.output.dense      | Linear            | 590 K \n",
            "69  | bert.encoder.layer.3.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
            "70  | bert.encoder.layer.3.attention.output.dropout    | Dropout           | 0     \n",
            "71  | bert.encoder.layer.3.intermediate                | BertIntermediate  | 2 M   \n",
            "72  | bert.encoder.layer.3.intermediate.dense          | Linear            | 2 M   \n",
            "73  | bert.encoder.layer.3.output                      | BertOutput        | 2 M   \n",
            "74  | bert.encoder.layer.3.output.dense                | Linear            | 2 M   \n",
            "75  | bert.encoder.layer.3.output.LayerNorm            | LayerNorm         | 1 K   \n",
            "76  | bert.encoder.layer.3.output.dropout              | Dropout           | 0     \n",
            "77  | bert.encoder.layer.4                             | BertLayer         | 7 M   \n",
            "78  | bert.encoder.layer.4.attention                   | BertAttention     | 2 M   \n",
            "79  | bert.encoder.layer.4.attention.self              | BertSelfAttention | 1 M   \n",
            "80  | bert.encoder.layer.4.attention.self.query        | Linear            | 590 K \n",
            "81  | bert.encoder.layer.4.attention.self.key          | Linear            | 590 K \n",
            "82  | bert.encoder.layer.4.attention.self.value        | Linear            | 590 K \n",
            "83  | bert.encoder.layer.4.attention.self.dropout      | Dropout           | 0     \n",
            "84  | bert.encoder.layer.4.attention.output            | BertSelfOutput    | 592 K \n",
            "85  | bert.encoder.layer.4.attention.output.dense      | Linear            | 590 K \n",
            "86  | bert.encoder.layer.4.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
            "87  | bert.encoder.layer.4.attention.output.dropout    | Dropout           | 0     \n",
            "88  | bert.encoder.layer.4.intermediate                | BertIntermediate  | 2 M   \n",
            "89  | bert.encoder.layer.4.intermediate.dense          | Linear            | 2 M   \n",
            "90  | bert.encoder.layer.4.output                      | BertOutput        | 2 M   \n",
            "91  | bert.encoder.layer.4.output.dense                | Linear            | 2 M   \n",
            "92  | bert.encoder.layer.4.output.LayerNorm            | LayerNorm         | 1 K   \n",
            "93  | bert.encoder.layer.4.output.dropout              | Dropout           | 0     \n",
            "94  | bert.encoder.layer.5                             | BertLayer         | 7 M   \n",
            "95  | bert.encoder.layer.5.attention                   | BertAttention     | 2 M   \n",
            "96  | bert.encoder.layer.5.attention.self              | BertSelfAttention | 1 M   \n",
            "97  | bert.encoder.layer.5.attention.self.query        | Linear            | 590 K \n",
            "98  | bert.encoder.layer.5.attention.self.key          | Linear            | 590 K \n",
            "99  | bert.encoder.layer.5.attention.self.value        | Linear            | 590 K \n",
            "100 | bert.encoder.layer.5.attention.self.dropout      | Dropout           | 0     \n",
            "101 | bert.encoder.layer.5.attention.output            | BertSelfOutput    | 592 K \n",
            "102 | bert.encoder.layer.5.attention.output.dense      | Linear            | 590 K \n",
            "103 | bert.encoder.layer.5.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
            "104 | bert.encoder.layer.5.attention.output.dropout    | Dropout           | 0     \n",
            "105 | bert.encoder.layer.5.intermediate                | BertIntermediate  | 2 M   \n",
            "106 | bert.encoder.layer.5.intermediate.dense          | Linear            | 2 M   \n",
            "107 | bert.encoder.layer.5.output                      | BertOutput        | 2 M   \n",
            "108 | bert.encoder.layer.5.output.dense                | Linear            | 2 M   \n",
            "109 | bert.encoder.layer.5.output.LayerNorm            | LayerNorm         | 1 K   \n",
            "110 | bert.encoder.layer.5.output.dropout              | Dropout           | 0     \n",
            "111 | bert.encoder.layer.6                             | BertLayer         | 7 M   \n",
            "112 | bert.encoder.layer.6.attention                   | BertAttention     | 2 M   \n",
            "113 | bert.encoder.layer.6.attention.self              | BertSelfAttention | 1 M   \n",
            "114 | bert.encoder.layer.6.attention.self.query        | Linear            | 590 K \n",
            "115 | bert.encoder.layer.6.attention.self.key          | Linear            | 590 K \n",
            "116 | bert.encoder.layer.6.attention.self.value        | Linear            | 590 K \n",
            "117 | bert.encoder.layer.6.attention.self.dropout      | Dropout           | 0     \n",
            "118 | bert.encoder.layer.6.attention.output            | BertSelfOutput    | 592 K \n",
            "119 | bert.encoder.layer.6.attention.output.dense      | Linear            | 590 K \n",
            "120 | bert.encoder.layer.6.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
            "121 | bert.encoder.layer.6.attention.output.dropout    | Dropout           | 0     \n",
            "122 | bert.encoder.layer.6.intermediate                | BertIntermediate  | 2 M   \n",
            "123 | bert.encoder.layer.6.intermediate.dense          | Linear            | 2 M   \n",
            "124 | bert.encoder.layer.6.output                      | BertOutput        | 2 M   \n",
            "125 | bert.encoder.layer.6.output.dense                | Linear            | 2 M   \n",
            "126 | bert.encoder.layer.6.output.LayerNorm            | LayerNorm         | 1 K   \n",
            "127 | bert.encoder.layer.6.output.dropout              | Dropout           | 0     \n",
            "128 | bert.encoder.layer.7                             | BertLayer         | 7 M   \n",
            "129 | bert.encoder.layer.7.attention                   | BertAttention     | 2 M   \n",
            "130 | bert.encoder.layer.7.attention.self              | BertSelfAttention | 1 M   \n",
            "131 | bert.encoder.layer.7.attention.self.query        | Linear            | 590 K \n",
            "132 | bert.encoder.layer.7.attention.self.key          | Linear            | 590 K \n",
            "133 | bert.encoder.layer.7.attention.self.value        | Linear            | 590 K \n",
            "134 | bert.encoder.layer.7.attention.self.dropout      | Dropout           | 0     \n",
            "135 | bert.encoder.layer.7.attention.output            | BertSelfOutput    | 592 K \n",
            "136 | bert.encoder.layer.7.attention.output.dense      | Linear            | 590 K \n",
            "137 | bert.encoder.layer.7.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
            "138 | bert.encoder.layer.7.attention.output.dropout    | Dropout           | 0     \n",
            "139 | bert.encoder.layer.7.intermediate                | BertIntermediate  | 2 M   \n",
            "140 | bert.encoder.layer.7.intermediate.dense          | Linear            | 2 M   \n",
            "141 | bert.encoder.layer.7.output                      | BertOutput        | 2 M   \n",
            "142 | bert.encoder.layer.7.output.dense                | Linear            | 2 M   \n",
            "143 | bert.encoder.layer.7.output.LayerNorm            | LayerNorm         | 1 K   \n",
            "144 | bert.encoder.layer.7.output.dropout              | Dropout           | 0     \n",
            "145 | bert.encoder.layer.8                             | BertLayer         | 7 M   \n",
            "146 | bert.encoder.layer.8.attention                   | BertAttention     | 2 M   \n",
            "147 | bert.encoder.layer.8.attention.self              | BertSelfAttention | 1 M   \n",
            "148 | bert.encoder.layer.8.attention.self.query        | Linear            | 590 K \n",
            "149 | bert.encoder.layer.8.attention.self.key          | Linear            | 590 K \n",
            "150 | bert.encoder.layer.8.attention.self.value        | Linear            | 590 K \n",
            "151 | bert.encoder.layer.8.attention.self.dropout      | Dropout           | 0     \n",
            "152 | bert.encoder.layer.8.attention.output            | BertSelfOutput    | 592 K \n",
            "153 | bert.encoder.layer.8.attention.output.dense      | Linear            | 590 K \n",
            "154 | bert.encoder.layer.8.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
            "155 | bert.encoder.layer.8.attention.output.dropout    | Dropout           | 0     \n",
            "156 | bert.encoder.layer.8.intermediate                | BertIntermediate  | 2 M   \n",
            "157 | bert.encoder.layer.8.intermediate.dense          | Linear            | 2 M   \n",
            "158 | bert.encoder.layer.8.output                      | BertOutput        | 2 M   \n",
            "159 | bert.encoder.layer.8.output.dense                | Linear            | 2 M   \n",
            "160 | bert.encoder.layer.8.output.LayerNorm            | LayerNorm         | 1 K   \n",
            "161 | bert.encoder.layer.8.output.dropout              | Dropout           | 0     \n",
            "162 | bert.encoder.layer.9                             | BertLayer         | 7 M   \n",
            "163 | bert.encoder.layer.9.attention                   | BertAttention     | 2 M   \n",
            "164 | bert.encoder.layer.9.attention.self              | BertSelfAttention | 1 M   \n",
            "165 | bert.encoder.layer.9.attention.self.query        | Linear            | 590 K \n",
            "166 | bert.encoder.layer.9.attention.self.key          | Linear            | 590 K \n",
            "167 | bert.encoder.layer.9.attention.self.value        | Linear            | 590 K \n",
            "168 | bert.encoder.layer.9.attention.self.dropout      | Dropout           | 0     \n",
            "169 | bert.encoder.layer.9.attention.output            | BertSelfOutput    | 592 K \n",
            "170 | bert.encoder.layer.9.attention.output.dense      | Linear            | 590 K \n",
            "171 | bert.encoder.layer.9.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
            "172 | bert.encoder.layer.9.attention.output.dropout    | Dropout           | 0     \n",
            "173 | bert.encoder.layer.9.intermediate                | BertIntermediate  | 2 M   \n",
            "174 | bert.encoder.layer.9.intermediate.dense          | Linear            | 2 M   \n",
            "175 | bert.encoder.layer.9.output                      | BertOutput        | 2 M   \n",
            "176 | bert.encoder.layer.9.output.dense                | Linear            | 2 M   \n",
            "177 | bert.encoder.layer.9.output.LayerNorm            | LayerNorm         | 1 K   \n",
            "178 | bert.encoder.layer.9.output.dropout              | Dropout           | 0     \n",
            "179 | bert.encoder.layer.10                            | BertLayer         | 7 M   \n",
            "180 | bert.encoder.layer.10.attention                  | BertAttention     | 2 M   \n",
            "181 | bert.encoder.layer.10.attention.self             | BertSelfAttention | 1 M   \n",
            "182 | bert.encoder.layer.10.attention.self.query       | Linear            | 590 K \n",
            "183 | bert.encoder.layer.10.attention.self.key         | Linear            | 590 K \n",
            "184 | bert.encoder.layer.10.attention.self.value       | Linear            | 590 K \n",
            "185 | bert.encoder.layer.10.attention.self.dropout     | Dropout           | 0     \n",
            "186 | bert.encoder.layer.10.attention.output           | BertSelfOutput    | 592 K \n",
            "187 | bert.encoder.layer.10.attention.output.dense     | Linear            | 590 K \n",
            "188 | bert.encoder.layer.10.attention.output.LayerNorm | LayerNorm         | 1 K   \n",
            "189 | bert.encoder.layer.10.attention.output.dropout   | Dropout           | 0     \n",
            "190 | bert.encoder.layer.10.intermediate               | BertIntermediate  | 2 M   \n",
            "191 | bert.encoder.layer.10.intermediate.dense         | Linear            | 2 M   \n",
            "192 | bert.encoder.layer.10.output                     | BertOutput        | 2 M   \n",
            "193 | bert.encoder.layer.10.output.dense               | Linear            | 2 M   \n",
            "194 | bert.encoder.layer.10.output.LayerNorm           | LayerNorm         | 1 K   \n",
            "195 | bert.encoder.layer.10.output.dropout             | Dropout           | 0     \n",
            "196 | bert.encoder.layer.11                            | BertLayer         | 7 M   \n",
            "197 | bert.encoder.layer.11.attention                  | BertAttention     | 2 M   \n",
            "198 | bert.encoder.layer.11.attention.self             | BertSelfAttention | 1 M   \n",
            "199 | bert.encoder.layer.11.attention.self.query       | Linear            | 590 K \n",
            "200 | bert.encoder.layer.11.attention.self.key         | Linear            | 590 K \n",
            "201 | bert.encoder.layer.11.attention.self.value       | Linear            | 590 K \n",
            "202 | bert.encoder.layer.11.attention.self.dropout     | Dropout           | 0     \n",
            "203 | bert.encoder.layer.11.attention.output           | BertSelfOutput    | 592 K \n",
            "204 | bert.encoder.layer.11.attention.output.dense     | Linear            | 590 K \n",
            "205 | bert.encoder.layer.11.attention.output.LayerNorm | LayerNorm         | 1 K   \n",
            "206 | bert.encoder.layer.11.attention.output.dropout   | Dropout           | 0     \n",
            "207 | bert.encoder.layer.11.intermediate               | BertIntermediate  | 2 M   \n",
            "208 | bert.encoder.layer.11.intermediate.dense         | Linear            | 2 M   \n",
            "209 | bert.encoder.layer.11.output                     | BertOutput        | 2 M   \n",
            "210 | bert.encoder.layer.11.output.dense               | Linear            | 2 M   \n",
            "211 | bert.encoder.layer.11.output.LayerNorm           | LayerNorm         | 1 K   \n",
            "212 | bert.encoder.layer.11.output.dropout             | Dropout           | 0     \n",
            "213 | bert.pooler                                      | BertPooler        | 590 K \n",
            "214 | bert.pooler.dense                                | Linear            | 590 K \n",
            "215 | bert.pooler.activation                           | Tanh              | 0     \n",
            "216 | drop                                             | Dropout           | 0     \n",
            "217 | lin                                              | Linear            | 60 K  \n",
            "218 | soft                                             | Softmax           | 0     \n",
            "219 | loss_fn                                          | CrossEntropyLoss  | 0     \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0bb2b0321d74be0add961f514e2d600",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaef8d2268ed45b8bc8ed16767756819",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 0 < 2; dropping {'train_loss': 4.245400428771973, 'epoch': 0}.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74ccf3d4c8c843ed86cb134f2375bece",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 17921 < 23894; dropping {'val_epoch_loss': 0.6231042742729187, 'val_epoch_auc': 0.9873873498300311, 'epoch': 0}.\n",
            "\n",
            "Epoch 00000: val_epoch_auc reached 0.98739 (best 0.98739), saving model to drive/My Drive/Colab Notebooks/checkpoints/DeepPavlov_ru-SBERT_epoch=0-val_epoch_auc=0.99.ckpt as top 1\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 17922 < 23894; dropping {'train_epoch_loss': 1.4838446378707886, 'epoch': 0}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 17922 < 23894; dropping {'train_loss': 0.9266144037246704, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 17932 < 23894; dropping {'train_loss': 0.3804187476634979, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 17942 < 23894; dropping {'train_loss': 0.10336706787347794, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 17952 < 23894; dropping {'train_loss': 0.1610838621854782, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 17962 < 23894; dropping {'train_loss': 0.10364694893360138, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 17972 < 23894; dropping {'train_loss': 0.2925131320953369, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 17982 < 23894; dropping {'train_loss': 0.18105673789978027, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 17992 < 23894; dropping {'train_loss': 0.12012091279029846, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18002 < 23894; dropping {'train_loss': 0.1749366670846939, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18012 < 23894; dropping {'train_loss': 0.19752851128578186, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18022 < 23894; dropping {'train_loss': 0.866290807723999, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18032 < 23894; dropping {'train_loss': 1.6803514957427979, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18042 < 23894; dropping {'train_loss': 0.18931829929351807, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18052 < 23894; dropping {'train_loss': 0.8804559707641602, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18062 < 23894; dropping {'train_loss': 0.2918158769607544, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18072 < 23894; dropping {'train_loss': 0.14094316959381104, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18082 < 23894; dropping {'train_loss': 1.1132323741912842, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18092 < 23894; dropping {'train_loss': 0.09131645411252975, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18102 < 23894; dropping {'train_loss': 1.320546269416809, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18112 < 23894; dropping {'train_loss': 0.07843281328678131, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18122 < 23894; dropping {'train_loss': 0.0743180513381958, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18132 < 23894; dropping {'train_loss': 0.25178349018096924, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18142 < 23894; dropping {'train_loss': 0.2097521424293518, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18152 < 23894; dropping {'train_loss': 0.21140554547309875, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18162 < 23894; dropping {'train_loss': 0.18816255033016205, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18172 < 23894; dropping {'train_loss': 0.13572624325752258, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18182 < 23894; dropping {'train_loss': 0.20253585278987885, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18192 < 23894; dropping {'train_loss': 0.1576739251613617, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18202 < 23894; dropping {'train_loss': 0.1297404021024704, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18212 < 23894; dropping {'train_loss': 0.11066393554210663, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18222 < 23894; dropping {'train_loss': 0.8579531311988831, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18232 < 23894; dropping {'train_loss': 2.024219274520874, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18242 < 23894; dropping {'train_loss': 0.29410320520401, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18252 < 23894; dropping {'train_loss': 0.22997929155826569, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18262 < 23894; dropping {'train_loss': 0.5198277235031128, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18272 < 23894; dropping {'train_loss': 0.3434080183506012, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18282 < 23894; dropping {'train_loss': 0.10501983016729355, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18292 < 23894; dropping {'train_loss': 1.9923713207244873, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18302 < 23894; dropping {'train_loss': 0.2177424132823944, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18312 < 23894; dropping {'train_loss': 0.5270506143569946, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18322 < 23894; dropping {'train_loss': 1.444430947303772, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18332 < 23894; dropping {'train_loss': 0.09558043628931046, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18342 < 23894; dropping {'train_loss': 0.2648050785064697, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18352 < 23894; dropping {'train_loss': 0.321663498878479, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18362 < 23894; dropping {'train_loss': 0.4378177225589752, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18372 < 23894; dropping {'train_loss': 0.22275769710540771, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18382 < 23894; dropping {'train_loss': 1.1744576692581177, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18392 < 23894; dropping {'train_loss': 0.18049490451812744, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18402 < 23894; dropping {'train_loss': 0.11173725873231888, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18412 < 23894; dropping {'train_loss': 0.11870694160461426, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18422 < 23894; dropping {'train_loss': 0.2964951694011688, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18432 < 23894; dropping {'train_loss': 0.5168326497077942, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18442 < 23894; dropping {'train_loss': 0.19414466619491577, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18452 < 23894; dropping {'train_loss': 0.07541462779045105, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18462 < 23894; dropping {'train_loss': 0.7475368976593018, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18472 < 23894; dropping {'train_loss': 0.11403590440750122, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18482 < 23894; dropping {'train_loss': 0.5726551413536072, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18492 < 23894; dropping {'train_loss': 1.8307830095291138, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18502 < 23894; dropping {'train_loss': 0.2034039944410324, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18512 < 23894; dropping {'train_loss': 0.16406740248203278, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18522 < 23894; dropping {'train_loss': 0.3551616966724396, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18532 < 23894; dropping {'train_loss': 0.11211777478456497, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18542 < 23894; dropping {'train_loss': 0.2246953845024109, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18552 < 23894; dropping {'train_loss': 1.3564636707305908, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18562 < 23894; dropping {'train_loss': 0.10713198035955429, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18572 < 23894; dropping {'train_loss': 1.0397109985351562, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18582 < 23894; dropping {'train_loss': 0.23588059842586517, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18592 < 23894; dropping {'train_loss': 1.2592079639434814, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18602 < 23894; dropping {'train_loss': 0.3181382417678833, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18612 < 23894; dropping {'train_loss': 1.1574695110321045, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18622 < 23894; dropping {'train_loss': 1.0519609451293945, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18632 < 23894; dropping {'train_loss': 1.0100263357162476, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18642 < 23894; dropping {'train_loss': 0.1323510855436325, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18652 < 23894; dropping {'train_loss': 0.8944828510284424, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18662 < 23894; dropping {'train_loss': 2.7143149375915527, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18672 < 23894; dropping {'train_loss': 0.7116162180900574, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18682 < 23894; dropping {'train_loss': 0.14501556754112244, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18692 < 23894; dropping {'train_loss': 0.2157684713602066, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18702 < 23894; dropping {'train_loss': 0.14131464064121246, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18712 < 23894; dropping {'train_loss': 0.21277523040771484, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18722 < 23894; dropping {'train_loss': 0.1337616890668869, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18732 < 23894; dropping {'train_loss': 0.1709776073694229, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18742 < 23894; dropping {'train_loss': 0.2723903954029083, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18752 < 23894; dropping {'train_loss': 0.24875254929065704, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18762 < 23894; dropping {'train_loss': 0.0445992574095726, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18772 < 23894; dropping {'train_loss': 0.06013429909944534, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18782 < 23894; dropping {'train_loss': 0.07906269282102585, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18792 < 23894; dropping {'train_loss': 0.1776399314403534, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18802 < 23894; dropping {'train_loss': 0.17013731598854065, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18812 < 23894; dropping {'train_loss': 0.05683388188481331, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18822 < 23894; dropping {'train_loss': 0.21743452548980713, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18832 < 23894; dropping {'train_loss': 0.2618791460990906, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18842 < 23894; dropping {'train_loss': 1.295584797859192, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18852 < 23894; dropping {'train_loss': 1.0035498142242432, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18862 < 23894; dropping {'train_loss': 0.13171052932739258, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18872 < 23894; dropping {'train_loss': 0.38599103689193726, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18882 < 23894; dropping {'train_loss': 1.2152371406555176, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18892 < 23894; dropping {'train_loss': 0.07268726825714111, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18902 < 23894; dropping {'train_loss': 0.5169750452041626, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18912 < 23894; dropping {'train_loss': 0.5908012390136719, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18922 < 23894; dropping {'train_loss': 0.24641360342502594, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18932 < 23894; dropping {'train_loss': 0.3071994483470917, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18942 < 23894; dropping {'train_loss': 0.05712661147117615, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18952 < 23894; dropping {'train_loss': 0.5963443517684937, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18962 < 23894; dropping {'train_loss': 0.2793552279472351, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18972 < 23894; dropping {'train_loss': 0.3284928500652313, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18982 < 23894; dropping {'train_loss': 0.10683965682983398, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 18992 < 23894; dropping {'train_loss': 0.863283097743988, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19002 < 23894; dropping {'train_loss': 0.23714579641819, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19012 < 23894; dropping {'train_loss': 0.2251780480146408, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19022 < 23894; dropping {'train_loss': 0.17709292471408844, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19032 < 23894; dropping {'train_loss': 1.3312110900878906, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19042 < 23894; dropping {'train_loss': 0.13045746088027954, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19052 < 23894; dropping {'train_loss': 0.07838518917560577, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19062 < 23894; dropping {'train_loss': 0.25899165868759155, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19072 < 23894; dropping {'train_loss': 0.31470632553100586, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19082 < 23894; dropping {'train_loss': 2.955183506011963, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19092 < 23894; dropping {'train_loss': 0.2193726897239685, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19102 < 23894; dropping {'train_loss': 0.3441322147846222, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19112 < 23894; dropping {'train_loss': 0.19343659281730652, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19122 < 23894; dropping {'train_loss': 0.4555116295814514, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19132 < 23894; dropping {'train_loss': 0.44456759095191956, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19142 < 23894; dropping {'train_loss': 0.33758464455604553, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19152 < 23894; dropping {'train_loss': 0.4326440691947937, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19162 < 23894; dropping {'train_loss': 0.25291475653648376, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19172 < 23894; dropping {'train_loss': 0.14997658133506775, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19182 < 23894; dropping {'train_loss': 0.1857595145702362, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19192 < 23894; dropping {'train_loss': 0.4927026927471161, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19202 < 23894; dropping {'train_loss': 0.9115523099899292, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19212 < 23894; dropping {'train_loss': 0.3252428472042084, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19222 < 23894; dropping {'train_loss': 0.40580064058303833, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19232 < 23894; dropping {'train_loss': 0.32660892605781555, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19242 < 23894; dropping {'train_loss': 0.18979641795158386, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19252 < 23894; dropping {'train_loss': 0.7496330738067627, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19262 < 23894; dropping {'train_loss': 0.09609904885292053, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19272 < 23894; dropping {'train_loss': 0.1008860394358635, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19282 < 23894; dropping {'train_loss': 0.17020510137081146, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19292 < 23894; dropping {'train_loss': 1.6508177518844604, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19302 < 23894; dropping {'train_loss': 0.2310606688261032, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19312 < 23894; dropping {'train_loss': 0.3006238639354706, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19322 < 23894; dropping {'train_loss': 0.08848868310451508, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19332 < 23894; dropping {'train_loss': 0.7575072050094604, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19342 < 23894; dropping {'train_loss': 0.4733019471168518, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19352 < 23894; dropping {'train_loss': 0.3148590922355652, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19362 < 23894; dropping {'train_loss': 2.054575204849243, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19372 < 23894; dropping {'train_loss': 0.31604427099227905, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19382 < 23894; dropping {'train_loss': 0.5208461284637451, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19392 < 23894; dropping {'train_loss': 0.09006237238645554, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19402 < 23894; dropping {'train_loss': 0.25562039017677307, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19412 < 23894; dropping {'train_loss': 0.8532166481018066, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19422 < 23894; dropping {'train_loss': 0.21148382127285004, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19432 < 23894; dropping {'train_loss': 0.3128821551799774, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19442 < 23894; dropping {'train_loss': 0.3530740737915039, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19452 < 23894; dropping {'train_loss': 0.20679841935634613, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19462 < 23894; dropping {'train_loss': 0.305885374546051, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19472 < 23894; dropping {'train_loss': 0.15282103419303894, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19482 < 23894; dropping {'train_loss': 0.2547352612018585, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19492 < 23894; dropping {'train_loss': 1.362724781036377, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19502 < 23894; dropping {'train_loss': 0.13710299134254456, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19512 < 23894; dropping {'train_loss': 3.7972850799560547, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19522 < 23894; dropping {'train_loss': 0.9812325835227966, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19532 < 23894; dropping {'train_loss': 0.042381979525089264, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19542 < 23894; dropping {'train_loss': 0.22224535048007965, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19552 < 23894; dropping {'train_loss': 0.1727125346660614, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19562 < 23894; dropping {'train_loss': 0.40625905990600586, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19572 < 23894; dropping {'train_loss': 0.1436346024274826, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19582 < 23894; dropping {'train_loss': 0.20080706477165222, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19592 < 23894; dropping {'train_loss': 1.5650557279586792, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19602 < 23894; dropping {'train_loss': 0.5031272172927856, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19612 < 23894; dropping {'train_loss': 0.340762197971344, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19622 < 23894; dropping {'train_loss': 0.0907149389386177, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19632 < 23894; dropping {'train_loss': 0.22272799909114838, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19642 < 23894; dropping {'train_loss': 0.03738344460725784, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19652 < 23894; dropping {'train_loss': 0.1725901961326599, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19662 < 23894; dropping {'train_loss': 0.1276073008775711, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19672 < 23894; dropping {'train_loss': 0.6331285834312439, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19682 < 23894; dropping {'train_loss': 0.18869389593601227, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19692 < 23894; dropping {'train_loss': 0.18544578552246094, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19702 < 23894; dropping {'train_loss': 0.18713797628879547, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19712 < 23894; dropping {'train_loss': 0.19018301367759705, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19722 < 23894; dropping {'train_loss': 0.058285851031541824, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19732 < 23894; dropping {'train_loss': 0.9779438972473145, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19742 < 23894; dropping {'train_loss': 1.1349314451217651, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19752 < 23894; dropping {'train_loss': 0.05260777845978737, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19762 < 23894; dropping {'train_loss': 0.04138178005814552, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19772 < 23894; dropping {'train_loss': 1.1312280893325806, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19782 < 23894; dropping {'train_loss': 0.129201740026474, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19792 < 23894; dropping {'train_loss': 0.08889415860176086, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19802 < 23894; dropping {'train_loss': 0.440227746963501, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19812 < 23894; dropping {'train_loss': 0.37376868724823, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19822 < 23894; dropping {'train_loss': 0.6038805246353149, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19832 < 23894; dropping {'train_loss': 0.2635086476802826, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19842 < 23894; dropping {'train_loss': 0.31676584482192993, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19852 < 23894; dropping {'train_loss': 0.2419540137052536, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19862 < 23894; dropping {'train_loss': 0.3515913486480713, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19872 < 23894; dropping {'train_loss': 2.0689334869384766, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19882 < 23894; dropping {'train_loss': 0.3096383512020111, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19892 < 23894; dropping {'train_loss': 0.03190169855952263, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19902 < 23894; dropping {'train_loss': 0.26487332582473755, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19912 < 23894; dropping {'train_loss': 0.2107706516981125, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19922 < 23894; dropping {'train_loss': 0.20812678337097168, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19932 < 23894; dropping {'train_loss': 0.09815911203622818, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19942 < 23894; dropping {'train_loss': 0.8224999308586121, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19952 < 23894; dropping {'train_loss': 0.5325078964233398, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19962 < 23894; dropping {'train_loss': 0.09325767308473587, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19972 < 23894; dropping {'train_loss': 0.11059878766536713, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19982 < 23894; dropping {'train_loss': 0.07770750671625137, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 19992 < 23894; dropping {'train_loss': 0.8499075174331665, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20002 < 23894; dropping {'train_loss': 0.05637931451201439, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20012 < 23894; dropping {'train_loss': 0.6315210461616516, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20022 < 23894; dropping {'train_loss': 0.556015133857727, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20032 < 23894; dropping {'train_loss': 0.4795454442501068, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20042 < 23894; dropping {'train_loss': 0.8842743635177612, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20052 < 23894; dropping {'train_loss': 0.3340907692909241, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20062 < 23894; dropping {'train_loss': 0.4590795934200287, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20072 < 23894; dropping {'train_loss': 0.036157816648483276, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20082 < 23894; dropping {'train_loss': 1.9353275299072266, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20092 < 23894; dropping {'train_loss': 0.5171405076980591, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20102 < 23894; dropping {'train_loss': 0.3779125213623047, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20112 < 23894; dropping {'train_loss': 1.7368360757827759, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20122 < 23894; dropping {'train_loss': 0.30266720056533813, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20132 < 23894; dropping {'train_loss': 0.2671006917953491, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20142 < 23894; dropping {'train_loss': 0.2453731894493103, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20152 < 23894; dropping {'train_loss': 0.21886245906352997, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20162 < 23894; dropping {'train_loss': 0.33895421028137207, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20172 < 23894; dropping {'train_loss': 4.972450256347656, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20182 < 23894; dropping {'train_loss': 0.8831706047058105, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20192 < 23894; dropping {'train_loss': 0.152929425239563, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20202 < 23894; dropping {'train_loss': 0.08424698561429977, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20212 < 23894; dropping {'train_loss': 0.6319527626037598, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20222 < 23894; dropping {'train_loss': 0.38605430722236633, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20232 < 23894; dropping {'train_loss': 0.16348733007907867, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20242 < 23894; dropping {'train_loss': 0.5834282040596008, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20252 < 23894; dropping {'train_loss': 0.22725486755371094, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20262 < 23894; dropping {'train_loss': 0.7612953782081604, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20272 < 23894; dropping {'train_loss': 0.13859441876411438, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20282 < 23894; dropping {'train_loss': 0.18576884269714355, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20292 < 23894; dropping {'train_loss': 0.04116141051054001, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20302 < 23894; dropping {'train_loss': 0.19999895989894867, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20312 < 23894; dropping {'train_loss': 0.2650463581085205, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20322 < 23894; dropping {'train_loss': 0.6658490896224976, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20332 < 23894; dropping {'train_loss': 0.6969882845878601, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20342 < 23894; dropping {'train_loss': 0.37930023670196533, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20352 < 23894; dropping {'train_loss': 0.5764375925064087, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20362 < 23894; dropping {'train_loss': 0.17525887489318848, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20372 < 23894; dropping {'train_loss': 0.18995113670825958, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20382 < 23894; dropping {'train_loss': 0.14320749044418335, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20392 < 23894; dropping {'train_loss': 1.044481873512268, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20402 < 23894; dropping {'train_loss': 0.19852039217948914, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20412 < 23894; dropping {'train_loss': 0.1863420456647873, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20422 < 23894; dropping {'train_loss': 1.2865254878997803, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20432 < 23894; dropping {'train_loss': 0.08239147067070007, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20442 < 23894; dropping {'train_loss': 0.143353670835495, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20452 < 23894; dropping {'train_loss': 0.47909867763519287, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20462 < 23894; dropping {'train_loss': 0.7854033708572388, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20472 < 23894; dropping {'train_loss': 0.61505526304245, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20482 < 23894; dropping {'train_loss': 1.6223758459091187, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20492 < 23894; dropping {'train_loss': 0.11333926767110825, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20502 < 23894; dropping {'train_loss': 0.16961704194545746, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20512 < 23894; dropping {'train_loss': 0.18029171228408813, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20522 < 23894; dropping {'train_loss': 0.1433214396238327, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20532 < 23894; dropping {'train_loss': 0.2198943942785263, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20542 < 23894; dropping {'train_loss': 0.3174305856227875, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20552 < 23894; dropping {'train_loss': 1.0370079278945923, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20562 < 23894; dropping {'train_loss': 0.15285329520702362, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20572 < 23894; dropping {'train_loss': 0.9236082434654236, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20582 < 23894; dropping {'train_loss': 0.07440055906772614, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20592 < 23894; dropping {'train_loss': 0.23180150985717773, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20602 < 23894; dropping {'train_loss': 0.4541109502315521, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20612 < 23894; dropping {'train_loss': 0.09769371896982193, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20622 < 23894; dropping {'train_loss': 0.4280530512332916, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20632 < 23894; dropping {'train_loss': 0.11834783107042313, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20642 < 23894; dropping {'train_loss': 2.483306646347046, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20652 < 23894; dropping {'train_loss': 0.42499327659606934, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20662 < 23894; dropping {'train_loss': 0.2229941487312317, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20672 < 23894; dropping {'train_loss': 0.959864616394043, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20682 < 23894; dropping {'train_loss': 1.2954411506652832, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20692 < 23894; dropping {'train_loss': 0.22272053360939026, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20702 < 23894; dropping {'train_loss': 0.1682809442281723, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20712 < 23894; dropping {'train_loss': 0.05619417876005173, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20722 < 23894; dropping {'train_loss': 0.13177034258842468, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20732 < 23894; dropping {'train_loss': 0.05713965371251106, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20742 < 23894; dropping {'train_loss': 0.09498865902423859, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20752 < 23894; dropping {'train_loss': 1.4640967845916748, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20762 < 23894; dropping {'train_loss': 0.44931116700172424, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20772 < 23894; dropping {'train_loss': 0.20096097886562347, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20782 < 23894; dropping {'train_loss': 0.19913016259670258, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20792 < 23894; dropping {'train_loss': 0.19535738229751587, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20802 < 23894; dropping {'train_loss': 0.2124670296907425, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20812 < 23894; dropping {'train_loss': 0.17979387938976288, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20822 < 23894; dropping {'train_loss': 0.42468589544296265, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20832 < 23894; dropping {'train_loss': 0.7752225399017334, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20842 < 23894; dropping {'train_loss': 0.26161307096481323, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20852 < 23894; dropping {'train_loss': 0.10786682367324829, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20862 < 23894; dropping {'train_loss': 0.04911819472908974, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20872 < 23894; dropping {'train_loss': 1.2099777460098267, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20882 < 23894; dropping {'train_loss': 0.01683735102415085, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20892 < 23894; dropping {'train_loss': 0.12765741348266602, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20902 < 23894; dropping {'train_loss': 0.17952130734920502, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20912 < 23894; dropping {'train_loss': 1.1429933309555054, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20922 < 23894; dropping {'train_loss': 0.13263298571109772, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20932 < 23894; dropping {'train_loss': 1.5297616720199585, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20942 < 23894; dropping {'train_loss': 0.6610073447227478, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20952 < 23894; dropping {'train_loss': 0.27318501472473145, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20962 < 23894; dropping {'train_loss': 0.1695273518562317, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20972 < 23894; dropping {'train_loss': 0.09712643176317215, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20982 < 23894; dropping {'train_loss': 0.6469610333442688, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 20992 < 23894; dropping {'train_loss': 0.31947553157806396, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21002 < 23894; dropping {'train_loss': 0.15063665807247162, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21012 < 23894; dropping {'train_loss': 0.4509657323360443, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21022 < 23894; dropping {'train_loss': 0.3068305552005768, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21032 < 23894; dropping {'train_loss': 0.5948032736778259, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21042 < 23894; dropping {'train_loss': 1.4395904541015625, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21052 < 23894; dropping {'train_loss': 0.15372762084007263, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21062 < 23894; dropping {'train_loss': 1.9593051671981812, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21072 < 23894; dropping {'train_loss': 0.6653752326965332, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21082 < 23894; dropping {'train_loss': 1.117855191230774, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21092 < 23894; dropping {'train_loss': 0.23432764410972595, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21102 < 23894; dropping {'train_loss': 0.2763899564743042, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21112 < 23894; dropping {'train_loss': 0.09011168032884598, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21122 < 23894; dropping {'train_loss': 0.05397369712591171, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21132 < 23894; dropping {'train_loss': 0.44245922565460205, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21142 < 23894; dropping {'train_loss': 0.4481091797351837, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21152 < 23894; dropping {'train_loss': 0.13504156470298767, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21162 < 23894; dropping {'train_loss': 0.22871220111846924, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21172 < 23894; dropping {'train_loss': 0.1404840499162674, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21182 < 23894; dropping {'train_loss': 2.954308032989502, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21192 < 23894; dropping {'train_loss': 0.2914559543132782, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21202 < 23894; dropping {'train_loss': 0.5461528301239014, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21212 < 23894; dropping {'train_loss': 0.16297586262226105, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21222 < 23894; dropping {'train_loss': 0.26896145939826965, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21232 < 23894; dropping {'train_loss': 1.2520406246185303, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21242 < 23894; dropping {'train_loss': 1.4734585285186768, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21252 < 23894; dropping {'train_loss': 0.5044540166854858, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21262 < 23894; dropping {'train_loss': 0.43169280886650085, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21272 < 23894; dropping {'train_loss': 0.1498929113149643, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21282 < 23894; dropping {'train_loss': 0.08847597986459732, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21292 < 23894; dropping {'train_loss': 0.17049728333950043, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21302 < 23894; dropping {'train_loss': 0.13410481810569763, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21312 < 23894; dropping {'train_loss': 0.5800061225891113, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21322 < 23894; dropping {'train_loss': 0.13748301565647125, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21332 < 23894; dropping {'train_loss': 0.35142701864242554, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21342 < 23894; dropping {'train_loss': 0.4526420533657074, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21352 < 23894; dropping {'train_loss': 2.4514565467834473, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21362 < 23894; dropping {'train_loss': 0.4762515425682068, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21372 < 23894; dropping {'train_loss': 0.1742132306098938, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21382 < 23894; dropping {'train_loss': 0.75677889585495, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21392 < 23894; dropping {'train_loss': 1.1635923385620117, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21402 < 23894; dropping {'train_loss': 0.30580249428749084, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21412 < 23894; dropping {'train_loss': 1.740098476409912, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21422 < 23894; dropping {'train_loss': 1.6011602878570557, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21432 < 23894; dropping {'train_loss': 0.2387089729309082, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21442 < 23894; dropping {'train_loss': 0.28148114681243896, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21452 < 23894; dropping {'train_loss': 0.05183153972029686, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21462 < 23894; dropping {'train_loss': 0.2006225883960724, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21472 < 23894; dropping {'train_loss': 0.1594783514738083, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21482 < 23894; dropping {'train_loss': 0.11908157169818878, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21492 < 23894; dropping {'train_loss': 0.31636491417884827, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21502 < 23894; dropping {'train_loss': 0.17199508845806122, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21512 < 23894; dropping {'train_loss': 0.14215141534805298, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21522 < 23894; dropping {'train_loss': 0.395460307598114, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21532 < 23894; dropping {'train_loss': 1.1147361993789673, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21542 < 23894; dropping {'train_loss': 0.4931830167770386, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21552 < 23894; dropping {'train_loss': 0.2125125676393509, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21562 < 23894; dropping {'train_loss': 0.2713099420070648, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21572 < 23894; dropping {'train_loss': 0.5964954495429993, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21582 < 23894; dropping {'train_loss': 0.3583531975746155, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21592 < 23894; dropping {'train_loss': 0.1447805017232895, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21602 < 23894; dropping {'train_loss': 3.7931642532348633, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21612 < 23894; dropping {'train_loss': 0.45820167660713196, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21622 < 23894; dropping {'train_loss': 0.03706996887922287, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21632 < 23894; dropping {'train_loss': 0.18464219570159912, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21642 < 23894; dropping {'train_loss': 1.27181077003479, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21652 < 23894; dropping {'train_loss': 0.19751140475273132, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21662 < 23894; dropping {'train_loss': 0.16672511398792267, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21672 < 23894; dropping {'train_loss': 0.2459695190191269, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21682 < 23894; dropping {'train_loss': 0.7363786697387695, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21692 < 23894; dropping {'train_loss': 0.5637361407279968, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21702 < 23894; dropping {'train_loss': 0.24203599989414215, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21712 < 23894; dropping {'train_loss': 0.7209897637367249, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21722 < 23894; dropping {'train_loss': 0.05313916504383087, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21732 < 23894; dropping {'train_loss': 5.2949113845825195, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21742 < 23894; dropping {'train_loss': 0.275784432888031, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21752 < 23894; dropping {'train_loss': 0.09354544430971146, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21762 < 23894; dropping {'train_loss': 0.1841825246810913, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21772 < 23894; dropping {'train_loss': 1.389626383781433, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21782 < 23894; dropping {'train_loss': 0.8490551710128784, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21792 < 23894; dropping {'train_loss': 0.2683278024196625, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21802 < 23894; dropping {'train_loss': 0.3095291554927826, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21812 < 23894; dropping {'train_loss': 0.775251030921936, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21822 < 23894; dropping {'train_loss': 0.9930776357650757, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21832 < 23894; dropping {'train_loss': 0.5569705367088318, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21842 < 23894; dropping {'train_loss': 0.27536070346832275, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21852 < 23894; dropping {'train_loss': 0.8156918287277222, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21862 < 23894; dropping {'train_loss': 4.756795883178711, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21872 < 23894; dropping {'train_loss': 0.04381375387310982, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21882 < 23894; dropping {'train_loss': 2.2280473709106445, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21892 < 23894; dropping {'train_loss': 0.939955472946167, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21902 < 23894; dropping {'train_loss': 0.08009102195501328, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21912 < 23894; dropping {'train_loss': 0.39986830949783325, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21922 < 23894; dropping {'train_loss': 0.06836985796689987, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21932 < 23894; dropping {'train_loss': 0.41702625155448914, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21942 < 23894; dropping {'train_loss': 0.1365898847579956, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21952 < 23894; dropping {'train_loss': 0.30224913358688354, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21962 < 23894; dropping {'train_loss': 0.21708358824253082, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21972 < 23894; dropping {'train_loss': 0.16865329444408417, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21982 < 23894; dropping {'train_loss': 0.24947413802146912, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 21992 < 23894; dropping {'train_loss': 0.1690281182527542, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22002 < 23894; dropping {'train_loss': 0.11687711626291275, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22012 < 23894; dropping {'train_loss': 0.2857647240161896, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22022 < 23894; dropping {'train_loss': 2.41068959236145, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22032 < 23894; dropping {'train_loss': 0.1088780090212822, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22042 < 23894; dropping {'train_loss': 3.5078930854797363, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22052 < 23894; dropping {'train_loss': 0.03074975125491619, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22062 < 23894; dropping {'train_loss': 0.27058613300323486, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22072 < 23894; dropping {'train_loss': 0.4818163812160492, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22082 < 23894; dropping {'train_loss': 0.08287347108125687, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22092 < 23894; dropping {'train_loss': 1.708880066871643, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22102 < 23894; dropping {'train_loss': 0.414417028427124, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22112 < 23894; dropping {'train_loss': 0.016758479177951813, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22122 < 23894; dropping {'train_loss': 0.3308977484703064, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22132 < 23894; dropping {'train_loss': 0.25262945890426636, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22142 < 23894; dropping {'train_loss': 0.2508910894393921, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22152 < 23894; dropping {'train_loss': 0.35840871930122375, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22162 < 23894; dropping {'train_loss': 0.6004709601402283, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22172 < 23894; dropping {'train_loss': 0.4003187119960785, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22182 < 23894; dropping {'train_loss': 0.2332724630832672, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22192 < 23894; dropping {'train_loss': 0.052048761397600174, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22202 < 23894; dropping {'train_loss': 0.9936134815216064, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22212 < 23894; dropping {'train_loss': 1.87494695186615, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22222 < 23894; dropping {'train_loss': 0.5132318139076233, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22232 < 23894; dropping {'train_loss': 4.9441938400268555, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22242 < 23894; dropping {'train_loss': 0.29352089762687683, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22252 < 23894; dropping {'train_loss': 0.35231444239616394, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22262 < 23894; dropping {'train_loss': 0.2192074954509735, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22272 < 23894; dropping {'train_loss': 3.0148675441741943, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22282 < 23894; dropping {'train_loss': 0.6754381060600281, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22292 < 23894; dropping {'train_loss': 0.2927441895008087, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22302 < 23894; dropping {'train_loss': 0.42212051153182983, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22312 < 23894; dropping {'train_loss': 0.20668983459472656, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22322 < 23894; dropping {'train_loss': 0.17690381407737732, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22332 < 23894; dropping {'train_loss': 0.19308221340179443, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22342 < 23894; dropping {'train_loss': 0.4680628478527069, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22352 < 23894; dropping {'train_loss': 0.17473001778125763, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22362 < 23894; dropping {'train_loss': 0.23295623064041138, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22372 < 23894; dropping {'train_loss': 0.37165123224258423, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22382 < 23894; dropping {'train_loss': 0.09786296635866165, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22392 < 23894; dropping {'train_loss': 0.3094773590564728, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22402 < 23894; dropping {'train_loss': 0.5492289662361145, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22412 < 23894; dropping {'train_loss': 0.11353012919425964, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22422 < 23894; dropping {'train_loss': 1.1403038501739502, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22432 < 23894; dropping {'train_loss': 0.06426825374364853, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22442 < 23894; dropping {'train_loss': 0.2919039726257324, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22452 < 23894; dropping {'train_loss': 0.6518579721450806, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22462 < 23894; dropping {'train_loss': 1.1557599306106567, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22472 < 23894; dropping {'train_loss': 0.0792364627122879, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22482 < 23894; dropping {'train_loss': 0.05389408394694328, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22492 < 23894; dropping {'train_loss': 0.04708867892622948, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22502 < 23894; dropping {'train_loss': 0.7117136120796204, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22512 < 23894; dropping {'train_loss': 0.31621450185775757, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22522 < 23894; dropping {'train_loss': 0.11285486817359924, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22532 < 23894; dropping {'train_loss': 0.4403463304042816, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22542 < 23894; dropping {'train_loss': 0.2574785351753235, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22552 < 23894; dropping {'train_loss': 0.04585973918437958, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22562 < 23894; dropping {'train_loss': 0.36327773332595825, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22572 < 23894; dropping {'train_loss': 0.4215088188648224, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22582 < 23894; dropping {'train_loss': 0.04489881172776222, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22592 < 23894; dropping {'train_loss': 0.7122229337692261, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22602 < 23894; dropping {'train_loss': 0.15480124950408936, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22612 < 23894; dropping {'train_loss': 0.03976713493466377, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22622 < 23894; dropping {'train_loss': 0.027286279946565628, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22632 < 23894; dropping {'train_loss': 0.2631993591785431, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22642 < 23894; dropping {'train_loss': 0.10170914977788925, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22652 < 23894; dropping {'train_loss': 0.15449240803718567, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22662 < 23894; dropping {'train_loss': 0.9713857769966125, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22672 < 23894; dropping {'train_loss': 0.27073487639427185, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22682 < 23894; dropping {'train_loss': 0.2283662110567093, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22692 < 23894; dropping {'train_loss': 0.10494944453239441, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22702 < 23894; dropping {'train_loss': 0.11364822089672089, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22712 < 23894; dropping {'train_loss': 0.20395198464393616, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22722 < 23894; dropping {'train_loss': 0.47429370880126953, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22732 < 23894; dropping {'train_loss': 0.4130544364452362, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22742 < 23894; dropping {'train_loss': 0.26901331543922424, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22752 < 23894; dropping {'train_loss': 0.25076302886009216, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22762 < 23894; dropping {'train_loss': 0.12976020574569702, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22772 < 23894; dropping {'train_loss': 0.08856850117444992, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22782 < 23894; dropping {'train_loss': 0.13848130404949188, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22792 < 23894; dropping {'train_loss': 0.32925787568092346, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22802 < 23894; dropping {'train_loss': 1.041748285293579, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22812 < 23894; dropping {'train_loss': 0.7042368054389954, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22822 < 23894; dropping {'train_loss': 0.7008038759231567, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22832 < 23894; dropping {'train_loss': 0.31067365407943726, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22842 < 23894; dropping {'train_loss': 0.4164753258228302, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22852 < 23894; dropping {'train_loss': 0.14399035274982452, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22862 < 23894; dropping {'train_loss': 0.13879872858524323, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22872 < 23894; dropping {'train_loss': 0.138840451836586, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22882 < 23894; dropping {'train_loss': 0.10629423707723618, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22892 < 23894; dropping {'train_loss': 1.6517095565795898, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22902 < 23894; dropping {'train_loss': 3.7780301570892334, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22912 < 23894; dropping {'train_loss': 0.1211194321513176, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22922 < 23894; dropping {'train_loss': 0.6658771634101868, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22932 < 23894; dropping {'train_loss': 1.1989364624023438, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22942 < 23894; dropping {'train_loss': 0.2905725836753845, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22952 < 23894; dropping {'train_loss': 0.06264012306928635, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22962 < 23894; dropping {'train_loss': 1.6981574296951294, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22972 < 23894; dropping {'train_loss': 1.4164704084396362, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22982 < 23894; dropping {'train_loss': 0.2579677104949951, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 22992 < 23894; dropping {'train_loss': 3.204007387161255, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23002 < 23894; dropping {'train_loss': 0.5390603542327881, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23012 < 23894; dropping {'train_loss': 0.13541093468666077, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23022 < 23894; dropping {'train_loss': 0.12945689260959625, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23032 < 23894; dropping {'train_loss': 0.3395419716835022, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23042 < 23894; dropping {'train_loss': 1.302224040031433, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23052 < 23894; dropping {'train_loss': 0.7486458420753479, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23062 < 23894; dropping {'train_loss': 0.07602453976869583, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23072 < 23894; dropping {'train_loss': 0.10312876850366592, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23082 < 23894; dropping {'train_loss': 0.8703740239143372, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23092 < 23894; dropping {'train_loss': 0.10477373749017715, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23102 < 23894; dropping {'train_loss': 0.17827358841896057, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23112 < 23894; dropping {'train_loss': 0.023883670568466187, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23122 < 23894; dropping {'train_loss': 0.1372680813074112, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23132 < 23894; dropping {'train_loss': 0.5567826628684998, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23142 < 23894; dropping {'train_loss': 0.2771366238594055, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23152 < 23894; dropping {'train_loss': 0.2807159423828125, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23162 < 23894; dropping {'train_loss': 0.13175250589847565, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23172 < 23894; dropping {'train_loss': 0.6499202251434326, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23182 < 23894; dropping {'train_loss': 0.17504866421222687, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23192 < 23894; dropping {'train_loss': 0.1406845599412918, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23202 < 23894; dropping {'train_loss': 0.14155180752277374, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23212 < 23894; dropping {'train_loss': 0.445019394159317, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23222 < 23894; dropping {'train_loss': 0.07405364513397217, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23232 < 23894; dropping {'train_loss': 0.13793030381202698, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23242 < 23894; dropping {'train_loss': 0.6011733412742615, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23252 < 23894; dropping {'train_loss': 0.03320840746164322, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23262 < 23894; dropping {'train_loss': 0.9416830539703369, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23272 < 23894; dropping {'train_loss': 0.1150302067399025, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23282 < 23894; dropping {'train_loss': 0.10841184854507446, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23292 < 23894; dropping {'train_loss': 0.1627151221036911, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23302 < 23894; dropping {'train_loss': 0.05335789918899536, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23312 < 23894; dropping {'train_loss': 0.2646467089653015, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23322 < 23894; dropping {'train_loss': 0.31705689430236816, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23332 < 23894; dropping {'train_loss': 1.0792375802993774, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23342 < 23894; dropping {'train_loss': 0.9058247804641724, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23352 < 23894; dropping {'train_loss': 0.8500804901123047, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23362 < 23894; dropping {'train_loss': 0.17430609464645386, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23372 < 23894; dropping {'train_loss': 1.3874971866607666, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23382 < 23894; dropping {'train_loss': 2.515061616897583, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23392 < 23894; dropping {'train_loss': 2.0866780281066895, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23402 < 23894; dropping {'train_loss': 0.409890353679657, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23412 < 23894; dropping {'train_loss': 0.47260773181915283, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23422 < 23894; dropping {'train_loss': 0.10029295831918716, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23432 < 23894; dropping {'train_loss': 0.1212390661239624, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23442 < 23894; dropping {'train_loss': 0.14424680173397064, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23452 < 23894; dropping {'train_loss': 0.5494504570960999, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23462 < 23894; dropping {'train_loss': 0.0687757059931755, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23472 < 23894; dropping {'train_loss': 0.052089061588048935, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23482 < 23894; dropping {'train_loss': 0.23931962251663208, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23492 < 23894; dropping {'train_loss': 1.1108300685882568, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23502 < 23894; dropping {'train_loss': 0.30742618441581726, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23512 < 23894; dropping {'train_loss': 1.1155662536621094, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23522 < 23894; dropping {'train_loss': 2.2484729290008545, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23532 < 23894; dropping {'train_loss': 0.3167138993740082, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23542 < 23894; dropping {'train_loss': 0.4609833061695099, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23552 < 23894; dropping {'train_loss': 0.27937570214271545, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23562 < 23894; dropping {'train_loss': 0.06973486393690109, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23572 < 23894; dropping {'train_loss': 0.7930396199226379, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23582 < 23894; dropping {'train_loss': 0.1610412895679474, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23592 < 23894; dropping {'train_loss': 0.6274670362472534, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23602 < 23894; dropping {'train_loss': 0.0586470328271389, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23612 < 23894; dropping {'train_loss': 0.13136790692806244, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23622 < 23894; dropping {'train_loss': 0.44317516684532166, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23632 < 23894; dropping {'train_loss': 0.46562692523002625, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23642 < 23894; dropping {'train_loss': 0.06586715579032898, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23652 < 23894; dropping {'train_loss': 0.9121343493461609, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23662 < 23894; dropping {'train_loss': 0.6677951216697693, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23672 < 23894; dropping {'train_loss': 0.19888736307621002, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23682 < 23894; dropping {'train_loss': 1.2095495462417603, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23692 < 23894; dropping {'train_loss': 0.2967137396335602, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23702 < 23894; dropping {'train_loss': 1.4106110334396362, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23712 < 23894; dropping {'train_loss': 0.1685323417186737, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23722 < 23894; dropping {'train_loss': 1.2347087860107422, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23732 < 23894; dropping {'train_loss': 0.11625885963439941, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23742 < 23894; dropping {'train_loss': 0.08245211094617844, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23752 < 23894; dropping {'train_loss': 0.02730834111571312, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23762 < 23894; dropping {'train_loss': 0.4698491394519806, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23772 < 23894; dropping {'train_loss': 0.1679326444864273, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23782 < 23894; dropping {'train_loss': 0.18483905494213104, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23792 < 23894; dropping {'train_loss': 0.06317711621522903, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23802 < 23894; dropping {'train_loss': 0.1533961147069931, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23812 < 23894; dropping {'train_loss': 0.4052852988243103, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23822 < 23894; dropping {'train_loss': 0.11676356196403503, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23832 < 23894; dropping {'train_loss': 0.29620206356048584, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23842 < 23894; dropping {'train_loss': 0.06701447814702988, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23852 < 23894; dropping {'train_loss': 0.5354852676391602, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23862 < 23894; dropping {'train_loss': 0.390647292137146, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23872 < 23894; dropping {'train_loss': 0.21681255102157593, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23882 < 23894; dropping {'train_loss': 0.0213317908346653, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 23892 < 23894; dropping {'train_loss': 0.21028472483158112, 'epoch': 1}.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bfff87f567846ff804f8181d40c55f5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35843 < 41816; dropping {'val_epoch_loss': 0.4251424968242645, 'val_epoch_auc': 0.9910331201465145, 'epoch': 1}.\n",
            "\n",
            "Epoch 00001: val_epoch_auc reached 0.99103 (best 0.99103), saving model to drive/My Drive/Colab Notebooks/checkpoints/DeepPavlov_ru-SBERT_epoch=1-val_epoch_auc=0.99.ckpt as top 1\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35844 < 41816; dropping {'train_epoch_loss': 0.42969146370887756, 'epoch': 1}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35844 < 41816; dropping {'train_loss': 0.03629476949572563, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35854 < 41816; dropping {'train_loss': 0.11126705259084702, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35864 < 41816; dropping {'train_loss': 0.2484203428030014, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35874 < 41816; dropping {'train_loss': 0.1335466504096985, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35884 < 41816; dropping {'train_loss': 0.11639285087585449, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35894 < 41816; dropping {'train_loss': 0.34347525238990784, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35904 < 41816; dropping {'train_loss': 0.02523997239768505, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35914 < 41816; dropping {'train_loss': 0.22567766904830933, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35924 < 41816; dropping {'train_loss': 0.18452145159244537, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35934 < 41816; dropping {'train_loss': 0.008631917648017406, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35944 < 41816; dropping {'train_loss': 0.005422449670732021, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35954 < 41816; dropping {'train_loss': 1.0776325464248657, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35964 < 41816; dropping {'train_loss': 0.01350420992821455, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35974 < 41816; dropping {'train_loss': 0.0687798410654068, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35984 < 41816; dropping {'train_loss': 0.04897278919816017, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 35994 < 41816; dropping {'train_loss': 0.1707916110754013, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36004 < 41816; dropping {'train_loss': 0.04079895839095116, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36014 < 41816; dropping {'train_loss': 0.8022028803825378, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36024 < 41816; dropping {'train_loss': 0.05487999692559242, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36034 < 41816; dropping {'train_loss': 0.062256794422864914, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36044 < 41816; dropping {'train_loss': 0.0998406782746315, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36054 < 41816; dropping {'train_loss': 0.1858011931180954, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36064 < 41816; dropping {'train_loss': 0.0295342318713665, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36074 < 41816; dropping {'train_loss': 0.04626830667257309, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36084 < 41816; dropping {'train_loss': 0.030129844322800636, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36094 < 41816; dropping {'train_loss': 0.14857988059520721, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36104 < 41816; dropping {'train_loss': 0.017518073320388794, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36114 < 41816; dropping {'train_loss': 0.40899723768234253, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36124 < 41816; dropping {'train_loss': 0.034142062067985535, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36134 < 41816; dropping {'train_loss': 0.006950105540454388, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36144 < 41816; dropping {'train_loss': 0.2560606002807617, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36154 < 41816; dropping {'train_loss': 0.3177935779094696, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36164 < 41816; dropping {'train_loss': 0.0611325241625309, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36174 < 41816; dropping {'train_loss': 0.08236121386289597, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36184 < 41816; dropping {'train_loss': 0.09939524531364441, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36194 < 41816; dropping {'train_loss': 1.0303256511688232, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36204 < 41816; dropping {'train_loss': 0.40252330899238586, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36214 < 41816; dropping {'train_loss': 0.06350164115428925, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36224 < 41816; dropping {'train_loss': 0.13129937648773193, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36234 < 41816; dropping {'train_loss': 2.2030489444732666, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36244 < 41816; dropping {'train_loss': 0.31223875284194946, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36254 < 41816; dropping {'train_loss': 0.08197876065969467, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36264 < 41816; dropping {'train_loss': 0.06690265983343124, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36274 < 41816; dropping {'train_loss': 0.5438441038131714, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36284 < 41816; dropping {'train_loss': 0.012571222148835659, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36294 < 41816; dropping {'train_loss': 0.05424342676997185, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36304 < 41816; dropping {'train_loss': 0.038841694593429565, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36314 < 41816; dropping {'train_loss': 0.15925398468971252, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36324 < 41816; dropping {'train_loss': 0.0065706027671694756, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36334 < 41816; dropping {'train_loss': 0.03099772520363331, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36344 < 41816; dropping {'train_loss': 0.19729270040988922, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36354 < 41816; dropping {'train_loss': 0.2593162953853607, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36364 < 41816; dropping {'train_loss': 3.414717674255371, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36374 < 41816; dropping {'train_loss': 0.04760068282485008, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36384 < 41816; dropping {'train_loss': 0.04274888336658478, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36394 < 41816; dropping {'train_loss': 0.09755659103393555, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36404 < 41816; dropping {'train_loss': 1.5447808504104614, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36414 < 41816; dropping {'train_loss': 0.06324779987335205, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36424 < 41816; dropping {'train_loss': 0.25558221340179443, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36434 < 41816; dropping {'train_loss': 1.0094149112701416, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36444 < 41816; dropping {'train_loss': 0.1236543357372284, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36454 < 41816; dropping {'train_loss': 0.15855640172958374, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36464 < 41816; dropping {'train_loss': 0.03112982027232647, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36474 < 41816; dropping {'train_loss': 0.05737827345728874, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36484 < 41816; dropping {'train_loss': 0.10918740928173065, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36494 < 41816; dropping {'train_loss': 0.1644544154405594, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36504 < 41816; dropping {'train_loss': 0.0671222060918808, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36514 < 41816; dropping {'train_loss': 0.18085315823554993, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36524 < 41816; dropping {'train_loss': 0.21654124557971954, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36534 < 41816; dropping {'train_loss': 0.18785172700881958, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36544 < 41816; dropping {'train_loss': 0.026849966496229172, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36554 < 41816; dropping {'train_loss': 0.2293907105922699, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36564 < 41816; dropping {'train_loss': 0.061449941247701645, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36574 < 41816; dropping {'train_loss': 1.8676140308380127, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36584 < 41816; dropping {'train_loss': 0.039103709161281586, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36594 < 41816; dropping {'train_loss': 1.5042632818222046, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36604 < 41816; dropping {'train_loss': 0.03691992908716202, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36614 < 41816; dropping {'train_loss': 0.12029096484184265, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36624 < 41816; dropping {'train_loss': 0.011437814682722092, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36634 < 41816; dropping {'train_loss': 0.04544561356306076, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36644 < 41816; dropping {'train_loss': 0.11775022745132446, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36654 < 41816; dropping {'train_loss': 0.1758088320493698, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36664 < 41816; dropping {'train_loss': 0.06545060127973557, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36674 < 41816; dropping {'train_loss': 0.12310803681612015, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36684 < 41816; dropping {'train_loss': 0.03964349627494812, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36694 < 41816; dropping {'train_loss': 0.14355780184268951, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36704 < 41816; dropping {'train_loss': 0.03909938409924507, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36714 < 41816; dropping {'train_loss': 0.36856597661972046, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36724 < 41816; dropping {'train_loss': 0.09693262726068497, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36734 < 41816; dropping {'train_loss': 0.045055635273456573, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36744 < 41816; dropping {'train_loss': 0.10431835800409317, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36754 < 41816; dropping {'train_loss': 0.06006518378853798, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36764 < 41816; dropping {'train_loss': 0.01938796229660511, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36774 < 41816; dropping {'train_loss': 0.032320570200681686, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36784 < 41816; dropping {'train_loss': 0.02306041494011879, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36794 < 41816; dropping {'train_loss': 0.026103343814611435, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36804 < 41816; dropping {'train_loss': 0.022024380043148994, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36814 < 41816; dropping {'train_loss': 0.015500258654356003, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36824 < 41816; dropping {'train_loss': 0.006175727117806673, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36834 < 41816; dropping {'train_loss': 0.17310218513011932, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36844 < 41816; dropping {'train_loss': 0.15528443455696106, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36854 < 41816; dropping {'train_loss': 0.07410552352666855, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36864 < 41816; dropping {'train_loss': 0.00728933559730649, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36874 < 41816; dropping {'train_loss': 0.011147493496537209, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36884 < 41816; dropping {'train_loss': 0.005017190705984831, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36894 < 41816; dropping {'train_loss': 0.07727568596601486, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36904 < 41816; dropping {'train_loss': 0.024309920147061348, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36914 < 41816; dropping {'train_loss': 0.09601113945245743, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36924 < 41816; dropping {'train_loss': 0.09242480993270874, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36934 < 41816; dropping {'train_loss': 0.12490949779748917, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36944 < 41816; dropping {'train_loss': 0.05928825959563255, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36954 < 41816; dropping {'train_loss': 0.026522036641836166, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36964 < 41816; dropping {'train_loss': 0.06929819285869598, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36974 < 41816; dropping {'train_loss': 0.03073284961283207, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36984 < 41816; dropping {'train_loss': 0.02149912901222706, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 36994 < 41816; dropping {'train_loss': 0.9768193364143372, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37004 < 41816; dropping {'train_loss': 0.02574930340051651, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37014 < 41816; dropping {'train_loss': 0.0791153535246849, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37024 < 41816; dropping {'train_loss': 0.019161362200975418, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37034 < 41816; dropping {'train_loss': 0.2769545912742615, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37044 < 41816; dropping {'train_loss': 0.034760963171720505, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37054 < 41816; dropping {'train_loss': 0.05477325990796089, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37064 < 41816; dropping {'train_loss': 0.03920847550034523, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37074 < 41816; dropping {'train_loss': 0.027972187846899033, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37084 < 41816; dropping {'train_loss': 0.013405317440629005, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37094 < 41816; dropping {'train_loss': 0.2604033350944519, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37104 < 41816; dropping {'train_loss': 0.021572206169366837, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37114 < 41816; dropping {'train_loss': 0.47400233149528503, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37124 < 41816; dropping {'train_loss': 0.39184731245040894, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37134 < 41816; dropping {'train_loss': 0.18243694305419922, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37144 < 41816; dropping {'train_loss': 0.28726401925086975, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37154 < 41816; dropping {'train_loss': 0.06464637070894241, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37164 < 41816; dropping {'train_loss': 0.0561690516769886, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37174 < 41816; dropping {'train_loss': 0.04445621371269226, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37184 < 41816; dropping {'train_loss': 0.0574444942176342, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37194 < 41816; dropping {'train_loss': 0.06439913064241409, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37204 < 41816; dropping {'train_loss': 1.328677773475647, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37214 < 41816; dropping {'train_loss': 0.014964303001761436, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37224 < 41816; dropping {'train_loss': 0.04427316412329674, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37234 < 41816; dropping {'train_loss': 0.008606912568211555, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37244 < 41816; dropping {'train_loss': 0.13525626063346863, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37254 < 41816; dropping {'train_loss': 0.6366090178489685, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37264 < 41816; dropping {'train_loss': 0.04429584741592407, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37274 < 41816; dropping {'train_loss': 0.028255270794034004, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37284 < 41816; dropping {'train_loss': 0.049774155020713806, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37294 < 41816; dropping {'train_loss': 0.011372665874660015, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37304 < 41816; dropping {'train_loss': 0.028708981350064278, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37314 < 41816; dropping {'train_loss': 0.06685730814933777, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37324 < 41816; dropping {'train_loss': 0.12205779552459717, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37334 < 41816; dropping {'train_loss': 0.03943883255124092, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37344 < 41816; dropping {'train_loss': 0.06797227263450623, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37354 < 41816; dropping {'train_loss': 0.004904499277472496, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37364 < 41816; dropping {'train_loss': 0.010089525952935219, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37374 < 41816; dropping {'train_loss': 0.055081870406866074, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37384 < 41816; dropping {'train_loss': 1.1263076066970825, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37394 < 41816; dropping {'train_loss': 0.02280782163143158, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37404 < 41816; dropping {'train_loss': 0.05396372079849243, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37414 < 41816; dropping {'train_loss': 0.5536766648292542, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37424 < 41816; dropping {'train_loss': 0.2152249813079834, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37434 < 41816; dropping {'train_loss': 1.7978256940841675, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37444 < 41816; dropping {'train_loss': 0.04076070338487625, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37454 < 41816; dropping {'train_loss': 0.03292649984359741, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37464 < 41816; dropping {'train_loss': 0.05130578577518463, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37474 < 41816; dropping {'train_loss': 0.0954132005572319, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37484 < 41816; dropping {'train_loss': 0.015081764198839664, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37494 < 41816; dropping {'train_loss': 0.10842064023017883, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37504 < 41816; dropping {'train_loss': 0.08015420287847519, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37514 < 41816; dropping {'train_loss': 0.14958520233631134, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37524 < 41816; dropping {'train_loss': 0.010652914643287659, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37534 < 41816; dropping {'train_loss': 0.032525684684515, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37544 < 41816; dropping {'train_loss': 0.05553082376718521, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37554 < 41816; dropping {'train_loss': 0.08847653865814209, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37564 < 41816; dropping {'train_loss': 0.16190510988235474, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37574 < 41816; dropping {'train_loss': 0.09649019688367844, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37584 < 41816; dropping {'train_loss': 0.1044226810336113, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37594 < 41816; dropping {'train_loss': 0.1716862916946411, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37604 < 41816; dropping {'train_loss': 0.24326972663402557, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37614 < 41816; dropping {'train_loss': 0.48469167947769165, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37624 < 41816; dropping {'train_loss': 0.0347229465842247, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37634 < 41816; dropping {'train_loss': 0.22048909962177277, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37644 < 41816; dropping {'train_loss': 0.14144174754619598, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37654 < 41816; dropping {'train_loss': 0.0473824217915535, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37664 < 41816; dropping {'train_loss': 0.14554695785045624, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37674 < 41816; dropping {'train_loss': 0.2758156657218933, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37684 < 41816; dropping {'train_loss': 0.11584578454494476, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37694 < 41816; dropping {'train_loss': 0.020682960748672485, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37704 < 41816; dropping {'train_loss': 0.1163276806473732, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37714 < 41816; dropping {'train_loss': 1.5901691913604736, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37724 < 41816; dropping {'train_loss': 0.14677436649799347, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37734 < 41816; dropping {'train_loss': 0.13608820736408234, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37744 < 41816; dropping {'train_loss': 0.2884179651737213, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37754 < 41816; dropping {'train_loss': 0.30297571420669556, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37764 < 41816; dropping {'train_loss': 0.3251740038394928, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37774 < 41816; dropping {'train_loss': 0.2695819139480591, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37784 < 41816; dropping {'train_loss': 0.022801028564572334, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37794 < 41816; dropping {'train_loss': 0.09853722155094147, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37804 < 41816; dropping {'train_loss': 0.324599951505661, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37814 < 41816; dropping {'train_loss': 0.06364822387695312, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37824 < 41816; dropping {'train_loss': 0.025077493861317635, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37834 < 41816; dropping {'train_loss': 0.49839404225349426, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37844 < 41816; dropping {'train_loss': 0.03726187348365784, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37854 < 41816; dropping {'train_loss': 0.09532640129327774, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37864 < 41816; dropping {'train_loss': 0.029296372085809708, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37874 < 41816; dropping {'train_loss': 1.2007218599319458, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37884 < 41816; dropping {'train_loss': 0.5586919188499451, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37894 < 41816; dropping {'train_loss': 0.020581822842359543, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37904 < 41816; dropping {'train_loss': 0.10175525397062302, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37914 < 41816; dropping {'train_loss': 0.013646505773067474, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37924 < 41816; dropping {'train_loss': 2.470945358276367, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37934 < 41816; dropping {'train_loss': 0.04428544640541077, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37944 < 41816; dropping {'train_loss': 0.19633257389068604, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37954 < 41816; dropping {'train_loss': 0.015687892213463783, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37964 < 41816; dropping {'train_loss': 0.024306917563080788, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37974 < 41816; dropping {'train_loss': 0.013798236846923828, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37984 < 41816; dropping {'train_loss': 0.33477258682250977, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 37994 < 41816; dropping {'train_loss': 0.13594402372837067, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38004 < 41816; dropping {'train_loss': 0.039587944746017456, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38014 < 41816; dropping {'train_loss': 0.024732457473874092, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38024 < 41816; dropping {'train_loss': 0.027243370190262794, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38034 < 41816; dropping {'train_loss': 0.1558602750301361, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38044 < 41816; dropping {'train_loss': 0.042241472750902176, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38054 < 41816; dropping {'train_loss': 0.2422207146883011, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38064 < 41816; dropping {'train_loss': 0.16637298464775085, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38074 < 41816; dropping {'train_loss': 0.17115327715873718, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38084 < 41816; dropping {'train_loss': 0.085152268409729, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38094 < 41816; dropping {'train_loss': 0.08382627367973328, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38104 < 41816; dropping {'train_loss': 0.188138946890831, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38114 < 41816; dropping {'train_loss': 0.009359250776469707, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38124 < 41816; dropping {'train_loss': 0.01973148062825203, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38134 < 41816; dropping {'train_loss': 0.017466533929109573, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38144 < 41816; dropping {'train_loss': 0.00749337999150157, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38154 < 41816; dropping {'train_loss': 0.6584526300430298, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38164 < 41816; dropping {'train_loss': 0.5518370866775513, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38174 < 41816; dropping {'train_loss': 0.06368466466665268, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38184 < 41816; dropping {'train_loss': 0.21624694764614105, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38194 < 41816; dropping {'train_loss': 0.2925613522529602, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38204 < 41816; dropping {'train_loss': 0.6242595911026001, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38214 < 41816; dropping {'train_loss': 2.447821855545044, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38224 < 41816; dropping {'train_loss': 0.017847977578639984, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38234 < 41816; dropping {'train_loss': 0.13076207041740417, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38244 < 41816; dropping {'train_loss': 0.5551120042800903, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38254 < 41816; dropping {'train_loss': 0.08197912573814392, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38264 < 41816; dropping {'train_loss': 0.026880640536546707, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38274 < 41816; dropping {'train_loss': 0.01255002524703741, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38284 < 41816; dropping {'train_loss': 0.1645561158657074, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38294 < 41816; dropping {'train_loss': 0.4085277020931244, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38304 < 41816; dropping {'train_loss': 0.02159961871802807, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38314 < 41816; dropping {'train_loss': 0.3417370617389679, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38324 < 41816; dropping {'train_loss': 0.0967501848936081, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38334 < 41816; dropping {'train_loss': 2.9955976009368896, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38344 < 41816; dropping {'train_loss': 0.05905913561582565, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38354 < 41816; dropping {'train_loss': 0.14189989864826202, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38364 < 41816; dropping {'train_loss': 0.0646812915802002, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38374 < 41816; dropping {'train_loss': 0.2085411548614502, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38384 < 41816; dropping {'train_loss': 0.05769285932183266, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38394 < 41816; dropping {'train_loss': 0.08312749117612839, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38404 < 41816; dropping {'train_loss': 0.32974308729171753, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38414 < 41816; dropping {'train_loss': 0.07978110760450363, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38424 < 41816; dropping {'train_loss': 0.026701927185058594, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38434 < 41816; dropping {'train_loss': 0.3096979856491089, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38444 < 41816; dropping {'train_loss': 0.022509921342134476, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38454 < 41816; dropping {'train_loss': 0.05562102422118187, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38464 < 41816; dropping {'train_loss': 2.8086111545562744, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38474 < 41816; dropping {'train_loss': 0.3810505270957947, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38484 < 41816; dropping {'train_loss': 0.2506132125854492, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38494 < 41816; dropping {'train_loss': 0.013613168150186539, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38504 < 41816; dropping {'train_loss': 0.3395257592201233, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38514 < 41816; dropping {'train_loss': 0.2084507942199707, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38524 < 41816; dropping {'train_loss': 0.049911074340343475, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38534 < 41816; dropping {'train_loss': 0.41272497177124023, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38544 < 41816; dropping {'train_loss': 0.1950208842754364, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38554 < 41816; dropping {'train_loss': 0.010823676362633705, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38564 < 41816; dropping {'train_loss': 0.017608415335416794, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38574 < 41816; dropping {'train_loss': 2.063948392868042, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38584 < 41816; dropping {'train_loss': 0.04455668106675148, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38594 < 41816; dropping {'train_loss': 0.32102757692337036, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38604 < 41816; dropping {'train_loss': 0.4404434561729431, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38614 < 41816; dropping {'train_loss': 0.03828173130750656, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38624 < 41816; dropping {'train_loss': 0.030128948390483856, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38634 < 41816; dropping {'train_loss': 0.11396826803684235, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38644 < 41816; dropping {'train_loss': 1.6051712036132812, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38654 < 41816; dropping {'train_loss': 0.17163191735744476, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38664 < 41816; dropping {'train_loss': 0.012558857910335064, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38674 < 41816; dropping {'train_loss': 0.5134820938110352, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38684 < 41816; dropping {'train_loss': 0.057618189603090286, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38694 < 41816; dropping {'train_loss': 0.11443084478378296, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38704 < 41816; dropping {'train_loss': 0.012362214736640453, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38714 < 41816; dropping {'train_loss': 0.08514989912509918, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38724 < 41816; dropping {'train_loss': 0.6139018535614014, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38734 < 41816; dropping {'train_loss': 0.029087195172905922, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38744 < 41816; dropping {'train_loss': 0.02486874908208847, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38754 < 41816; dropping {'train_loss': 2.056861162185669, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38764 < 41816; dropping {'train_loss': 0.31770768761634827, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38774 < 41816; dropping {'train_loss': 0.13234667479991913, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38784 < 41816; dropping {'train_loss': 0.08575639873743057, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38794 < 41816; dropping {'train_loss': 0.06557250767946243, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38804 < 41816; dropping {'train_loss': 0.08400001376867294, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38814 < 41816; dropping {'train_loss': 0.12231382727622986, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38824 < 41816; dropping {'train_loss': 0.005900426767766476, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38834 < 41816; dropping {'train_loss': 0.06152460351586342, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38844 < 41816; dropping {'train_loss': 0.008768509142100811, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38854 < 41816; dropping {'train_loss': 0.08629506081342697, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38864 < 41816; dropping {'train_loss': 0.3218761086463928, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38874 < 41816; dropping {'train_loss': 0.006976292468607426, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38884 < 41816; dropping {'train_loss': 0.0744422972202301, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38894 < 41816; dropping {'train_loss': 0.11552789062261581, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38904 < 41816; dropping {'train_loss': 0.7738543152809143, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38914 < 41816; dropping {'train_loss': 0.03952883183956146, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38924 < 41816; dropping {'train_loss': 0.010831103660166264, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38934 < 41816; dropping {'train_loss': 0.03901011124253273, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38944 < 41816; dropping {'train_loss': 0.012185041792690754, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38954 < 41816; dropping {'train_loss': 0.07492917776107788, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38964 < 41816; dropping {'train_loss': 0.013722449541091919, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38974 < 41816; dropping {'train_loss': 0.08055993914604187, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38984 < 41816; dropping {'train_loss': 0.01974324695765972, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 38994 < 41816; dropping {'train_loss': 0.18919049203395844, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39004 < 41816; dropping {'train_loss': 0.6654626131057739, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39014 < 41816; dropping {'train_loss': 0.043858591467142105, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39024 < 41816; dropping {'train_loss': 0.3905256390571594, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39034 < 41816; dropping {'train_loss': 0.036769717931747437, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39044 < 41816; dropping {'train_loss': 0.021841099485754967, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39054 < 41816; dropping {'train_loss': 0.12885399162769318, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39064 < 41816; dropping {'train_loss': 6.039491653442383, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39074 < 41816; dropping {'train_loss': 0.07669525593519211, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39084 < 41816; dropping {'train_loss': 0.7556934952735901, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39094 < 41816; dropping {'train_loss': 0.007593789603561163, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39104 < 41816; dropping {'train_loss': 2.2704696655273438, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39114 < 41816; dropping {'train_loss': 0.07674624770879745, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39124 < 41816; dropping {'train_loss': 1.6412224769592285, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39134 < 41816; dropping {'train_loss': 0.9927261471748352, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39144 < 41816; dropping {'train_loss': 1.7985190153121948, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39154 < 41816; dropping {'train_loss': 0.13737475872039795, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39164 < 41816; dropping {'train_loss': 0.20939072966575623, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39174 < 41816; dropping {'train_loss': 0.29874664545059204, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39184 < 41816; dropping {'train_loss': 0.14413441717624664, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39194 < 41816; dropping {'train_loss': 0.14955608546733856, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39204 < 41816; dropping {'train_loss': 0.0334605798125267, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39214 < 41816; dropping {'train_loss': 0.34374141693115234, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39224 < 41816; dropping {'train_loss': 0.039526812732219696, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39234 < 41816; dropping {'train_loss': 0.014872545376420021, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39244 < 41816; dropping {'train_loss': 0.26240217685699463, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39254 < 41816; dropping {'train_loss': 0.08807568997144699, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39264 < 41816; dropping {'train_loss': 0.5117538571357727, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39274 < 41816; dropping {'train_loss': 0.06155535206198692, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39284 < 41816; dropping {'train_loss': 0.17150099575519562, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39294 < 41816; dropping {'train_loss': 0.019311685115098953, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39304 < 41816; dropping {'train_loss': 0.4399855136871338, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39314 < 41816; dropping {'train_loss': 0.02565096504986286, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39324 < 41816; dropping {'train_loss': 0.44676750898361206, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39334 < 41816; dropping {'train_loss': 0.4555611312389374, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39344 < 41816; dropping {'train_loss': 0.07189807295799255, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39354 < 41816; dropping {'train_loss': 0.11049916595220566, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39364 < 41816; dropping {'train_loss': 0.05309753492474556, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39374 < 41816; dropping {'train_loss': 0.029903991147875786, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39384 < 41816; dropping {'train_loss': 0.046258702874183655, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39394 < 41816; dropping {'train_loss': 0.8336138725280762, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39404 < 41816; dropping {'train_loss': 0.08970049768686295, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39414 < 41816; dropping {'train_loss': 0.0393061600625515, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39424 < 41816; dropping {'train_loss': 0.012787573970854282, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39434 < 41816; dropping {'train_loss': 0.273163378238678, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39444 < 41816; dropping {'train_loss': 0.027784494683146477, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39454 < 41816; dropping {'train_loss': 0.08869637548923492, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39464 < 41816; dropping {'train_loss': 1.455190658569336, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39474 < 41816; dropping {'train_loss': 0.1357303112745285, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39484 < 41816; dropping {'train_loss': 0.1035827249288559, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39494 < 41816; dropping {'train_loss': 0.023418797180056572, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39504 < 41816; dropping {'train_loss': 0.07663104683160782, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39514 < 41816; dropping {'train_loss': 0.027291329577565193, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39524 < 41816; dropping {'train_loss': 0.03732890635728836, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39534 < 41816; dropping {'train_loss': 1.0223345756530762, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39544 < 41816; dropping {'train_loss': 0.18074074387550354, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39554 < 41816; dropping {'train_loss': 0.03944503143429756, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39564 < 41816; dropping {'train_loss': 0.22319373488426208, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39574 < 41816; dropping {'train_loss': 0.048540517687797546, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39584 < 41816; dropping {'train_loss': 0.017185358330607414, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39594 < 41816; dropping {'train_loss': 0.015772830694913864, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39604 < 41816; dropping {'train_loss': 0.06893551349639893, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39614 < 41816; dropping {'train_loss': 0.12157511711120605, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39624 < 41816; dropping {'train_loss': 0.06788654625415802, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39634 < 41816; dropping {'train_loss': 0.047406502068042755, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39644 < 41816; dropping {'train_loss': 2.5655391216278076, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39654 < 41816; dropping {'train_loss': 0.2606610953807831, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39664 < 41816; dropping {'train_loss': 1.1814156770706177, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39674 < 41816; dropping {'train_loss': 0.021336369216442108, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39684 < 41816; dropping {'train_loss': 0.456866055727005, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39694 < 41816; dropping {'train_loss': 0.18584173917770386, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39704 < 41816; dropping {'train_loss': 0.1636359989643097, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39714 < 41816; dropping {'train_loss': 1.1782588958740234, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39724 < 41816; dropping {'train_loss': 0.1462477445602417, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39734 < 41816; dropping {'train_loss': 0.09722508490085602, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39744 < 41816; dropping {'train_loss': 0.13178656995296478, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39754 < 41816; dropping {'train_loss': 0.6405001878738403, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39764 < 41816; dropping {'train_loss': 0.629763126373291, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39774 < 41816; dropping {'train_loss': 0.028609300032258034, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39784 < 41816; dropping {'train_loss': 0.12493591755628586, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39794 < 41816; dropping {'train_loss': 0.36044755578041077, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39804 < 41816; dropping {'train_loss': 0.0806577131152153, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39814 < 41816; dropping {'train_loss': 0.04974435269832611, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39824 < 41816; dropping {'train_loss': 0.7822007536888123, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39834 < 41816; dropping {'train_loss': 0.25217902660369873, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39844 < 41816; dropping {'train_loss': 0.14835362136363983, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39854 < 41816; dropping {'train_loss': 0.23065946996212006, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39864 < 41816; dropping {'train_loss': 0.36575374007225037, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39874 < 41816; dropping {'train_loss': 0.046558961272239685, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39884 < 41816; dropping {'train_loss': 0.2530083656311035, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39894 < 41816; dropping {'train_loss': 0.17272847890853882, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39904 < 41816; dropping {'train_loss': 0.16702279448509216, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39914 < 41816; dropping {'train_loss': 0.010140758007764816, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39924 < 41816; dropping {'train_loss': 0.11702436208724976, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39934 < 41816; dropping {'train_loss': 0.21963201463222504, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39944 < 41816; dropping {'train_loss': 0.15510393679141998, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39954 < 41816; dropping {'train_loss': 0.6341114044189453, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39964 < 41816; dropping {'train_loss': 5.259863376617432, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39974 < 41816; dropping {'train_loss': 0.46419623494148254, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39984 < 41816; dropping {'train_loss': 1.7909600734710693, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 39994 < 41816; dropping {'train_loss': 0.03133751451969147, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40004 < 41816; dropping {'train_loss': 0.09894352406263351, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40014 < 41816; dropping {'train_loss': 0.23502199351787567, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40024 < 41816; dropping {'train_loss': 0.09269838780164719, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40034 < 41816; dropping {'train_loss': 0.033295221626758575, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40044 < 41816; dropping {'train_loss': 0.04300514608621597, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40054 < 41816; dropping {'train_loss': 0.3871747851371765, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40064 < 41816; dropping {'train_loss': 0.1996256709098816, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40074 < 41816; dropping {'train_loss': 0.15595707297325134, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40084 < 41816; dropping {'train_loss': 0.009268735535442829, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40094 < 41816; dropping {'train_loss': 0.011130680330097675, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40104 < 41816; dropping {'train_loss': 0.09172128885984421, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40114 < 41816; dropping {'train_loss': 0.02858436480164528, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40124 < 41816; dropping {'train_loss': 0.03173552826046944, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40134 < 41816; dropping {'train_loss': 1.3811155557632446, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40144 < 41816; dropping {'train_loss': 0.05188541114330292, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40154 < 41816; dropping {'train_loss': 0.09283117949962616, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40164 < 41816; dropping {'train_loss': 0.10499057173728943, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40174 < 41816; dropping {'train_loss': 0.018760161474347115, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40184 < 41816; dropping {'train_loss': 0.235225647687912, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40194 < 41816; dropping {'train_loss': 0.050755538046360016, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40204 < 41816; dropping {'train_loss': 0.21812613308429718, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40214 < 41816; dropping {'train_loss': 1.4084733724594116, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40224 < 41816; dropping {'train_loss': 0.051219627261161804, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40234 < 41816; dropping {'train_loss': 0.11418076604604721, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40244 < 41816; dropping {'train_loss': 0.11500687152147293, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40254 < 41816; dropping {'train_loss': 0.03922576084733009, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40264 < 41816; dropping {'train_loss': 0.020307322964072227, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40274 < 41816; dropping {'train_loss': 0.18770314753055573, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40284 < 41816; dropping {'train_loss': 0.005797173827886581, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40294 < 41816; dropping {'train_loss': 0.041490357369184494, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40304 < 41816; dropping {'train_loss': 0.036810729652643204, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40314 < 41816; dropping {'train_loss': 0.05363794416189194, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40324 < 41816; dropping {'train_loss': 0.07298799604177475, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40334 < 41816; dropping {'train_loss': 3.379768133163452, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40344 < 41816; dropping {'train_loss': 0.07917394489049911, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40354 < 41816; dropping {'train_loss': 0.46843743324279785, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40364 < 41816; dropping {'train_loss': 0.25908344984054565, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40374 < 41816; dropping {'train_loss': 0.06621117889881134, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40384 < 41816; dropping {'train_loss': 0.01780831068754196, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40394 < 41816; dropping {'train_loss': 0.039331816136837006, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40404 < 41816; dropping {'train_loss': 0.2757910490036011, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40414 < 41816; dropping {'train_loss': 0.1672799438238144, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40424 < 41816; dropping {'train_loss': 0.006966094486415386, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40434 < 41816; dropping {'train_loss': 0.017454296350479126, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40444 < 41816; dropping {'train_loss': 0.1533413976430893, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40454 < 41816; dropping {'train_loss': 0.046212054789066315, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40464 < 41816; dropping {'train_loss': 0.11022135615348816, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40474 < 41816; dropping {'train_loss': 0.026455795392394066, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40484 < 41816; dropping {'train_loss': 0.45867443084716797, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40494 < 41816; dropping {'train_loss': 0.03179766237735748, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40504 < 41816; dropping {'train_loss': 0.006239491980522871, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40514 < 41816; dropping {'train_loss': 0.07576967775821686, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40524 < 41816; dropping {'train_loss': 0.12074767053127289, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40534 < 41816; dropping {'train_loss': 0.9010646343231201, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40544 < 41816; dropping {'train_loss': 0.1263827383518219, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40554 < 41816; dropping {'train_loss': 0.6803081631660461, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40564 < 41816; dropping {'train_loss': 1.3381378650665283, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40574 < 41816; dropping {'train_loss': 0.10781387239694595, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40584 < 41816; dropping {'train_loss': 0.03835843876004219, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40594 < 41816; dropping {'train_loss': 0.04016208276152611, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40604 < 41816; dropping {'train_loss': 0.3484768569469452, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40614 < 41816; dropping {'train_loss': 0.03571576625108719, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40624 < 41816; dropping {'train_loss': 0.20209425687789917, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40634 < 41816; dropping {'train_loss': 0.010151277296245098, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40644 < 41816; dropping {'train_loss': 0.0763491541147232, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40654 < 41816; dropping {'train_loss': 0.01272920984774828, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40664 < 41816; dropping {'train_loss': 0.0139065682888031, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40674 < 41816; dropping {'train_loss': 0.49776020646095276, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40684 < 41816; dropping {'train_loss': 0.20343369245529175, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40694 < 41816; dropping {'train_loss': 0.10138854384422302, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40704 < 41816; dropping {'train_loss': 0.4777253270149231, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40714 < 41816; dropping {'train_loss': 0.016479508951306343, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40724 < 41816; dropping {'train_loss': 0.28660961985588074, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40734 < 41816; dropping {'train_loss': 0.06835402548313141, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40744 < 41816; dropping {'train_loss': 0.016224555671215057, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40754 < 41816; dropping {'train_loss': 0.20184630155563354, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40764 < 41816; dropping {'train_loss': 0.12738177180290222, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40774 < 41816; dropping {'train_loss': 0.016852006316184998, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40784 < 41816; dropping {'train_loss': 0.06608208268880844, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40794 < 41816; dropping {'train_loss': 0.2910613417625427, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40804 < 41816; dropping {'train_loss': 0.012746216729283333, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40814 < 41816; dropping {'train_loss': 0.1922072023153305, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40824 < 41816; dropping {'train_loss': 0.011084271594882011, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40834 < 41816; dropping {'train_loss': 0.09365709871053696, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40844 < 41816; dropping {'train_loss': 1.0741196870803833, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40854 < 41816; dropping {'train_loss': 1.470387578010559, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40864 < 41816; dropping {'train_loss': 0.5228948593139648, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40874 < 41816; dropping {'train_loss': 0.15181860327720642, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40884 < 41816; dropping {'train_loss': 0.031555213034152985, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40894 < 41816; dropping {'train_loss': 1.9886020421981812, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40904 < 41816; dropping {'train_loss': 0.17527960240840912, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40914 < 41816; dropping {'train_loss': 0.18781034648418427, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40924 < 41816; dropping {'train_loss': 0.07781682908535004, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40934 < 41816; dropping {'train_loss': 0.19214341044425964, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40944 < 41816; dropping {'train_loss': 0.01331665925681591, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40954 < 41816; dropping {'train_loss': 0.14165663719177246, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40964 < 41816; dropping {'train_loss': 0.12532921135425568, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40974 < 41816; dropping {'train_loss': 0.2464248090982437, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40984 < 41816; dropping {'train_loss': 0.041298627853393555, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 40994 < 41816; dropping {'train_loss': 0.10037156939506531, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41004 < 41816; dropping {'train_loss': 0.39606115221977234, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41014 < 41816; dropping {'train_loss': 0.2441604733467102, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41024 < 41816; dropping {'train_loss': 0.04188011959195137, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41034 < 41816; dropping {'train_loss': 0.22066421806812286, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41044 < 41816; dropping {'train_loss': 0.022642165422439575, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41054 < 41816; dropping {'train_loss': 0.3531469404697418, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41064 < 41816; dropping {'train_loss': 0.2085127830505371, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41074 < 41816; dropping {'train_loss': 0.06939411908388138, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41084 < 41816; dropping {'train_loss': 0.4980058968067169, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41094 < 41816; dropping {'train_loss': 0.011174079030752182, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41104 < 41816; dropping {'train_loss': 0.020048493519425392, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41114 < 41816; dropping {'train_loss': 0.1156083196401596, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41124 < 41816; dropping {'train_loss': 0.011769752018153667, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41134 < 41816; dropping {'train_loss': 0.22450925409793854, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41144 < 41816; dropping {'train_loss': 0.27772581577301025, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41154 < 41816; dropping {'train_loss': 0.03348980471491814, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41164 < 41816; dropping {'train_loss': 0.9531702399253845, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41174 < 41816; dropping {'train_loss': 0.05488256365060806, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41184 < 41816; dropping {'train_loss': 0.042091112583875656, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41194 < 41816; dropping {'train_loss': 2.7316057682037354, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41204 < 41816; dropping {'train_loss': 0.027166835963726044, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41214 < 41816; dropping {'train_loss': 0.29482343792915344, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41224 < 41816; dropping {'train_loss': 0.7427041530609131, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41234 < 41816; dropping {'train_loss': 0.004638440907001495, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41244 < 41816; dropping {'train_loss': 0.05840832740068436, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41254 < 41816; dropping {'train_loss': 0.11891239136457443, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41264 < 41816; dropping {'train_loss': 0.045020610094070435, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41274 < 41816; dropping {'train_loss': 0.1561143845319748, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41284 < 41816; dropping {'train_loss': 0.0224966648966074, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41294 < 41816; dropping {'train_loss': 0.02044942043721676, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41304 < 41816; dropping {'train_loss': 0.006039987783879042, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41314 < 41816; dropping {'train_loss': 0.009832127951085567, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41324 < 41816; dropping {'train_loss': 0.3214983642101288, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41334 < 41816; dropping {'train_loss': 0.13897323608398438, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41344 < 41816; dropping {'train_loss': 0.03550906479358673, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41354 < 41816; dropping {'train_loss': 0.8432268500328064, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41364 < 41816; dropping {'train_loss': 0.2140563577413559, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41374 < 41816; dropping {'train_loss': 0.00864215288311243, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41384 < 41816; dropping {'train_loss': 0.12108675390481949, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41394 < 41816; dropping {'train_loss': 0.07639636844396591, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41404 < 41816; dropping {'train_loss': 0.19573476910591125, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41414 < 41816; dropping {'train_loss': 0.1873556673526764, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41424 < 41816; dropping {'train_loss': 0.2657734453678131, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41434 < 41816; dropping {'train_loss': 0.8321987390518188, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41444 < 41816; dropping {'train_loss': 0.3006255030632019, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41454 < 41816; dropping {'train_loss': 0.05881493538618088, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41464 < 41816; dropping {'train_loss': 0.05454043298959732, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41474 < 41816; dropping {'train_loss': 0.2533996105194092, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41484 < 41816; dropping {'train_loss': 0.05081934481859207, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41494 < 41816; dropping {'train_loss': 0.7024651765823364, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41504 < 41816; dropping {'train_loss': 0.770810067653656, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41514 < 41816; dropping {'train_loss': 0.06680883467197418, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41524 < 41816; dropping {'train_loss': 0.36163389682769775, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41534 < 41816; dropping {'train_loss': 0.08768180012702942, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41544 < 41816; dropping {'train_loss': 0.01800844818353653, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41554 < 41816; dropping {'train_loss': 0.07103677839040756, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41564 < 41816; dropping {'train_loss': 0.421251505613327, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41574 < 41816; dropping {'train_loss': 0.12022625654935837, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41584 < 41816; dropping {'train_loss': 0.01063955295830965, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41594 < 41816; dropping {'train_loss': 0.09894075989723206, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41604 < 41816; dropping {'train_loss': 0.018895775079727173, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41614 < 41816; dropping {'train_loss': 0.026151269674301147, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41624 < 41816; dropping {'train_loss': 0.008482466451823711, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41634 < 41816; dropping {'train_loss': 0.00600634329020977, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41644 < 41816; dropping {'train_loss': 0.2879696190357208, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41654 < 41816; dropping {'train_loss': 0.012718805111944675, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41664 < 41816; dropping {'train_loss': 0.139606311917305, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41674 < 41816; dropping {'train_loss': 0.023605599999427795, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41684 < 41816; dropping {'train_loss': 0.30681100487709045, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41694 < 41816; dropping {'train_loss': 0.8182852268218994, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41704 < 41816; dropping {'train_loss': 0.21194922924041748, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41714 < 41816; dropping {'train_loss': 0.03444112837314606, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41724 < 41816; dropping {'train_loss': 1.249571442604065, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41734 < 41816; dropping {'train_loss': 0.02484658546745777, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41744 < 41816; dropping {'train_loss': 0.056414008140563965, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41754 < 41816; dropping {'train_loss': 0.011940077878534794, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41764 < 41816; dropping {'train_loss': 0.2791518270969391, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41774 < 41816; dropping {'train_loss': 0.20347297191619873, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41784 < 41816; dropping {'train_loss': 0.014611795544624329, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41794 < 41816; dropping {'train_loss': 0.6049286127090454, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41804 < 41816; dropping {'train_loss': 0.037932660430669785, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 41814 < 41816; dropping {'train_loss': 0.03401260823011398, 'epoch': 2}.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e96879e04b9540898bfe4b0cd32aca6e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53765 < 59738; dropping {'val_epoch_loss': 0.2817366421222687, 'val_epoch_auc': 0.9948564190644321, 'epoch': 2}.\n",
            "\n",
            "Epoch 00002: val_epoch_auc reached 0.99486 (best 0.99486), saving model to drive/My Drive/Colab Notebooks/checkpoints/DeepPavlov_ru-SBERT_epoch=2-val_epoch_auc=0.99.ckpt as top 1\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53766 < 59738; dropping {'train_epoch_loss': 0.2515919804573059, 'epoch': 2}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53766 < 59738; dropping {'train_loss': 0.016358355060219765, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53776 < 59738; dropping {'train_loss': 0.011007015593349934, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53786 < 59738; dropping {'train_loss': 0.2984521985054016, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53796 < 59738; dropping {'train_loss': 0.015351028181612492, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53806 < 59738; dropping {'train_loss': 0.19833143055438995, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53816 < 59738; dropping {'train_loss': 0.030884651467204094, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53826 < 59738; dropping {'train_loss': 0.005863908212631941, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53836 < 59738; dropping {'train_loss': 0.06909807026386261, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53846 < 59738; dropping {'train_loss': 0.18594475090503693, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53856 < 59738; dropping {'train_loss': 0.00716592650860548, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53866 < 59738; dropping {'train_loss': 0.043645210564136505, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53876 < 59738; dropping {'train_loss': 0.007292766589671373, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53886 < 59738; dropping {'train_loss': 0.12487594783306122, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53896 < 59738; dropping {'train_loss': 0.008284708485007286, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53906 < 59738; dropping {'train_loss': 0.1933351755142212, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53916 < 59738; dropping {'train_loss': 0.01757110096514225, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53926 < 59738; dropping {'train_loss': 0.044009726494550705, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53936 < 59738; dropping {'train_loss': 0.004151362460106611, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53946 < 59738; dropping {'train_loss': 0.06918618828058243, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53956 < 59738; dropping {'train_loss': 0.006259957328438759, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53966 < 59738; dropping {'train_loss': 0.043508850038051605, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53976 < 59738; dropping {'train_loss': 0.013841596432030201, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53986 < 59738; dropping {'train_loss': 0.009597101248800755, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 53996 < 59738; dropping {'train_loss': 0.26742497086524963, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54006 < 59738; dropping {'train_loss': 0.10372087359428406, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54016 < 59738; dropping {'train_loss': 0.07720302790403366, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54026 < 59738; dropping {'train_loss': 0.08340304344892502, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54036 < 59738; dropping {'train_loss': 0.045277852565050125, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54046 < 59738; dropping {'train_loss': 0.07200979441404343, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54056 < 59738; dropping {'train_loss': 0.024304687976837158, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54066 < 59738; dropping {'train_loss': 0.24422279000282288, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54076 < 59738; dropping {'train_loss': 0.011773030273616314, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54086 < 59738; dropping {'train_loss': 1.2899882793426514, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54096 < 59738; dropping {'train_loss': 0.2886126935482025, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54106 < 59738; dropping {'train_loss': 0.0033874146174639463, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54116 < 59738; dropping {'train_loss': 0.01833665557205677, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54126 < 59738; dropping {'train_loss': 0.09821899980306625, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54136 < 59738; dropping {'train_loss': 0.01895725354552269, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54146 < 59738; dropping {'train_loss': 0.16373419761657715, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54156 < 59738; dropping {'train_loss': 0.005777786485850811, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54166 < 59738; dropping {'train_loss': 0.005169115960597992, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54176 < 59738; dropping {'train_loss': 0.00446547893807292, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54186 < 59738; dropping {'train_loss': 0.015604197047650814, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54196 < 59738; dropping {'train_loss': 0.023281395435333252, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54206 < 59738; dropping {'train_loss': 0.006672691088169813, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54216 < 59738; dropping {'train_loss': 0.16679146885871887, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54226 < 59738; dropping {'train_loss': 0.020087022334337234, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54236 < 59738; dropping {'train_loss': 0.18173210322856903, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54246 < 59738; dropping {'train_loss': 0.012823029421269894, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54256 < 59738; dropping {'train_loss': 0.007772981654852629, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54266 < 59738; dropping {'train_loss': 0.12482159584760666, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54276 < 59738; dropping {'train_loss': 0.006907028611749411, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54286 < 59738; dropping {'train_loss': 0.20777197182178497, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54296 < 59738; dropping {'train_loss': 0.021413924172520638, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54306 < 59738; dropping {'train_loss': 0.013099626637995243, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54316 < 59738; dropping {'train_loss': 0.037174150347709656, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54326 < 59738; dropping {'train_loss': 0.195697620511055, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54336 < 59738; dropping {'train_loss': 0.32723748683929443, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54346 < 59738; dropping {'train_loss': 0.01322109717875719, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54356 < 59738; dropping {'train_loss': 0.11234935373067856, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54366 < 59738; dropping {'train_loss': 0.3307981789112091, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54376 < 59738; dropping {'train_loss': 0.141755148768425, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54386 < 59738; dropping {'train_loss': 0.06671436876058578, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54396 < 59738; dropping {'train_loss': 0.010210440494120121, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54406 < 59738; dropping {'train_loss': 0.007241060491651297, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54416 < 59738; dropping {'train_loss': 0.12736129760742188, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54426 < 59738; dropping {'train_loss': 0.07817273586988449, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54436 < 59738; dropping {'train_loss': 0.044006749987602234, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54446 < 59738; dropping {'train_loss': 0.1174311637878418, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54456 < 59738; dropping {'train_loss': 0.033968400210142136, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54466 < 59738; dropping {'train_loss': 0.10803954303264618, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54476 < 59738; dropping {'train_loss': 0.027090124785900116, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54486 < 59738; dropping {'train_loss': 0.2870977222919464, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54496 < 59738; dropping {'train_loss': 0.05059747025370598, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54506 < 59738; dropping {'train_loss': 0.004670374095439911, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54516 < 59738; dropping {'train_loss': 0.0031753224320709705, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54526 < 59738; dropping {'train_loss': 0.11597035080194473, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54536 < 59738; dropping {'train_loss': 0.07540259510278702, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54546 < 59738; dropping {'train_loss': 0.12351057678461075, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54556 < 59738; dropping {'train_loss': 0.01696530357003212, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54566 < 59738; dropping {'train_loss': 0.003203661646693945, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54576 < 59738; dropping {'train_loss': 0.04327380284667015, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54586 < 59738; dropping {'train_loss': 0.019118569791316986, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54596 < 59738; dropping {'train_loss': 0.05595458298921585, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54606 < 59738; dropping {'train_loss': 0.06870204955339432, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54616 < 59738; dropping {'train_loss': 0.08447527140378952, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54626 < 59738; dropping {'train_loss': 0.2881718873977661, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54636 < 59738; dropping {'train_loss': 0.13260047137737274, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54646 < 59738; dropping {'train_loss': 0.015912087634205818, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54656 < 59738; dropping {'train_loss': 0.10230625420808792, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54666 < 59738; dropping {'train_loss': 0.036693014204502106, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54676 < 59738; dropping {'train_loss': 0.03503846749663353, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54686 < 59738; dropping {'train_loss': 0.02051421068608761, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54696 < 59738; dropping {'train_loss': 0.32513824105262756, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54706 < 59738; dropping {'train_loss': 0.017674053087830544, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54716 < 59738; dropping {'train_loss': 0.008674656972289085, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54726 < 59738; dropping {'train_loss': 0.04322386160492897, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54736 < 59738; dropping {'train_loss': 0.007368599995970726, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54746 < 59738; dropping {'train_loss': 0.030055606737732887, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54756 < 59738; dropping {'train_loss': 0.024773526936769485, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54766 < 59738; dropping {'train_loss': 0.11347920447587967, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54776 < 59738; dropping {'train_loss': 0.03476550057530403, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54786 < 59738; dropping {'train_loss': 0.011552102863788605, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54796 < 59738; dropping {'train_loss': 0.0032770682591944933, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54806 < 59738; dropping {'train_loss': 0.005395079962909222, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54816 < 59738; dropping {'train_loss': 0.07991422712802887, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54826 < 59738; dropping {'train_loss': 0.008372860960662365, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54836 < 59738; dropping {'train_loss': 0.008951839990913868, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54846 < 59738; dropping {'train_loss': 0.015372226014733315, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54856 < 59738; dropping {'train_loss': 0.055826928466558456, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54866 < 59738; dropping {'train_loss': 0.016628528013825417, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54876 < 59738; dropping {'train_loss': 0.011588559485971928, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54886 < 59738; dropping {'train_loss': 0.002748918952420354, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54896 < 59738; dropping {'train_loss': 0.007968136109411716, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54906 < 59738; dropping {'train_loss': 0.03207118809223175, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54916 < 59738; dropping {'train_loss': 0.3854871690273285, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54926 < 59738; dropping {'train_loss': 0.029512671753764153, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54936 < 59738; dropping {'train_loss': 0.05695243179798126, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54946 < 59738; dropping {'train_loss': 0.20012880861759186, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54956 < 59738; dropping {'train_loss': 0.9984074831008911, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54966 < 59738; dropping {'train_loss': 0.03327102214097977, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54976 < 59738; dropping {'train_loss': 0.02198738045990467, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54986 < 59738; dropping {'train_loss': 0.0817682072520256, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 54996 < 59738; dropping {'train_loss': 0.0049201264046132565, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55006 < 59738; dropping {'train_loss': 0.007320820819586515, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55016 < 59738; dropping {'train_loss': 0.22647078335285187, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55026 < 59738; dropping {'train_loss': 0.06834664195775986, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55036 < 59738; dropping {'train_loss': 0.022085802629590034, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55046 < 59738; dropping {'train_loss': 0.07108866423368454, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55056 < 59738; dropping {'train_loss': 0.1050863042473793, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55066 < 59738; dropping {'train_loss': 0.1127973198890686, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55076 < 59738; dropping {'train_loss': 0.0018351605394855142, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55086 < 59738; dropping {'train_loss': 0.0034487226512283087, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55096 < 59738; dropping {'train_loss': 0.10067620873451233, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55106 < 59738; dropping {'train_loss': 0.045882824808359146, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55116 < 59738; dropping {'train_loss': 0.29639026522636414, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55126 < 59738; dropping {'train_loss': 0.04215208441019058, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55136 < 59738; dropping {'train_loss': 0.01484950166195631, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55146 < 59738; dropping {'train_loss': 0.07510033994913101, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55156 < 59738; dropping {'train_loss': 0.004431738052517176, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55166 < 59738; dropping {'train_loss': 3.31492280960083, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55176 < 59738; dropping {'train_loss': 0.009194294922053814, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55186 < 59738; dropping {'train_loss': 0.5412889122962952, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55196 < 59738; dropping {'train_loss': 0.34767600893974304, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55206 < 59738; dropping {'train_loss': 2.9003381729125977, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55216 < 59738; dropping {'train_loss': 0.038432527333498, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55226 < 59738; dropping {'train_loss': 0.8507340550422668, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55236 < 59738; dropping {'train_loss': 0.4623930752277374, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55246 < 59738; dropping {'train_loss': 0.1520453840494156, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55256 < 59738; dropping {'train_loss': 0.0951850414276123, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55266 < 59738; dropping {'train_loss': 0.07428889721632004, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55276 < 59738; dropping {'train_loss': 0.05224652588367462, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55286 < 59738; dropping {'train_loss': 0.07118132710456848, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55296 < 59738; dropping {'train_loss': 0.0454353429377079, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55306 < 59738; dropping {'train_loss': 0.007126898970454931, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55316 < 59738; dropping {'train_loss': 0.28863975405693054, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55326 < 59738; dropping {'train_loss': 0.02706853300333023, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55336 < 59738; dropping {'train_loss': 0.1157972663640976, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55346 < 59738; dropping {'train_loss': 0.27871495485305786, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55356 < 59738; dropping {'train_loss': 0.014829677529633045, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55366 < 59738; dropping {'train_loss': 0.00920134223997593, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55376 < 59738; dropping {'train_loss': 0.06693820655345917, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55386 < 59738; dropping {'train_loss': 0.14412590861320496, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55396 < 59738; dropping {'train_loss': 0.03464788943529129, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55406 < 59738; dropping {'train_loss': 0.06699074059724808, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55416 < 59738; dropping {'train_loss': 0.05340702831745148, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55426 < 59738; dropping {'train_loss': 0.1302843987941742, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55436 < 59738; dropping {'train_loss': 0.10283932834863663, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55446 < 59738; dropping {'train_loss': 0.013346251100301743, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55456 < 59738; dropping {'train_loss': 1.8107281923294067, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55466 < 59738; dropping {'train_loss': 0.10690844804048538, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55476 < 59738; dropping {'train_loss': 0.550594687461853, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55486 < 59738; dropping {'train_loss': 0.007859786972403526, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55496 < 59738; dropping {'train_loss': 0.24495983123779297, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55506 < 59738; dropping {'train_loss': 0.009272083640098572, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55516 < 59738; dropping {'train_loss': 0.01698431186378002, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55526 < 59738; dropping {'train_loss': 0.14060766994953156, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55536 < 59738; dropping {'train_loss': 0.035331930965185165, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55546 < 59738; dropping {'train_loss': 0.009129283018410206, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55556 < 59738; dropping {'train_loss': 0.14743280410766602, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55566 < 59738; dropping {'train_loss': 0.15940847992897034, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55576 < 59738; dropping {'train_loss': 0.014959056861698627, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55586 < 59738; dropping {'train_loss': 0.02871931530535221, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55596 < 59738; dropping {'train_loss': 0.014574844390153885, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55606 < 59738; dropping {'train_loss': 1.1918643712997437, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55616 < 59738; dropping {'train_loss': 0.2782910168170929, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55626 < 59738; dropping {'train_loss': 0.5523342490196228, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55636 < 59738; dropping {'train_loss': 0.00845267716795206, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55646 < 59738; dropping {'train_loss': 0.3065745234489441, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55656 < 59738; dropping {'train_loss': 0.4542832672595978, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55666 < 59738; dropping {'train_loss': 0.1423027217388153, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55676 < 59738; dropping {'train_loss': 1.5130423307418823, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55686 < 59738; dropping {'train_loss': 0.016299167647957802, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55696 < 59738; dropping {'train_loss': 0.07709214091300964, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55706 < 59738; dropping {'train_loss': 0.07528620958328247, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55716 < 59738; dropping {'train_loss': 0.6153955459594727, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55726 < 59738; dropping {'train_loss': 0.10717768967151642, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55736 < 59738; dropping {'train_loss': 0.6123799085617065, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55746 < 59738; dropping {'train_loss': 0.004335243720561266, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55756 < 59738; dropping {'train_loss': 0.7415036559104919, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55766 < 59738; dropping {'train_loss': 0.009847594425082207, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55776 < 59738; dropping {'train_loss': 0.44312402606010437, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55786 < 59738; dropping {'train_loss': 0.13945911824703217, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55796 < 59738; dropping {'train_loss': 0.025206400081515312, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55806 < 59738; dropping {'train_loss': 0.09753725677728653, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55816 < 59738; dropping {'train_loss': 0.041926804929971695, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55826 < 59738; dropping {'train_loss': 0.06997215002775192, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55836 < 59738; dropping {'train_loss': 0.03440789878368378, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55846 < 59738; dropping {'train_loss': 0.03287399560213089, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55856 < 59738; dropping {'train_loss': 0.10295848548412323, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55866 < 59738; dropping {'train_loss': 0.06714732944965363, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55876 < 59738; dropping {'train_loss': 0.048821739852428436, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55886 < 59738; dropping {'train_loss': 0.010395050048828125, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55896 < 59738; dropping {'train_loss': 0.1759372353553772, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55906 < 59738; dropping {'train_loss': 0.04207346960902214, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55916 < 59738; dropping {'train_loss': 0.007168615702539682, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55926 < 59738; dropping {'train_loss': 0.10235214233398438, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55936 < 59738; dropping {'train_loss': 0.03557113930583, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55946 < 59738; dropping {'train_loss': 0.13094563782215118, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55956 < 59738; dropping {'train_loss': 0.12447910010814667, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55966 < 59738; dropping {'train_loss': 0.03434496745467186, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55976 < 59738; dropping {'train_loss': 0.027187665924429893, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55986 < 59738; dropping {'train_loss': 0.021165618672966957, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 55996 < 59738; dropping {'train_loss': 0.04803549498319626, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56006 < 59738; dropping {'train_loss': 0.03661416471004486, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56016 < 59738; dropping {'train_loss': 0.02846931293606758, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56026 < 59738; dropping {'train_loss': 0.0505773164331913, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56036 < 59738; dropping {'train_loss': 0.014631750993430614, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56046 < 59738; dropping {'train_loss': 0.024854904040694237, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56056 < 59738; dropping {'train_loss': 0.017161553725600243, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56066 < 59738; dropping {'train_loss': 0.22813910245895386, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56076 < 59738; dropping {'train_loss': 0.04934585839509964, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56086 < 59738; dropping {'train_loss': 0.40577542781829834, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56096 < 59738; dropping {'train_loss': 0.031168077141046524, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56106 < 59738; dropping {'train_loss': 0.008959827944636345, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56116 < 59738; dropping {'train_loss': 0.06469649076461792, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56126 < 59738; dropping {'train_loss': 0.07194782048463821, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56136 < 59738; dropping {'train_loss': 0.4999675154685974, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56146 < 59738; dropping {'train_loss': 0.06234776973724365, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56156 < 59738; dropping {'train_loss': 0.049333952367305756, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56166 < 59738; dropping {'train_loss': 0.023281030356884003, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56176 < 59738; dropping {'train_loss': 0.07696300745010376, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56186 < 59738; dropping {'train_loss': 0.008636780083179474, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56196 < 59738; dropping {'train_loss': 0.048935212194919586, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56206 < 59738; dropping {'train_loss': 0.0024016660172492266, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56216 < 59738; dropping {'train_loss': 0.21117912232875824, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56226 < 59738; dropping {'train_loss': 0.026632655411958694, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56236 < 59738; dropping {'train_loss': 0.18645420670509338, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56246 < 59738; dropping {'train_loss': 1.0955920219421387, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56256 < 59738; dropping {'train_loss': 0.12357570976018906, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56266 < 59738; dropping {'train_loss': 0.08909539878368378, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56276 < 59738; dropping {'train_loss': 0.379587858915329, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56286 < 59738; dropping {'train_loss': 0.061955247074365616, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56296 < 59738; dropping {'train_loss': 0.040235232561826706, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56306 < 59738; dropping {'train_loss': 0.07243908196687698, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56316 < 59738; dropping {'train_loss': 0.7856550812721252, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56326 < 59738; dropping {'train_loss': 0.0428658090531826, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56336 < 59738; dropping {'train_loss': 0.010950339026749134, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56346 < 59738; dropping {'train_loss': 0.013121257536113262, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56356 < 59738; dropping {'train_loss': 0.04173371568322182, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56366 < 59738; dropping {'train_loss': 1.4723882675170898, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56376 < 59738; dropping {'train_loss': 0.6493138074874878, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56386 < 59738; dropping {'train_loss': 0.010198340751230717, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56396 < 59738; dropping {'train_loss': 0.1814257800579071, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56406 < 59738; dropping {'train_loss': 0.22814080119132996, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56416 < 59738; dropping {'train_loss': 0.03169461712241173, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56426 < 59738; dropping {'train_loss': 0.041837140917778015, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56436 < 59738; dropping {'train_loss': 0.08332420140504837, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56446 < 59738; dropping {'train_loss': 0.13261590898036957, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56456 < 59738; dropping {'train_loss': 0.04670124128460884, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56466 < 59738; dropping {'train_loss': 0.035887088626623154, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56476 < 59738; dropping {'train_loss': 0.16250985860824585, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56486 < 59738; dropping {'train_loss': 0.01332898810505867, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56496 < 59738; dropping {'train_loss': 0.0045034135691821575, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56506 < 59738; dropping {'train_loss': 0.14484116435050964, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56516 < 59738; dropping {'train_loss': 0.09423574805259705, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56526 < 59738; dropping {'train_loss': 0.08326836675405502, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56536 < 59738; dropping {'train_loss': 0.01648273505270481, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56546 < 59738; dropping {'train_loss': 0.384331077337265, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56556 < 59738; dropping {'train_loss': 0.008823332376778126, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56566 < 59738; dropping {'train_loss': 0.013674573041498661, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56576 < 59738; dropping {'train_loss': 0.005487298127263784, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56586 < 59738; dropping {'train_loss': 0.015942487865686417, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56596 < 59738; dropping {'train_loss': 0.11925115436315536, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56606 < 59738; dropping {'train_loss': 0.9366244673728943, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56616 < 59738; dropping {'train_loss': 0.003988638520240784, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56626 < 59738; dropping {'train_loss': 0.015337229706346989, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56636 < 59738; dropping {'train_loss': 0.018522614613175392, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56646 < 59738; dropping {'train_loss': 0.0167466402053833, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56656 < 59738; dropping {'train_loss': 0.0054893894121050835, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56666 < 59738; dropping {'train_loss': 0.8848368525505066, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56676 < 59738; dropping {'train_loss': 0.5707522630691528, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56686 < 59738; dropping {'train_loss': 0.016709430143237114, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56696 < 59738; dropping {'train_loss': 0.008378660306334496, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56706 < 59738; dropping {'train_loss': 0.06799857318401337, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56716 < 59738; dropping {'train_loss': 0.154992014169693, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56726 < 59738; dropping {'train_loss': 0.20637129247188568, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56736 < 59738; dropping {'train_loss': 0.03342117741703987, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56746 < 59738; dropping {'train_loss': 0.04172465577721596, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56756 < 59738; dropping {'train_loss': 0.40738752484321594, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56766 < 59738; dropping {'train_loss': 0.023668350651860237, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56776 < 59738; dropping {'train_loss': 0.028772279620170593, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56786 < 59738; dropping {'train_loss': 0.005471779964864254, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56796 < 59738; dropping {'train_loss': 0.04844151437282562, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56806 < 59738; dropping {'train_loss': 0.022766225039958954, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56816 < 59738; dropping {'train_loss': 0.11978057026863098, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56826 < 59738; dropping {'train_loss': 0.028594665229320526, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56836 < 59738; dropping {'train_loss': 0.028235355392098427, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56846 < 59738; dropping {'train_loss': 0.018115989863872528, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56856 < 59738; dropping {'train_loss': 0.37818440794944763, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56866 < 59738; dropping {'train_loss': 0.03402477502822876, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56876 < 59738; dropping {'train_loss': 0.2194107323884964, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56886 < 59738; dropping {'train_loss': 0.009838241152465343, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56896 < 59738; dropping {'train_loss': 0.04385625943541527, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56906 < 59738; dropping {'train_loss': 0.004376542288810015, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56916 < 59738; dropping {'train_loss': 0.2831311523914337, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56926 < 59738; dropping {'train_loss': 0.08305834978818893, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56936 < 59738; dropping {'train_loss': 0.004026475362479687, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56946 < 59738; dropping {'train_loss': 0.009962644428014755, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56956 < 59738; dropping {'train_loss': 0.17385394871234894, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56966 < 59738; dropping {'train_loss': 0.024645758792757988, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56976 < 59738; dropping {'train_loss': 0.0033294225577265024, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56986 < 59738; dropping {'train_loss': 0.189222514629364, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 56996 < 59738; dropping {'train_loss': 0.016558092087507248, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57006 < 59738; dropping {'train_loss': 0.006152220070362091, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57016 < 59738; dropping {'train_loss': 0.046654634177684784, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57026 < 59738; dropping {'train_loss': 0.036480631679296494, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57036 < 59738; dropping {'train_loss': 0.1162642240524292, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57046 < 59738; dropping {'train_loss': 0.04180736839771271, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57056 < 59738; dropping {'train_loss': 0.016136858612298965, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57066 < 59738; dropping {'train_loss': 0.006850570905953646, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57076 < 59738; dropping {'train_loss': 0.010497469455003738, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57086 < 59738; dropping {'train_loss': 0.0071425787173211575, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57096 < 59738; dropping {'train_loss': 0.022095385938882828, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57106 < 59738; dropping {'train_loss': 2.669764757156372, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57116 < 59738; dropping {'train_loss': 0.11528630554676056, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57126 < 59738; dropping {'train_loss': 0.0422695130109787, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57136 < 59738; dropping {'train_loss': 0.006266545969992876, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57146 < 59738; dropping {'train_loss': 0.058300577104091644, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57156 < 59738; dropping {'train_loss': 0.017001017928123474, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57166 < 59738; dropping {'train_loss': 0.33035725355148315, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57176 < 59738; dropping {'train_loss': 0.04982026293873787, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57186 < 59738; dropping {'train_loss': 0.008384596556425095, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57196 < 59738; dropping {'train_loss': 0.018946165218949318, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57206 < 59738; dropping {'train_loss': 0.04561171680688858, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57216 < 59738; dropping {'train_loss': 0.043202776461839676, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57226 < 59738; dropping {'train_loss': 0.04378977045416832, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57236 < 59738; dropping {'train_loss': 0.07288151979446411, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57246 < 59738; dropping {'train_loss': 0.04631645232439041, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57256 < 59738; dropping {'train_loss': 0.0028008653316646814, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57266 < 59738; dropping {'train_loss': 0.01551563385874033, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57276 < 59738; dropping {'train_loss': 0.7436966300010681, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57286 < 59738; dropping {'train_loss': 1.1303871870040894, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57296 < 59738; dropping {'train_loss': 0.10296481102705002, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57306 < 59738; dropping {'train_loss': 0.14578767120838165, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57316 < 59738; dropping {'train_loss': 0.02199413627386093, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57326 < 59738; dropping {'train_loss': 0.06579471379518509, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57336 < 59738; dropping {'train_loss': 0.11010323464870453, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57346 < 59738; dropping {'train_loss': 0.3558536171913147, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57356 < 59738; dropping {'train_loss': 0.10504226386547089, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57366 < 59738; dropping {'train_loss': 0.12061350792646408, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57376 < 59738; dropping {'train_loss': 0.06796350330114365, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57386 < 59738; dropping {'train_loss': 0.0019245935836806893, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57396 < 59738; dropping {'train_loss': 0.019125964492559433, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57406 < 59738; dropping {'train_loss': 0.006097088567912579, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57416 < 59738; dropping {'train_loss': 0.052063606679439545, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57426 < 59738; dropping {'train_loss': 0.2537873089313507, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57436 < 59738; dropping {'train_loss': 0.1896381825208664, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57446 < 59738; dropping {'train_loss': 0.019965607672929764, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57456 < 59738; dropping {'train_loss': 0.5601610541343689, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57466 < 59738; dropping {'train_loss': 0.031068872660398483, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57476 < 59738; dropping {'train_loss': 0.08398010581731796, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57486 < 59738; dropping {'train_loss': 0.020265165716409683, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57496 < 59738; dropping {'train_loss': 0.11303671449422836, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57506 < 59738; dropping {'train_loss': 0.01850961148738861, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57516 < 59738; dropping {'train_loss': 0.05626160278916359, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57526 < 59738; dropping {'train_loss': 0.6279159188270569, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57536 < 59738; dropping {'train_loss': 0.046000879257917404, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57546 < 59738; dropping {'train_loss': 0.5579261183738708, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57556 < 59738; dropping {'train_loss': 0.09524909406900406, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57566 < 59738; dropping {'train_loss': 0.031945932656526566, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57576 < 59738; dropping {'train_loss': 0.8608518838882446, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57586 < 59738; dropping {'train_loss': 0.0689413994550705, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57596 < 59738; dropping {'train_loss': 0.028385691344738007, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57606 < 59738; dropping {'train_loss': 0.2916024327278137, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57616 < 59738; dropping {'train_loss': 0.0623287595808506, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57626 < 59738; dropping {'train_loss': 0.5760951638221741, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57636 < 59738; dropping {'train_loss': 2.563572883605957, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57646 < 59738; dropping {'train_loss': 0.014497624710202217, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57656 < 59738; dropping {'train_loss': 0.3756392002105713, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57666 < 59738; dropping {'train_loss': 0.1659523844718933, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57676 < 59738; dropping {'train_loss': 0.18721012771129608, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57686 < 59738; dropping {'train_loss': 0.006274750456213951, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57696 < 59738; dropping {'train_loss': 0.6722850799560547, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57706 < 59738; dropping {'train_loss': 0.06324680894613266, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57716 < 59738; dropping {'train_loss': 0.012315413914620876, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57726 < 59738; dropping {'train_loss': 0.00321055855602026, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57736 < 59738; dropping {'train_loss': 0.26258715987205505, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57746 < 59738; dropping {'train_loss': 0.02181091345846653, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57756 < 59738; dropping {'train_loss': 0.12456467747688293, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57766 < 59738; dropping {'train_loss': 0.09164054691791534, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57776 < 59738; dropping {'train_loss': 0.12273869663476944, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57786 < 59738; dropping {'train_loss': 0.04091639444231987, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57796 < 59738; dropping {'train_loss': 0.018664956092834473, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57806 < 59738; dropping {'train_loss': 0.034760721027851105, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57816 < 59738; dropping {'train_loss': 0.017008567228913307, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57826 < 59738; dropping {'train_loss': 0.40250805020332336, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57836 < 59738; dropping {'train_loss': 0.024235572665929794, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57846 < 59738; dropping {'train_loss': 0.4086049199104309, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57856 < 59738; dropping {'train_loss': 0.03520229458808899, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57866 < 59738; dropping {'train_loss': 0.1571945995092392, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57876 < 59738; dropping {'train_loss': 0.004545519594103098, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57886 < 59738; dropping {'train_loss': 0.08878450840711594, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57896 < 59738; dropping {'train_loss': 0.12097899615764618, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57906 < 59738; dropping {'train_loss': 0.3000510632991791, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57916 < 59738; dropping {'train_loss': 0.026274939998984337, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57926 < 59738; dropping {'train_loss': 0.2443731129169464, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57936 < 59738; dropping {'train_loss': 0.0067536961287260056, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57946 < 59738; dropping {'train_loss': 0.0437408909201622, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57956 < 59738; dropping {'train_loss': 0.06927855312824249, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57966 < 59738; dropping {'train_loss': 0.005303939338773489, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57976 < 59738; dropping {'train_loss': 0.020535573363304138, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57986 < 59738; dropping {'train_loss': 1.2730662822723389, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 57996 < 59738; dropping {'train_loss': 0.5209906101226807, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58006 < 59738; dropping {'train_loss': 0.0016324833268299699, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58016 < 59738; dropping {'train_loss': 0.10747639089822769, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58026 < 59738; dropping {'train_loss': 0.007167647127062082, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58036 < 59738; dropping {'train_loss': 0.02525554969906807, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58046 < 59738; dropping {'train_loss': 0.023764455690979958, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58056 < 59738; dropping {'train_loss': 0.12121130526065826, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58066 < 59738; dropping {'train_loss': 0.02551853656768799, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58076 < 59738; dropping {'train_loss': 0.012580927461385727, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58086 < 59738; dropping {'train_loss': 0.3037864565849304, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58096 < 59738; dropping {'train_loss': 0.17203842103481293, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58106 < 59738; dropping {'train_loss': 0.09662044793367386, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58116 < 59738; dropping {'train_loss': 0.13422951102256775, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58126 < 59738; dropping {'train_loss': 0.02590281516313553, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58136 < 59738; dropping {'train_loss': 0.006937636528164148, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58146 < 59738; dropping {'train_loss': 0.17458881437778473, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58156 < 59738; dropping {'train_loss': 0.019976016134023666, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58166 < 59738; dropping {'train_loss': 0.16463248431682587, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58176 < 59738; dropping {'train_loss': 0.032160770148038864, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58186 < 59738; dropping {'train_loss': 0.1572580337524414, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58196 < 59738; dropping {'train_loss': 0.18242277204990387, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58206 < 59738; dropping {'train_loss': 0.056751471012830734, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58216 < 59738; dropping {'train_loss': 0.012091866694390774, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58226 < 59738; dropping {'train_loss': 0.05671289563179016, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58236 < 59738; dropping {'train_loss': 0.2599242925643921, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58246 < 59738; dropping {'train_loss': 0.021349603310227394, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58256 < 59738; dropping {'train_loss': 0.050153493881225586, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58266 < 59738; dropping {'train_loss': 0.3552221655845642, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58276 < 59738; dropping {'train_loss': 0.16446812450885773, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58286 < 59738; dropping {'train_loss': 0.11183460801839828, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58296 < 59738; dropping {'train_loss': 0.19246993958950043, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58306 < 59738; dropping {'train_loss': 0.0105070685967803, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58316 < 59738; dropping {'train_loss': 0.007529826369136572, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58326 < 59738; dropping {'train_loss': 0.010751059278845787, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58336 < 59738; dropping {'train_loss': 0.006928198039531708, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58346 < 59738; dropping {'train_loss': 0.01627253368496895, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58356 < 59738; dropping {'train_loss': 0.032027002424001694, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58366 < 59738; dropping {'train_loss': 0.016971377655863762, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58376 < 59738; dropping {'train_loss': 0.06580040603876114, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58386 < 59738; dropping {'train_loss': 0.007876711897552013, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58396 < 59738; dropping {'train_loss': 0.14001092314720154, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58406 < 59738; dropping {'train_loss': 0.031852301210165024, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58416 < 59738; dropping {'train_loss': 0.010795858688652515, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58426 < 59738; dropping {'train_loss': 0.04148275777697563, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58436 < 59738; dropping {'train_loss': 0.11833097785711288, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58446 < 59738; dropping {'train_loss': 0.034376900643110275, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58456 < 59738; dropping {'train_loss': 0.24757161736488342, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58466 < 59738; dropping {'train_loss': 0.007213802542537451, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58476 < 59738; dropping {'train_loss': 0.007386936340481043, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58486 < 59738; dropping {'train_loss': 0.3759949505329132, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58496 < 59738; dropping {'train_loss': 1.0682768821716309, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58506 < 59738; dropping {'train_loss': 0.010211065411567688, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58516 < 59738; dropping {'train_loss': 0.0072777546010911465, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58526 < 59738; dropping {'train_loss': 0.007714994251728058, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58536 < 59738; dropping {'train_loss': 0.09228356182575226, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58546 < 59738; dropping {'train_loss': 0.09010185301303864, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58556 < 59738; dropping {'train_loss': 0.28300759196281433, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58566 < 59738; dropping {'train_loss': 0.10778620094060898, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58576 < 59738; dropping {'train_loss': 0.01417290698736906, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58586 < 59738; dropping {'train_loss': 0.029428426176309586, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58596 < 59738; dropping {'train_loss': 0.0030424923170357943, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58606 < 59738; dropping {'train_loss': 0.1380947381258011, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58616 < 59738; dropping {'train_loss': 2.8287668228149414, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58626 < 59738; dropping {'train_loss': 0.009868822991847992, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58636 < 59738; dropping {'train_loss': 0.013848730362951756, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58646 < 59738; dropping {'train_loss': 0.04304865375161171, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58656 < 59738; dropping {'train_loss': 0.04494566097855568, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58666 < 59738; dropping {'train_loss': 0.017206745222210884, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58676 < 59738; dropping {'train_loss': 0.030507199466228485, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58686 < 59738; dropping {'train_loss': 0.026438429951667786, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58696 < 59738; dropping {'train_loss': 0.03795136138796806, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58706 < 59738; dropping {'train_loss': 0.22445866465568542, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58716 < 59738; dropping {'train_loss': 0.1315888613462448, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58726 < 59738; dropping {'train_loss': 0.24070990085601807, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58736 < 59738; dropping {'train_loss': 0.10710027813911438, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58746 < 59738; dropping {'train_loss': 0.03734096139669418, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58756 < 59738; dropping {'train_loss': 0.004218412563204765, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58766 < 59738; dropping {'train_loss': 0.14223499596118927, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58776 < 59738; dropping {'train_loss': 0.09982726722955704, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58786 < 59738; dropping {'train_loss': 0.02730395831167698, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58796 < 59738; dropping {'train_loss': 0.12925265729427338, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58806 < 59738; dropping {'train_loss': 0.019811108708381653, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58816 < 59738; dropping {'train_loss': 0.8192882537841797, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58826 < 59738; dropping {'train_loss': 0.023943504318594933, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58836 < 59738; dropping {'train_loss': 0.1215313971042633, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58846 < 59738; dropping {'train_loss': 0.021492984145879745, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58856 < 59738; dropping {'train_loss': 0.12365111708641052, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58866 < 59738; dropping {'train_loss': 0.15751226246356964, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58876 < 59738; dropping {'train_loss': 0.17994891107082367, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58886 < 59738; dropping {'train_loss': 0.01377753634005785, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58896 < 59738; dropping {'train_loss': 0.0843716636300087, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58906 < 59738; dropping {'train_loss': 0.12426801025867462, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58916 < 59738; dropping {'train_loss': 0.017580337822437286, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58926 < 59738; dropping {'train_loss': 0.022919787093997, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58936 < 59738; dropping {'train_loss': 0.05342136323451996, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58946 < 59738; dropping {'train_loss': 0.0038547462318092585, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58956 < 59738; dropping {'train_loss': 0.07239006459712982, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58966 < 59738; dropping {'train_loss': 0.07682979106903076, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58976 < 59738; dropping {'train_loss': 0.006689386907964945, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58986 < 59738; dropping {'train_loss': 1.6124221086502075, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 58996 < 59738; dropping {'train_loss': 3.3518764972686768, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59006 < 59738; dropping {'train_loss': 0.016803700476884842, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59016 < 59738; dropping {'train_loss': 0.11876031011343002, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59026 < 59738; dropping {'train_loss': 0.04956797510385513, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59036 < 59738; dropping {'train_loss': 0.007566124200820923, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59046 < 59738; dropping {'train_loss': 0.694704532623291, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59056 < 59738; dropping {'train_loss': 0.31689611077308655, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59066 < 59738; dropping {'train_loss': 0.05580270662903786, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59076 < 59738; dropping {'train_loss': 0.006591969169676304, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59086 < 59738; dropping {'train_loss': 0.22449654340744019, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59096 < 59738; dropping {'train_loss': 0.018733078613877296, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59106 < 59738; dropping {'train_loss': 0.09198486804962158, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59116 < 59738; dropping {'train_loss': 0.004101064521819353, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59126 < 59738; dropping {'train_loss': 0.0033859838731586933, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59136 < 59738; dropping {'train_loss': 0.002148645231500268, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59146 < 59738; dropping {'train_loss': 0.07405954599380493, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59156 < 59738; dropping {'train_loss': 0.06344626098871231, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59166 < 59738; dropping {'train_loss': 0.23541682958602905, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59176 < 59738; dropping {'train_loss': 0.12705232203006744, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59186 < 59738; dropping {'train_loss': 0.029160911217331886, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59196 < 59738; dropping {'train_loss': 0.003930447157472372, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59206 < 59738; dropping {'train_loss': 0.09265495091676712, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59216 < 59738; dropping {'train_loss': 0.040304768830537796, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59226 < 59738; dropping {'train_loss': 0.01418081484735012, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59236 < 59738; dropping {'train_loss': 0.07806047052145004, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59246 < 59738; dropping {'train_loss': 0.13079653680324554, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59256 < 59738; dropping {'train_loss': 0.002346884226426482, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59266 < 59738; dropping {'train_loss': 0.0036370602902024984, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59276 < 59738; dropping {'train_loss': 0.00448241364210844, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59286 < 59738; dropping {'train_loss': 0.055176861584186554, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59296 < 59738; dropping {'train_loss': 0.02001773752272129, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59306 < 59738; dropping {'train_loss': 0.04754973575472832, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59316 < 59738; dropping {'train_loss': 0.0037924039643257856, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59326 < 59738; dropping {'train_loss': 0.0029496399220079184, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59336 < 59738; dropping {'train_loss': 0.03210761770606041, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59346 < 59738; dropping {'train_loss': 0.0018752305768430233, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59356 < 59738; dropping {'train_loss': 0.04106813669204712, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59366 < 59738; dropping {'train_loss': 0.020502053201198578, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59376 < 59738; dropping {'train_loss': 0.017051734030246735, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59386 < 59738; dropping {'train_loss': 0.09506283700466156, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59396 < 59738; dropping {'train_loss': 0.07229841500520706, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59406 < 59738; dropping {'train_loss': 0.19016936421394348, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59416 < 59738; dropping {'train_loss': 0.18976394832134247, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59426 < 59738; dropping {'train_loss': 0.04698578268289566, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59436 < 59738; dropping {'train_loss': 0.050824686884880066, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59446 < 59738; dropping {'train_loss': 0.011273451149463654, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59456 < 59738; dropping {'train_loss': 0.05099882930517197, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59466 < 59738; dropping {'train_loss': 0.014396041631698608, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59476 < 59738; dropping {'train_loss': 0.006377342157065868, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59486 < 59738; dropping {'train_loss': 0.33476686477661133, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59496 < 59738; dropping {'train_loss': 0.01359396893531084, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59506 < 59738; dropping {'train_loss': 0.0631057620048523, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59516 < 59738; dropping {'train_loss': 0.006645764224231243, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59526 < 59738; dropping {'train_loss': 0.01507702935487032, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59536 < 59738; dropping {'train_loss': 0.0025601075030863285, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59546 < 59738; dropping {'train_loss': 0.025198588147759438, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59556 < 59738; dropping {'train_loss': 0.015328931622207165, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59566 < 59738; dropping {'train_loss': 0.0191098153591156, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59576 < 59738; dropping {'train_loss': 0.008875899948179722, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59586 < 59738; dropping {'train_loss': 0.005082383286207914, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59596 < 59738; dropping {'train_loss': 0.02946864441037178, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59606 < 59738; dropping {'train_loss': 0.0057268948294222355, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59616 < 59738; dropping {'train_loss': 0.004370091017335653, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59626 < 59738; dropping {'train_loss': 0.0051596565172076225, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59636 < 59738; dropping {'train_loss': 0.0627608373761177, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59646 < 59738; dropping {'train_loss': 0.01359359361231327, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59656 < 59738; dropping {'train_loss': 0.010411633178591728, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59666 < 59738; dropping {'train_loss': 0.005047784186899662, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59676 < 59738; dropping {'train_loss': 0.032295070588588715, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59686 < 59738; dropping {'train_loss': 0.06184907257556915, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59696 < 59738; dropping {'train_loss': 0.05042865872383118, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59706 < 59738; dropping {'train_loss': 0.03724189102649689, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59716 < 59738; dropping {'train_loss': 0.16370698809623718, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59726 < 59738; dropping {'train_loss': 0.026995185762643814, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 59736 < 59738; dropping {'train_loss': 0.01852126605808735, 'epoch': 3}.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a147bc9ed6c5424a87c49f040795e7a5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71687 < 77660; dropping {'val_epoch_loss': 0.2502492666244507, 'val_epoch_auc': 0.9933857892547169, 'epoch': 3}.\n",
            "\n",
            "Epoch 00003: val_epoch_auc  was not in top 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71688 < 77660; dropping {'train_epoch_loss': 0.1764739602804184, 'epoch': 3}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71688 < 77660; dropping {'train_loss': 0.015067234635353088, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71698 < 77660; dropping {'train_loss': 0.018068820238113403, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71708 < 77660; dropping {'train_loss': 0.002450789324939251, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71718 < 77660; dropping {'train_loss': 0.12131666392087936, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71728 < 77660; dropping {'train_loss': 0.019541151821613312, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71738 < 77660; dropping {'train_loss': 0.02274826169013977, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71748 < 77660; dropping {'train_loss': 0.4402998089790344, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71758 < 77660; dropping {'train_loss': 0.03928513079881668, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71768 < 77660; dropping {'train_loss': 0.01882287859916687, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71778 < 77660; dropping {'train_loss': 0.056261587888002396, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71788 < 77660; dropping {'train_loss': 0.09007686376571655, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71798 < 77660; dropping {'train_loss': 0.10292749106884003, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71808 < 77660; dropping {'train_loss': 0.10277687758207321, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71818 < 77660; dropping {'train_loss': 2.0252058506011963, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71828 < 77660; dropping {'train_loss': 0.0023395579773932695, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71838 < 77660; dropping {'train_loss': 0.0029299359302967787, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71848 < 77660; dropping {'train_loss': 0.0017534293001517653, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71858 < 77660; dropping {'train_loss': 0.018137576058506966, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71868 < 77660; dropping {'train_loss': 0.0030851331539452076, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71878 < 77660; dropping {'train_loss': 0.0027923628222197294, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71888 < 77660; dropping {'train_loss': 0.153924360871315, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71898 < 77660; dropping {'train_loss': 0.0021743474062532187, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71908 < 77660; dropping {'train_loss': 0.008563184179365635, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71918 < 77660; dropping {'train_loss': 0.009961163625121117, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71928 < 77660; dropping {'train_loss': 0.0027392723131924868, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71938 < 77660; dropping {'train_loss': 0.004507683217525482, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71948 < 77660; dropping {'train_loss': 0.006536088418215513, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71958 < 77660; dropping {'train_loss': 0.0542369969189167, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71968 < 77660; dropping {'train_loss': 0.016473326832056046, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71978 < 77660; dropping {'train_loss': 0.004706409759819508, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71988 < 77660; dropping {'train_loss': 0.007561751175671816, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 71998 < 77660; dropping {'train_loss': 0.0024655915331095457, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72008 < 77660; dropping {'train_loss': 0.005294174887239933, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72018 < 77660; dropping {'train_loss': 0.002964874031022191, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72028 < 77660; dropping {'train_loss': 0.08080600202083588, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72038 < 77660; dropping {'train_loss': 0.06434355676174164, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72048 < 77660; dropping {'train_loss': 0.06087986379861832, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72058 < 77660; dropping {'train_loss': 0.025166047737002373, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72068 < 77660; dropping {'train_loss': 0.035210415720939636, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72078 < 77660; dropping {'train_loss': 0.002481052652001381, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72088 < 77660; dropping {'train_loss': 0.005940742790699005, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72098 < 77660; dropping {'train_loss': 0.029882874339818954, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72108 < 77660; dropping {'train_loss': 0.0032654735259711742, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72118 < 77660; dropping {'train_loss': 0.33500880002975464, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72128 < 77660; dropping {'train_loss': 0.00407040910795331, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72138 < 77660; dropping {'train_loss': 0.04477087780833244, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72148 < 77660; dropping {'train_loss': 0.005106416996568441, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72158 < 77660; dropping {'train_loss': 0.021692071110010147, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72168 < 77660; dropping {'train_loss': 0.005651358515024185, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72178 < 77660; dropping {'train_loss': 0.004949198570102453, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72188 < 77660; dropping {'train_loss': 0.010831212624907494, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72198 < 77660; dropping {'train_loss': 0.012844903394579887, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72208 < 77660; dropping {'train_loss': 0.04472578689455986, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72218 < 77660; dropping {'train_loss': 0.028422165662050247, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72228 < 77660; dropping {'train_loss': 0.04680708423256874, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72238 < 77660; dropping {'train_loss': 0.0038266549818217754, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72248 < 77660; dropping {'train_loss': 0.005135347601026297, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72258 < 77660; dropping {'train_loss': 0.02417956106364727, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72268 < 77660; dropping {'train_loss': 0.010983878746628761, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72278 < 77660; dropping {'train_loss': 0.04166825860738754, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72288 < 77660; dropping {'train_loss': 0.00468300748616457, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72298 < 77660; dropping {'train_loss': 0.026777539402246475, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72308 < 77660; dropping {'train_loss': 0.04811825603246689, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72318 < 77660; dropping {'train_loss': 0.0017687667859718204, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72328 < 77660; dropping {'train_loss': 0.47158604860305786, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72338 < 77660; dropping {'train_loss': 0.19182531535625458, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72348 < 77660; dropping {'train_loss': 0.8466734886169434, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72358 < 77660; dropping {'train_loss': 0.062339212745428085, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72368 < 77660; dropping {'train_loss': 0.005463381297886372, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72378 < 77660; dropping {'train_loss': 0.06886129081249237, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72388 < 77660; dropping {'train_loss': 0.005824666004627943, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72398 < 77660; dropping {'train_loss': 0.04598182812333107, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72408 < 77660; dropping {'train_loss': 0.31480324268341064, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72418 < 77660; dropping {'train_loss': 0.12731440365314484, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72428 < 77660; dropping {'train_loss': 0.8648761510848999, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72438 < 77660; dropping {'train_loss': 0.017818819731473923, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72448 < 77660; dropping {'train_loss': 0.4658406972885132, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72458 < 77660; dropping {'train_loss': 0.02816060371696949, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72468 < 77660; dropping {'train_loss': 0.1987408846616745, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72478 < 77660; dropping {'train_loss': 0.5988227725028992, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72488 < 77660; dropping {'train_loss': 0.17173771560192108, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72498 < 77660; dropping {'train_loss': 0.16789761185646057, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72508 < 77660; dropping {'train_loss': 0.07523360848426819, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72518 < 77660; dropping {'train_loss': 0.046422217041254044, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72528 < 77660; dropping {'train_loss': 0.006128940265625715, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72538 < 77660; dropping {'train_loss': 0.008472848683595657, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72548 < 77660; dropping {'train_loss': 0.00711025670170784, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72558 < 77660; dropping {'train_loss': 0.03701220825314522, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72568 < 77660; dropping {'train_loss': 0.004352770745754242, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72578 < 77660; dropping {'train_loss': 0.2380737066268921, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72588 < 77660; dropping {'train_loss': 0.003734806552529335, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72598 < 77660; dropping {'train_loss': 0.0038577336817979813, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72608 < 77660; dropping {'train_loss': 0.014427942223846912, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72618 < 77660; dropping {'train_loss': 0.10511438548564911, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72628 < 77660; dropping {'train_loss': 0.5353755354881287, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72638 < 77660; dropping {'train_loss': 0.05306915566325188, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72648 < 77660; dropping {'train_loss': 0.024071287363767624, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72658 < 77660; dropping {'train_loss': 0.02262701466679573, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72668 < 77660; dropping {'train_loss': 0.054802533239126205, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72678 < 77660; dropping {'train_loss': 0.3876146674156189, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72688 < 77660; dropping {'train_loss': 0.18735162913799286, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72698 < 77660; dropping {'train_loss': 0.711892306804657, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72708 < 77660; dropping {'train_loss': 0.0030092305969446898, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72718 < 77660; dropping {'train_loss': 0.028917185962200165, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72728 < 77660; dropping {'train_loss': 0.14791718125343323, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72738 < 77660; dropping {'train_loss': 0.14701737463474274, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72748 < 77660; dropping {'train_loss': 0.0036608928348869085, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72758 < 77660; dropping {'train_loss': 0.01718105748295784, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72768 < 77660; dropping {'train_loss': 0.055281348526477814, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72778 < 77660; dropping {'train_loss': 0.03342407941818237, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72788 < 77660; dropping {'train_loss': 0.031908269971609116, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72798 < 77660; dropping {'train_loss': 0.007111776620149612, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72808 < 77660; dropping {'train_loss': 0.03528101369738579, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72818 < 77660; dropping {'train_loss': 0.12013626098632812, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72828 < 77660; dropping {'train_loss': 0.02714107558131218, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72838 < 77660; dropping {'train_loss': 0.0025521579664200544, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72848 < 77660; dropping {'train_loss': 0.0030787268187850714, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72858 < 77660; dropping {'train_loss': 0.004673773422837257, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72868 < 77660; dropping {'train_loss': 0.0033938027918338776, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72878 < 77660; dropping {'train_loss': 0.0032444284297525883, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72888 < 77660; dropping {'train_loss': 0.050113506615161896, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72898 < 77660; dropping {'train_loss': 0.023130111396312714, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72908 < 77660; dropping {'train_loss': 0.07517567276954651, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72918 < 77660; dropping {'train_loss': 0.016833042725920677, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72928 < 77660; dropping {'train_loss': 0.03738357126712799, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72938 < 77660; dropping {'train_loss': 0.0628187283873558, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72948 < 77660; dropping {'train_loss': 0.017178205773234367, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72958 < 77660; dropping {'train_loss': 0.04224620759487152, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72968 < 77660; dropping {'train_loss': 0.024482542648911476, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72978 < 77660; dropping {'train_loss': 0.005052359774708748, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72988 < 77660; dropping {'train_loss': 0.006068701855838299, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 72998 < 77660; dropping {'train_loss': 0.005198396276682615, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73008 < 77660; dropping {'train_loss': 0.002990970155224204, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73018 < 77660; dropping {'train_loss': 0.02608690969645977, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73028 < 77660; dropping {'train_loss': 0.04367148503661156, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73038 < 77660; dropping {'train_loss': 0.020092615857720375, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73048 < 77660; dropping {'train_loss': 0.022107211872935295, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73058 < 77660; dropping {'train_loss': 0.10022350400686264, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73068 < 77660; dropping {'train_loss': 0.0399588942527771, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73078 < 77660; dropping {'train_loss': 0.018977375701069832, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73088 < 77660; dropping {'train_loss': 0.0016456814482808113, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73098 < 77660; dropping {'train_loss': 0.0069000739604234695, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73108 < 77660; dropping {'train_loss': 0.07701517641544342, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73118 < 77660; dropping {'train_loss': 0.004389107692986727, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73128 < 77660; dropping {'train_loss': 0.021720929071307182, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73138 < 77660; dropping {'train_loss': 0.006186885759234428, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73148 < 77660; dropping {'train_loss': 0.005396557040512562, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73158 < 77660; dropping {'train_loss': 0.018405741080641747, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73168 < 77660; dropping {'train_loss': 0.0070586116053164005, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73178 < 77660; dropping {'train_loss': 0.052893251180648804, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73188 < 77660; dropping {'train_loss': 0.002728505525738001, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73198 < 77660; dropping {'train_loss': 0.0026805871166288853, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73208 < 77660; dropping {'train_loss': 0.12972502410411835, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73218 < 77660; dropping {'train_loss': 0.011435209773480892, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73228 < 77660; dropping {'train_loss': 0.01873980462551117, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73238 < 77660; dropping {'train_loss': 0.013352824375033379, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73248 < 77660; dropping {'train_loss': 0.005244048312306404, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73258 < 77660; dropping {'train_loss': 0.11850007623434067, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73268 < 77660; dropping {'train_loss': 0.05974472314119339, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73278 < 77660; dropping {'train_loss': 0.013324255123734474, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73288 < 77660; dropping {'train_loss': 0.020047081634402275, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73298 < 77660; dropping {'train_loss': 0.1604672372341156, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73308 < 77660; dropping {'train_loss': 0.03288079798221588, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73318 < 77660; dropping {'train_loss': 0.005154200829565525, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73328 < 77660; dropping {'train_loss': 0.008291536942124367, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73338 < 77660; dropping {'train_loss': 0.016992798075079918, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73348 < 77660; dropping {'train_loss': 0.002612723968923092, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73358 < 77660; dropping {'train_loss': 0.02662881650030613, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73368 < 77660; dropping {'train_loss': 0.005605516489595175, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73378 < 77660; dropping {'train_loss': 0.05006532371044159, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73388 < 77660; dropping {'train_loss': 0.006402464117854834, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73398 < 77660; dropping {'train_loss': 0.007134620100259781, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73408 < 77660; dropping {'train_loss': 0.13068266212940216, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73418 < 77660; dropping {'train_loss': 0.02771931141614914, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73428 < 77660; dropping {'train_loss': 0.04563302919268608, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73438 < 77660; dropping {'train_loss': 0.1860124170780182, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73448 < 77660; dropping {'train_loss': 0.012727678753435612, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73458 < 77660; dropping {'train_loss': 0.0016780777368694544, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73468 < 77660; dropping {'train_loss': 0.004989472683519125, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73478 < 77660; dropping {'train_loss': 0.08851877599954605, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73488 < 77660; dropping {'train_loss': 0.004086097702383995, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73498 < 77660; dropping {'train_loss': 0.005483374930918217, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73508 < 77660; dropping {'train_loss': 0.0065454174764454365, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73518 < 77660; dropping {'train_loss': 0.004736782982945442, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73528 < 77660; dropping {'train_loss': 0.0027559613808989525, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73538 < 77660; dropping {'train_loss': 0.002264136215671897, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73548 < 77660; dropping {'train_loss': 0.04700730741024017, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73558 < 77660; dropping {'train_loss': 1.957150936126709, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73568 < 77660; dropping {'train_loss': 0.025096803903579712, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73578 < 77660; dropping {'train_loss': 0.011274892836809158, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73588 < 77660; dropping {'train_loss': 0.18897154927253723, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73598 < 77660; dropping {'train_loss': 0.005069486331194639, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73608 < 77660; dropping {'train_loss': 0.018888922408223152, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73618 < 77660; dropping {'train_loss': 0.014864613302052021, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73628 < 77660; dropping {'train_loss': 0.0016326135955750942, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73638 < 77660; dropping {'train_loss': 0.15128575265407562, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73648 < 77660; dropping {'train_loss': 0.36908194422721863, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73658 < 77660; dropping {'train_loss': 0.003428178606554866, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73668 < 77660; dropping {'train_loss': 0.007614247500896454, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73678 < 77660; dropping {'train_loss': 0.056274790316820145, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73688 < 77660; dropping {'train_loss': 0.0015153952408581972, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73698 < 77660; dropping {'train_loss': 0.05874175950884819, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73708 < 77660; dropping {'train_loss': 0.006136155221611261, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73718 < 77660; dropping {'train_loss': 0.30436888337135315, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73728 < 77660; dropping {'train_loss': 0.020408695563673973, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73738 < 77660; dropping {'train_loss': 0.1983739584684372, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73748 < 77660; dropping {'train_loss': 0.06551939994096756, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73758 < 77660; dropping {'train_loss': 0.01317511685192585, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73768 < 77660; dropping {'train_loss': 0.019149521365761757, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73778 < 77660; dropping {'train_loss': 0.005936920642852783, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73788 < 77660; dropping {'train_loss': 0.003738213097676635, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73798 < 77660; dropping {'train_loss': 1.0675384998321533, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73808 < 77660; dropping {'train_loss': 0.03423435613512993, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73818 < 77660; dropping {'train_loss': 0.2068677395582199, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73828 < 77660; dropping {'train_loss': 0.07674235105514526, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73838 < 77660; dropping {'train_loss': 0.018507855013012886, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73848 < 77660; dropping {'train_loss': 0.02800448052585125, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73858 < 77660; dropping {'train_loss': 0.03615536913275719, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73868 < 77660; dropping {'train_loss': 0.2580357789993286, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73878 < 77660; dropping {'train_loss': 0.006328166928142309, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73888 < 77660; dropping {'train_loss': 0.11597716808319092, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73898 < 77660; dropping {'train_loss': 0.029366470873355865, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73908 < 77660; dropping {'train_loss': 0.10991033166646957, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73918 < 77660; dropping {'train_loss': 0.00503817992284894, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73928 < 77660; dropping {'train_loss': 0.004105583298951387, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73938 < 77660; dropping {'train_loss': 0.026685437187552452, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73948 < 77660; dropping {'train_loss': 0.027920741587877274, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73958 < 77660; dropping {'train_loss': 0.8771644234657288, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73968 < 77660; dropping {'train_loss': 0.0080258185043931, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73978 < 77660; dropping {'train_loss': 0.5296887159347534, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73988 < 77660; dropping {'train_loss': 0.010669179260730743, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 73998 < 77660; dropping {'train_loss': 0.3698234558105469, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74008 < 77660; dropping {'train_loss': 0.007805599831044674, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74018 < 77660; dropping {'train_loss': 0.016398141160607338, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74028 < 77660; dropping {'train_loss': 0.038220662623643875, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74038 < 77660; dropping {'train_loss': 0.0016410360112786293, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74048 < 77660; dropping {'train_loss': 1.0746655464172363, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74058 < 77660; dropping {'train_loss': 0.004870878532528877, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74068 < 77660; dropping {'train_loss': 0.004101061262190342, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74078 < 77660; dropping {'train_loss': 0.47668588161468506, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74088 < 77660; dropping {'train_loss': 0.03920987620949745, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74098 < 77660; dropping {'train_loss': 0.29954007267951965, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74108 < 77660; dropping {'train_loss': 0.09809087216854095, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74118 < 77660; dropping {'train_loss': 0.0023722145706415176, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74128 < 77660; dropping {'train_loss': 0.003652449231594801, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74138 < 77660; dropping {'train_loss': 0.0046200817450881, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74148 < 77660; dropping {'train_loss': 0.01882495917379856, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74158 < 77660; dropping {'train_loss': 0.013333948329091072, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74168 < 77660; dropping {'train_loss': 0.004123317543417215, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74178 < 77660; dropping {'train_loss': 0.06241145357489586, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74188 < 77660; dropping {'train_loss': 0.1136951744556427, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74198 < 77660; dropping {'train_loss': 0.011699809692800045, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74208 < 77660; dropping {'train_loss': 0.006200440693646669, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74218 < 77660; dropping {'train_loss': 0.0024627945385873318, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74228 < 77660; dropping {'train_loss': 1.3356971740722656, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74238 < 77660; dropping {'train_loss': 0.08071304112672806, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74248 < 77660; dropping {'train_loss': 0.0065466719679534435, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74258 < 77660; dropping {'train_loss': 0.049832820892333984, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74268 < 77660; dropping {'train_loss': 0.006996299140155315, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74278 < 77660; dropping {'train_loss': 0.05353672802448273, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74288 < 77660; dropping {'train_loss': 0.021143628284335136, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74298 < 77660; dropping {'train_loss': 0.5830844044685364, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74308 < 77660; dropping {'train_loss': 0.004228987265378237, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74318 < 77660; dropping {'train_loss': 0.005852631293237209, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74328 < 77660; dropping {'train_loss': 0.020948534831404686, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74338 < 77660; dropping {'train_loss': 0.016862928867340088, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74348 < 77660; dropping {'train_loss': 0.0028989508282393217, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74358 < 77660; dropping {'train_loss': 0.004238378256559372, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74368 < 77660; dropping {'train_loss': 0.072736956179142, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74378 < 77660; dropping {'train_loss': 0.17182807624340057, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74388 < 77660; dropping {'train_loss': 0.13929519057273865, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74398 < 77660; dropping {'train_loss': 0.002848193980753422, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74408 < 77660; dropping {'train_loss': 0.0038534209597855806, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74418 < 77660; dropping {'train_loss': 0.5079647898674011, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74428 < 77660; dropping {'train_loss': 0.1602247804403305, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74438 < 77660; dropping {'train_loss': 0.03125925734639168, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74448 < 77660; dropping {'train_loss': 0.7088326811790466, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74458 < 77660; dropping {'train_loss': 0.3724069893360138, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74468 < 77660; dropping {'train_loss': 0.535707950592041, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74478 < 77660; dropping {'train_loss': 0.0040791709907352924, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74488 < 77660; dropping {'train_loss': 0.016229966655373573, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74498 < 77660; dropping {'train_loss': 0.035082533955574036, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74508 < 77660; dropping {'train_loss': 0.015624375082552433, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74518 < 77660; dropping {'train_loss': 0.011780022643506527, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74528 < 77660; dropping {'train_loss': 0.00988836120814085, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74538 < 77660; dropping {'train_loss': 0.02888818271458149, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74548 < 77660; dropping {'train_loss': 1.2361997365951538, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74558 < 77660; dropping {'train_loss': 0.02648446150124073, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74568 < 77660; dropping {'train_loss': 0.004987485706806183, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74578 < 77660; dropping {'train_loss': 0.022964291274547577, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74588 < 77660; dropping {'train_loss': 0.031437963247299194, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74598 < 77660; dropping {'train_loss': 0.21780318021774292, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74608 < 77660; dropping {'train_loss': 0.33177968859672546, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74618 < 77660; dropping {'train_loss': 0.011423476971685886, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74628 < 77660; dropping {'train_loss': 0.007588629610836506, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74638 < 77660; dropping {'train_loss': 0.06695602089166641, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74648 < 77660; dropping {'train_loss': 0.1729612648487091, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74658 < 77660; dropping {'train_loss': 0.5257537364959717, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74668 < 77660; dropping {'train_loss': 0.053489115089178085, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74678 < 77660; dropping {'train_loss': 0.2405109852552414, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74688 < 77660; dropping {'train_loss': 0.03323250263929367, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74698 < 77660; dropping {'train_loss': 0.01608630083501339, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74708 < 77660; dropping {'train_loss': 0.03341013565659523, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74718 < 77660; dropping {'train_loss': 0.07452253997325897, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74728 < 77660; dropping {'train_loss': 0.26640570163726807, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74738 < 77660; dropping {'train_loss': 0.03405812010169029, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74748 < 77660; dropping {'train_loss': 0.004578855820000172, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74758 < 77660; dropping {'train_loss': 3.673232316970825, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74768 < 77660; dropping {'train_loss': 0.057355914264917374, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74778 < 77660; dropping {'train_loss': 0.015069179236888885, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74788 < 77660; dropping {'train_loss': 0.018675517290830612, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74798 < 77660; dropping {'train_loss': 0.08838694542646408, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74808 < 77660; dropping {'train_loss': 0.0038236379623413086, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74818 < 77660; dropping {'train_loss': 0.06349287182092667, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74828 < 77660; dropping {'train_loss': 0.2197834700345993, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74838 < 77660; dropping {'train_loss': 0.30379658937454224, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74848 < 77660; dropping {'train_loss': 0.017011858522892, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74858 < 77660; dropping {'train_loss': 1.1390224695205688, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74868 < 77660; dropping {'train_loss': 0.43848875164985657, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74878 < 77660; dropping {'train_loss': 0.045552100986242294, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74888 < 77660; dropping {'train_loss': 0.26661932468414307, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74898 < 77660; dropping {'train_loss': 0.27021026611328125, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74908 < 77660; dropping {'train_loss': 0.012763130478560925, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74918 < 77660; dropping {'train_loss': 0.05064677447080612, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74928 < 77660; dropping {'train_loss': 0.1584111601114273, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74938 < 77660; dropping {'train_loss': 0.01087632030248642, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74948 < 77660; dropping {'train_loss': 0.04929739609360695, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74958 < 77660; dropping {'train_loss': 0.022958287969231606, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74968 < 77660; dropping {'train_loss': 0.0071378545835614204, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74978 < 77660; dropping {'train_loss': 0.0038269590586423874, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74988 < 77660; dropping {'train_loss': 0.007953238673508167, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 74998 < 77660; dropping {'train_loss': 0.09061887860298157, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75008 < 77660; dropping {'train_loss': 0.015402461402118206, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75018 < 77660; dropping {'train_loss': 0.015565453097224236, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75028 < 77660; dropping {'train_loss': 0.03818491846323013, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75038 < 77660; dropping {'train_loss': 0.020015796646475792, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75048 < 77660; dropping {'train_loss': 0.010523132979869843, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75058 < 77660; dropping {'train_loss': 0.1241089254617691, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75068 < 77660; dropping {'train_loss': 0.05976998060941696, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75078 < 77660; dropping {'train_loss': 0.08252543210983276, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75088 < 77660; dropping {'train_loss': 0.055018357932567596, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75098 < 77660; dropping {'train_loss': 0.6165302991867065, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75108 < 77660; dropping {'train_loss': 0.0050246198661625385, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75118 < 77660; dropping {'train_loss': 0.5929944515228271, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75128 < 77660; dropping {'train_loss': 0.010282041504979134, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75138 < 77660; dropping {'train_loss': 0.03115878626704216, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75148 < 77660; dropping {'train_loss': 0.060370005667209625, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75158 < 77660; dropping {'train_loss': 0.11731810867786407, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75168 < 77660; dropping {'train_loss': 0.026739921420812607, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75178 < 77660; dropping {'train_loss': 0.5164303779602051, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75188 < 77660; dropping {'train_loss': 0.04038587212562561, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75198 < 77660; dropping {'train_loss': 0.0445764996111393, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75208 < 77660; dropping {'train_loss': 0.0041772727854549885, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75218 < 77660; dropping {'train_loss': 0.20898066461086273, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75228 < 77660; dropping {'train_loss': 0.03061845898628235, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75238 < 77660; dropping {'train_loss': 0.0025715725496411324, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75248 < 77660; dropping {'train_loss': 0.00542446319013834, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75258 < 77660; dropping {'train_loss': 0.007664112839847803, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75268 < 77660; dropping {'train_loss': 0.001138427876867354, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75278 < 77660; dropping {'train_loss': 1.033869743347168, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75288 < 77660; dropping {'train_loss': 0.00533966114744544, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75298 < 77660; dropping {'train_loss': 0.034643255174160004, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75308 < 77660; dropping {'train_loss': 0.0043490552343428135, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75318 < 77660; dropping {'train_loss': 0.01512239221483469, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75328 < 77660; dropping {'train_loss': 0.004573511891067028, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75338 < 77660; dropping {'train_loss': 0.09416832774877548, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75348 < 77660; dropping {'train_loss': 0.005969024728983641, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75358 < 77660; dropping {'train_loss': 0.064320869743824, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75368 < 77660; dropping {'train_loss': 0.20922896265983582, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75378 < 77660; dropping {'train_loss': 0.10200747847557068, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75388 < 77660; dropping {'train_loss': 0.19493956863880157, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75398 < 77660; dropping {'train_loss': 0.0416857935488224, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75408 < 77660; dropping {'train_loss': 0.0942234992980957, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75418 < 77660; dropping {'train_loss': 0.01042349636554718, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75428 < 77660; dropping {'train_loss': 0.004712642170488834, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75438 < 77660; dropping {'train_loss': 0.04195632413029671, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75448 < 77660; dropping {'train_loss': 0.029867462813854218, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75458 < 77660; dropping {'train_loss': 0.1731470674276352, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75468 < 77660; dropping {'train_loss': 0.03898582234978676, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75478 < 77660; dropping {'train_loss': 0.017342619597911835, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75488 < 77660; dropping {'train_loss': 0.03422309458255768, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75498 < 77660; dropping {'train_loss': 0.04399293288588524, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75508 < 77660; dropping {'train_loss': 0.0070080566219985485, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75518 < 77660; dropping {'train_loss': 0.04197109863162041, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75528 < 77660; dropping {'train_loss': 0.44105860590934753, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75538 < 77660; dropping {'train_loss': 0.03353613242506981, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75548 < 77660; dropping {'train_loss': 0.0061339447274804115, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75558 < 77660; dropping {'train_loss': 0.018189899623394012, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75568 < 77660; dropping {'train_loss': 0.05853157490491867, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75578 < 77660; dropping {'train_loss': 0.05755604803562164, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75588 < 77660; dropping {'train_loss': 0.047841448336839676, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75598 < 77660; dropping {'train_loss': 0.0019600107334554195, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75608 < 77660; dropping {'train_loss': 0.012259352952241898, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75618 < 77660; dropping {'train_loss': 0.0021575428545475006, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75628 < 77660; dropping {'train_loss': 0.030872615054249763, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75638 < 77660; dropping {'train_loss': 0.09760337322950363, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75648 < 77660; dropping {'train_loss': 0.004551645368337631, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75658 < 77660; dropping {'train_loss': 0.0035767240915447474, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75668 < 77660; dropping {'train_loss': 0.0036538932472467422, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75678 < 77660; dropping {'train_loss': 0.43422600626945496, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75688 < 77660; dropping {'train_loss': 0.30926838517189026, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75698 < 77660; dropping {'train_loss': 0.29827776551246643, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75708 < 77660; dropping {'train_loss': 0.32799193263053894, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75718 < 77660; dropping {'train_loss': 0.017860742285847664, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75728 < 77660; dropping {'train_loss': 0.06013632193207741, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75738 < 77660; dropping {'train_loss': 0.02109619975090027, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75748 < 77660; dropping {'train_loss': 0.004395581781864166, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75758 < 77660; dropping {'train_loss': 0.0030823582783341408, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75768 < 77660; dropping {'train_loss': 0.0027700536884367466, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75778 < 77660; dropping {'train_loss': 0.02291378192603588, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75788 < 77660; dropping {'train_loss': 0.0998087003827095, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75798 < 77660; dropping {'train_loss': 0.009935060515999794, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75808 < 77660; dropping {'train_loss': 0.049026329070329666, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75818 < 77660; dropping {'train_loss': 0.032873306423425674, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75828 < 77660; dropping {'train_loss': 0.013731280341744423, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75838 < 77660; dropping {'train_loss': 0.1016177386045456, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75848 < 77660; dropping {'train_loss': 0.019142519682645798, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75858 < 77660; dropping {'train_loss': 0.0026514935307204723, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75868 < 77660; dropping {'train_loss': 0.06333418190479279, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75878 < 77660; dropping {'train_loss': 0.013481874018907547, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75888 < 77660; dropping {'train_loss': 0.4345761239528656, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75898 < 77660; dropping {'train_loss': 0.2581002116203308, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75908 < 77660; dropping {'train_loss': 0.19389040768146515, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75918 < 77660; dropping {'train_loss': 0.0037705646827816963, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75928 < 77660; dropping {'train_loss': 0.007258730474859476, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75938 < 77660; dropping {'train_loss': 0.031144460663199425, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75948 < 77660; dropping {'train_loss': 0.008157198317348957, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75958 < 77660; dropping {'train_loss': 0.009977443143725395, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75968 < 77660; dropping {'train_loss': 0.007810343988239765, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75978 < 77660; dropping {'train_loss': 0.03131959214806557, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75988 < 77660; dropping {'train_loss': 0.022949576377868652, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 75998 < 77660; dropping {'train_loss': 0.04029767960309982, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76008 < 77660; dropping {'train_loss': 0.0933321937918663, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76018 < 77660; dropping {'train_loss': 2.5085136890411377, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76028 < 77660; dropping {'train_loss': 2.733649253845215, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76038 < 77660; dropping {'train_loss': 0.31932532787323, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76048 < 77660; dropping {'train_loss': 0.04619591310620308, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76058 < 77660; dropping {'train_loss': 0.11886700242757797, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76068 < 77660; dropping {'train_loss': 0.04012145474553108, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76078 < 77660; dropping {'train_loss': 0.015989094972610474, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76088 < 77660; dropping {'train_loss': 0.009364509023725986, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76098 < 77660; dropping {'train_loss': 0.24249236285686493, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76108 < 77660; dropping {'train_loss': 0.5420551300048828, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76118 < 77660; dropping {'train_loss': 0.010547729209065437, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76128 < 77660; dropping {'train_loss': 0.007985426113009453, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76138 < 77660; dropping {'train_loss': 0.00819605402648449, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76148 < 77660; dropping {'train_loss': 0.06022310629487038, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76158 < 77660; dropping {'train_loss': 0.0030970554798841476, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76168 < 77660; dropping {'train_loss': 0.2659513056278229, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76178 < 77660; dropping {'train_loss': 0.16039112210273743, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76188 < 77660; dropping {'train_loss': 0.11648557335138321, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76198 < 77660; dropping {'train_loss': 0.7976719737052917, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76208 < 77660; dropping {'train_loss': 0.36769676208496094, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76218 < 77660; dropping {'train_loss': 0.30649900436401367, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76228 < 77660; dropping {'train_loss': 0.015290101990103722, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76238 < 77660; dropping {'train_loss': 0.0024746668059378862, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76248 < 77660; dropping {'train_loss': 0.009598755277693272, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76258 < 77660; dropping {'train_loss': 0.34596726298332214, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76268 < 77660; dropping {'train_loss': 0.13755540549755096, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76278 < 77660; dropping {'train_loss': 0.02606472745537758, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76288 < 77660; dropping {'train_loss': 0.019843682646751404, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76298 < 77660; dropping {'train_loss': 0.009058711118996143, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76308 < 77660; dropping {'train_loss': 0.04043641686439514, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76318 < 77660; dropping {'train_loss': 1.3481292724609375, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76328 < 77660; dropping {'train_loss': 1.0811793804168701, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76338 < 77660; dropping {'train_loss': 0.16490471363067627, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76348 < 77660; dropping {'train_loss': 0.014379285275936127, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76358 < 77660; dropping {'train_loss': 0.18711629509925842, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76368 < 77660; dropping {'train_loss': 0.018176639452576637, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76378 < 77660; dropping {'train_loss': 0.6123732328414917, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76388 < 77660; dropping {'train_loss': 0.011557850986719131, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76398 < 77660; dropping {'train_loss': 0.36690106987953186, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76408 < 77660; dropping {'train_loss': 0.2258436679840088, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76418 < 77660; dropping {'train_loss': 0.10716620832681656, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76428 < 77660; dropping {'train_loss': 0.01735410839319229, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76438 < 77660; dropping {'train_loss': 0.0037772851064801216, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76448 < 77660; dropping {'train_loss': 0.011543958447873592, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76458 < 77660; dropping {'train_loss': 0.00320533593185246, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76468 < 77660; dropping {'train_loss': 0.010437269695103168, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76478 < 77660; dropping {'train_loss': 0.0033203531056642532, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76488 < 77660; dropping {'train_loss': 0.004569125361740589, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76498 < 77660; dropping {'train_loss': 1.0735191106796265, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76508 < 77660; dropping {'train_loss': 0.08215634524822235, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76518 < 77660; dropping {'train_loss': 0.12877722084522247, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76528 < 77660; dropping {'train_loss': 0.01350594125688076, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76538 < 77660; dropping {'train_loss': 0.032273922115564346, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76548 < 77660; dropping {'train_loss': 0.006468631327152252, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76558 < 77660; dropping {'train_loss': 0.004782813601195812, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76568 < 77660; dropping {'train_loss': 0.0039253635331988335, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76578 < 77660; dropping {'train_loss': 0.058191005140542984, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76588 < 77660; dropping {'train_loss': 0.20914098620414734, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76598 < 77660; dropping {'train_loss': 0.023319046944379807, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76608 < 77660; dropping {'train_loss': 0.020922888070344925, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76618 < 77660; dropping {'train_loss': 1.7112040519714355, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76628 < 77660; dropping {'train_loss': 0.03322984650731087, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76638 < 77660; dropping {'train_loss': 0.024803467094898224, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76648 < 77660; dropping {'train_loss': 0.012989748269319534, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76658 < 77660; dropping {'train_loss': 0.5037537813186646, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76668 < 77660; dropping {'train_loss': 0.01617761328816414, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76678 < 77660; dropping {'train_loss': 0.010102508589625359, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76688 < 77660; dropping {'train_loss': 0.003908239305019379, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76698 < 77660; dropping {'train_loss': 0.00494743837043643, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76708 < 77660; dropping {'train_loss': 0.07262884080410004, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76718 < 77660; dropping {'train_loss': 0.01950872130692005, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76728 < 77660; dropping {'train_loss': 0.3459923267364502, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76738 < 77660; dropping {'train_loss': 0.04795558750629425, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76748 < 77660; dropping {'train_loss': 0.27564939856529236, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76758 < 77660; dropping {'train_loss': 0.007917745038866997, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76768 < 77660; dropping {'train_loss': 0.02299581840634346, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76778 < 77660; dropping {'train_loss': 0.04522905871272087, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76788 < 77660; dropping {'train_loss': 0.19880011677742004, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76798 < 77660; dropping {'train_loss': 0.2658226490020752, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76808 < 77660; dropping {'train_loss': 0.005680321715772152, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76818 < 77660; dropping {'train_loss': 0.004742728546261787, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76828 < 77660; dropping {'train_loss': 0.10359328240156174, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76838 < 77660; dropping {'train_loss': 0.023819098249077797, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76848 < 77660; dropping {'train_loss': 0.676430881023407, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76858 < 77660; dropping {'train_loss': 0.1207241490483284, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76868 < 77660; dropping {'train_loss': 0.284930020570755, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76878 < 77660; dropping {'train_loss': 0.08296343684196472, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76888 < 77660; dropping {'train_loss': 0.006893218494951725, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76898 < 77660; dropping {'train_loss': 0.11640717089176178, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76908 < 77660; dropping {'train_loss': 0.06854686141014099, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76918 < 77660; dropping {'train_loss': 0.009220737963914871, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76928 < 77660; dropping {'train_loss': 0.004211604129523039, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76938 < 77660; dropping {'train_loss': 0.3853282630443573, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76948 < 77660; dropping {'train_loss': 0.005031371954828501, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76958 < 77660; dropping {'train_loss': 0.002068572910502553, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76968 < 77660; dropping {'train_loss': 0.019140025600790977, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76978 < 77660; dropping {'train_loss': 0.4307556748390198, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76988 < 77660; dropping {'train_loss': 0.014383983798325062, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 76998 < 77660; dropping {'train_loss': 0.03256130591034889, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77008 < 77660; dropping {'train_loss': 0.4262635409832001, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77018 < 77660; dropping {'train_loss': 0.022509070113301277, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77028 < 77660; dropping {'train_loss': 0.044500842690467834, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77038 < 77660; dropping {'train_loss': 0.014513506554067135, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77048 < 77660; dropping {'train_loss': 0.1230645701289177, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77058 < 77660; dropping {'train_loss': 0.009467830881476402, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77068 < 77660; dropping {'train_loss': 0.015415444038808346, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77078 < 77660; dropping {'train_loss': 0.013512554578483105, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77088 < 77660; dropping {'train_loss': 0.0029205218888819218, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77098 < 77660; dropping {'train_loss': 0.02413712441921234, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77108 < 77660; dropping {'train_loss': 0.0018824179423972964, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77118 < 77660; dropping {'train_loss': 0.0024871614295989275, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77128 < 77660; dropping {'train_loss': 0.013706579804420471, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77138 < 77660; dropping {'train_loss': 0.007794072385877371, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77148 < 77660; dropping {'train_loss': 0.016657644882798195, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77158 < 77660; dropping {'train_loss': 0.4944770336151123, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77168 < 77660; dropping {'train_loss': 0.48414766788482666, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77178 < 77660; dropping {'train_loss': 0.003653149586170912, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77188 < 77660; dropping {'train_loss': 0.01573088951408863, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77198 < 77660; dropping {'train_loss': 2.841294527053833, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77208 < 77660; dropping {'train_loss': 0.012704679742455482, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77218 < 77660; dropping {'train_loss': 0.002702175173908472, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77228 < 77660; dropping {'train_loss': 1.0847561359405518, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77238 < 77660; dropping {'train_loss': 0.03331933543086052, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77248 < 77660; dropping {'train_loss': 0.01825743354856968, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77258 < 77660; dropping {'train_loss': 0.010511833243072033, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77268 < 77660; dropping {'train_loss': 0.0074655963107943535, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77278 < 77660; dropping {'train_loss': 0.013004107400774956, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77288 < 77660; dropping {'train_loss': 0.07079102843999863, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77298 < 77660; dropping {'train_loss': 0.04214363172650337, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77308 < 77660; dropping {'train_loss': 0.004333858843892813, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77318 < 77660; dropping {'train_loss': 0.4317912757396698, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77328 < 77660; dropping {'train_loss': 0.6047319173812866, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77338 < 77660; dropping {'train_loss': 0.017625119537115097, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77348 < 77660; dropping {'train_loss': 0.3332497179508209, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77358 < 77660; dropping {'train_loss': 0.0315791592001915, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77368 < 77660; dropping {'train_loss': 0.0461612232029438, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77378 < 77660; dropping {'train_loss': 0.008192096836864948, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77388 < 77660; dropping {'train_loss': 0.036933694034814835, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77398 < 77660; dropping {'train_loss': 0.09325086325407028, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77408 < 77660; dropping {'train_loss': 0.0044700345024466515, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77418 < 77660; dropping {'train_loss': 0.03499055653810501, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77428 < 77660; dropping {'train_loss': 0.013946183025836945, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77438 < 77660; dropping {'train_loss': 0.037165265530347824, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77448 < 77660; dropping {'train_loss': 0.04379483684897423, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77458 < 77660; dropping {'train_loss': 0.01609974354505539, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77468 < 77660; dropping {'train_loss': 0.6986563801765442, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77478 < 77660; dropping {'train_loss': 0.024525996297597885, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77488 < 77660; dropping {'train_loss': 0.05136348679661751, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77498 < 77660; dropping {'train_loss': 0.09492488205432892, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77508 < 77660; dropping {'train_loss': 0.15466146171092987, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77518 < 77660; dropping {'train_loss': 0.009340054355561733, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77528 < 77660; dropping {'train_loss': 0.15118072926998138, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77538 < 77660; dropping {'train_loss': 0.012958314269781113, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77548 < 77660; dropping {'train_loss': 0.007880747318267822, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77558 < 77660; dropping {'train_loss': 0.008480333723127842, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77568 < 77660; dropping {'train_loss': 0.05743873864412308, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77578 < 77660; dropping {'train_loss': 0.06773634254932404, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77588 < 77660; dropping {'train_loss': 0.006467332597821951, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77598 < 77660; dropping {'train_loss': 0.15014532208442688, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77608 < 77660; dropping {'train_loss': 0.007824035361409187, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77618 < 77660; dropping {'train_loss': 0.03689851611852646, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77628 < 77660; dropping {'train_loss': 0.003762616543099284, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77638 < 77660; dropping {'train_loss': 0.013488490134477615, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77648 < 77660; dropping {'train_loss': 0.07160583138465881, 'epoch': 4}.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 77658 < 77660; dropping {'train_loss': 0.0037126317620277405, 'epoch': 4}.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfc8ebd9a37c4bec9304fc2338d670c2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 89609 < 95582; dropping {'val_epoch_loss': 0.29187747836112976, 'val_epoch_auc': 0.9937901011866044, 'epoch': 4}.\n",
            "\n",
            "Epoch 00004: val_epoch_auc  was not in top 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Adding to old History rows isn't currently supported.  Step 89610 < 95582; dropping {'train_epoch_loss': 0.1381446123123169, 'epoch': 4}.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-12f5452a019a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DeepPavlov_ru-SBERT/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweigths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRUN_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/source/train.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model_class, config, project_name, run_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwandb_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/Colab Notebooks/checkpoints/DeepPavlov_ru-SBERT'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLHc-yuPi7sv",
        "colab_type": "code",
        "outputId": "526fca3a-60a5-4f21-caa5-7e8b548e20c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "db382381a355414c93fd628bbad45849",
            "213c5dadf93347e283c422eef942f21b",
            "bfc9d100c69841c3a365ec64272165fb",
            "0f179e317fce4638be93189c1f765ffb",
            "f04e2b5c811a42f0b320691c5ff63fef",
            "5402bd2c99d24a84aa706627688f2681",
            "102645ef950240ba8d332b8228dc7bde",
            "8a01978041714664b45c74d00e77aa33",
            "19935618b3ee463d8eb201df62b27c9a",
            "e8db0544cc284092ae02bb6dfaf794fe",
            "3654083f336f4c35866bb12156b16e58",
            "a44a416c48ed427d8d2531fc9dd7b0cc",
            "cf9efe4b5bda4065af98d220511ba3e6",
            "71daeb26c87d4f16b06940f259f63bcc",
            "3826c788cb004f93aea811a84fc1276e",
            "b0c1be0a96584e61b93a035136246e78"
          ]
        }
      },
      "source": [
        "for file in os.listdir('drive/My Drive/Colab Notebooks/checkpoints'):\n",
        "  if file.endswith(\".ckpt\"):\n",
        "    ckpt_path = os.path.join('drive/My Drive/Colab Notebooks/checkpoints', file)\n",
        "\n",
        "model = ModelClass.load_from_checkpoint(ckpt_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db382381a355414c93fd628bbad45849",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=642.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19935618b3ee463d8eb201df62b27c9a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=711456784.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wACZjz57kcak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "\n",
        "def evaluate(model, workdir, run_name):\n",
        "    test = pd.read_parquet(os.path.join(workdir, 'data/test.parquet'))\n",
        "    annos = test.columns[1:]\n",
        "    features, labels = test['question'].to_numpy(), test.drop('question', axis=1).to_numpy()\n",
        "    del test\n",
        "\n",
        "    preds_probas = model.predict(features).cpu().numpy()\n",
        "    labels_max = np.argmax(labels, axis=1)\n",
        "    preds = np.argmax(preds_probas, axis=1)\n",
        "    #compute cm\n",
        "    cm = confusion_matrix(labels_max, preds, normalize='true')\n",
        "    #sort it\n",
        "    sorting_idx = np.argsort(np.diag(cm))[::-1]\n",
        "    sorted_cm = cm[sorting_idx,:][:,sorting_idx]\n",
        "    sorted_annos = []\n",
        "    for id in sorting_idx:\n",
        "        sorted_annos.append(annos[id])\n",
        "    #plot heatmap\n",
        "    fig, ax = plt.subplots(figsize=(20,20))\n",
        "    ax = sns.heatmap(sorted_cm, vmax=1, xticklabels=sorted_annos, yticklabels=sorted_annos)\n",
        "    fig.savefig(f\"drive/My Drive/Colab Notebooks/{run_name}_heatmap.svg\")\n",
        "    #print metric\n",
        "    auc = roc_auc_score(labels, preds_probas, average='macro', multi_class='ovo')\n",
        "    print(f'Macro ROC-AUC = {auc}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWCh2CIkkf5W",
        "colab_type": "code",
        "outputId": "2fb03070-0ac1-494d-b7a1-d1e5f9c49d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "de7302a1df4b4aa18039e6ca145d0419",
            "5681811defb148498287a230fa52c2d7",
            "becbf4b26d8d484cac0ff1933161aa23",
            "86d6ef8a620c45279154807718c7e33b",
            "abfa6983648647ad85f731e8563da971",
            "7c22e87c748c4e959a2a768b95744943",
            "32c0bae3b45f4edea98f196bff3a5b9a",
            "2ef5b8485edc485489c307f7e6d6240e",
            "28561b422e1640c1920b6e262c432a7c",
            "1402effb763d421da2a1c78bd798d12f",
            "c7f7a38cd9dc4703af964bb62ef90190",
            "81fa7fc9ec354facaf0d4e4af85a41ad",
            "71c710c6c8dc469194fc3bdb123dfda7",
            "a05898621e5742129ffaa3f4ce62a70d",
            "7650804caf1e437eb494bf2551b5abfb",
            "07e5a0bd593943b896bc513b3f8e8798",
            "7be9c6afb6b64c61abfa33141dee2fce",
            "62f5c56548da405bb6a9629e554f6fef",
            "5c4c41f64f3d4858be1d6a4537fdc5f5",
            "78b322f83ba54e5db7beabe4ff8de1d6",
            "d7f51724f3c748b48f9fe5c637f51971",
            "9d5bb709cfe4440db051ecd932672a55",
            "32f85a3d1f1840a1bcd1497617a7ab96",
            "fae644234b404780a42317adce28a7ca"
          ]
        }
      },
      "source": [
        "RUN_NAME = 'DeepPavlov_ru-SBERT'\n",
        "evaluate(model, 'DeepPavlov_ru-SBERT', RUN_NAME)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de7302a1df4b4aa18039e6ca145d0419",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1649718.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28561b422e1640c1920b6e262c432a7c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7be9c6afb6b64c61abfa33141dee2fce",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=24.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokenizing...: 100%|██████████| 71685/71685 [01:10<00:00, 1011.96it/s]\n",
            "Predicting on cuda: 100%|██████████| 2241/2241 [20:03<00:00,  1.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Macro ROC-AUC = 0.9951781542777379\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a65abc0e3874504af428145b47963c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_36f3d2472a0d4d8681ea23d14a3d7515",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5dcafa844227443db88656decab26bce",
              "IPY_MODEL_32d9627354e0496ca24a7e52d83d3ede"
            ]
          }
        },
        "36f3d2472a0d4d8681ea23d14a3d7515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5dcafa844227443db88656decab26bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4bfc264a66bb4d0e858f32a1545256e8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 642,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 642,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_033563de9068407ca4538fef8e23315a"
          }
        },
        "32d9627354e0496ca24a7e52d83d3ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5d522901073f49838f9314128850f539",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 642/642 [00:00&lt;00:00, 687B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c846078b6a964e4cb7c5cfb5d6e52ca7"
          }
        },
        "4bfc264a66bb4d0e858f32a1545256e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "033563de9068407ca4538fef8e23315a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d522901073f49838f9314128850f539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c846078b6a964e4cb7c5cfb5d6e52ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "442785f9d13e48b9b57e49ea78a4b168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f208d43f54fd48cfb1fd4b4907e48e63",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2e32d59a7b144c18b82e0c3e76ad62a0",
              "IPY_MODEL_91ce13a7504f4605bd3e26b8a42f9a28"
            ]
          }
        },
        "f208d43f54fd48cfb1fd4b4907e48e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e32d59a7b144c18b82e0c3e76ad62a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f750af402de74981bf6be47c8f6ff9e3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 711456784,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 711456784,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_206c60976dc3468e91a20b9400965894"
          }
        },
        "91ce13a7504f4605bd3e26b8a42f9a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_643fa155a9c74c2c80fc84d060d91b02",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 711M/711M [00:39&lt;00:00, 18.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83f067e18361484c80e14ece3b7a02b2"
          }
        },
        "f750af402de74981bf6be47c8f6ff9e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "206c60976dc3468e91a20b9400965894": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "643fa155a9c74c2c80fc84d060d91b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83f067e18361484c80e14ece3b7a02b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57148228d9d346e5b58b043a1219bdf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a75a71c32e84333a59f9a1f20bce099",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f0813054671f4752a1093ac948950722",
              "IPY_MODEL_74f391a7c9f44ca9ab55455c66bb52dd"
            ]
          }
        },
        "2a75a71c32e84333a59f9a1f20bce099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0813054671f4752a1093ac948950722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0e6d5026403e4c2a98b82831f31ff5e9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1649718,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1649718,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40bd8002bc6044909a7a735063154218"
          }
        },
        "74f391a7c9f44ca9ab55455c66bb52dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f286acfe806b41aca5551d87f02be9f5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.65M/1.65M [00:02&lt;00:00, 571kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_33601a45a8ba4025917619ad51e99ff3"
          }
        },
        "0e6d5026403e4c2a98b82831f31ff5e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40bd8002bc6044909a7a735063154218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f286acfe806b41aca5551d87f02be9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "33601a45a8ba4025917619ad51e99ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60ce3d8580bc4260b8dcff62d1820436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_664037ce464f4cc3898c3957db2c3cdc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_55957f7adf174ddaa1aff26d8d80f69e",
              "IPY_MODEL_bb985fc58fa04695a8330590ca69fcff"
            ]
          }
        },
        "664037ce464f4cc3898c3957db2c3cdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55957f7adf174ddaa1aff26d8d80f69e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14f1c71998014f80a9b9f223e099d89c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3374b90ab0964148817cfb577f086263"
          }
        },
        "bb985fc58fa04695a8330590ca69fcff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3c9e863b4e3148d1935aecc191d23e29",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:00&lt;00:00, 117B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6147f256b38740f3aff087b1ec32bef0"
          }
        },
        "14f1c71998014f80a9b9f223e099d89c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3374b90ab0964148817cfb577f086263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c9e863b4e3148d1935aecc191d23e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6147f256b38740f3aff087b1ec32bef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3de46a999f2458f85554e28e66d994a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ea3bcf44e2824e299de99da26fcb4bed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1fb07fb45a2c49079fd3be448a172ed5",
              "IPY_MODEL_6ba2ffa17750412c81c01f883469c8d9"
            ]
          }
        },
        "ea3bcf44e2824e299de99da26fcb4bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1fb07fb45a2c49079fd3be448a172ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_368e9cca0e5c410daae755e8f536e7d2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 24,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 24,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a388f24434444bb89e03fe7cac9014ca"
          }
        },
        "6ba2ffa17750412c81c01f883469c8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_62fa6b6f0fda4695bf373ebd632495b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 24.0/24.0 [00:17&lt;00:00, 1.37B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5127d4b878954ca7b1435ed1318575c3"
          }
        },
        "368e9cca0e5c410daae755e8f536e7d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a388f24434444bb89e03fe7cac9014ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62fa6b6f0fda4695bf373ebd632495b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5127d4b878954ca7b1435ed1318575c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0bb2b0321d74be0add961f514e2d600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7aaa48983292445fac3f7a165faad398",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a273640579c84530acd499db36149916",
              "IPY_MODEL_350cafaee4e44f93b2e4e25b1e1e45a6"
            ]
          }
        },
        "7aaa48983292445fac3f7a165faad398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "a273640579c84530acd499db36149916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de9d20ac9c84493db931c6dfffc95227",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed7d2e1c2a6140e69c05fadbe6a0a365"
          }
        },
        "350cafaee4e44f93b2e4e25b1e1e45a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3f2b90dd33464e7e83c830753965692e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:00&lt;00:00,  2.53it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_26c983211884477584072e28d0feae0b"
          }
        },
        "de9d20ac9c84493db931c6dfffc95227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed7d2e1c2a6140e69c05fadbe6a0a365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f2b90dd33464e7e83c830753965692e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "26c983211884477584072e28d0feae0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aaef8d2268ed45b8bc8ed16767756819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0c05455f4e284385b2f2047a8c3dd293",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7a96f0b57ab44502a63569b02f60c418",
              "IPY_MODEL_07e9ed013dd3422099d1acb70cf3967a"
            ]
          }
        },
        "0c05455f4e284385b2f2047a8c3dd293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "7a96f0b57ab44502a63569b02f60c418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3e6d6d3e7a2442bcae5c3fba511026a3",
            "_dom_classes": [],
            "description": "Epoch 5: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 23896,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 23896,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23ce2bcd8f4245b5aeb535cb9ced5853"
          }
        },
        "07e9ed013dd3422099d1acb70cf3967a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f7012a051ef143d3bdd3bed60901df93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23896/23896 [1:49:43&lt;00:00,  3.63it/s, loss=0.191, v_num=1pp96apw]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e352ae5cc0e543ccb824a27d053c4498"
          }
        },
        "3e6d6d3e7a2442bcae5c3fba511026a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23ce2bcd8f4245b5aeb535cb9ced5853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7012a051ef143d3bdd3bed60901df93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e352ae5cc0e543ccb824a27d053c4498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "74ccf3d4c8c843ed86cb134f2375bece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_69d8d096e4594a3ba438caa352eadbdc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5ddf9337a8794f6cb7845b707a76e5f5",
              "IPY_MODEL_3f6a82b2de4049c6b63b3d44c7650a01"
            ]
          }
        },
        "69d8d096e4594a3ba438caa352eadbdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "5ddf9337a8794f6cb7845b707a76e5f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e4edd9bd234c463b8bcd0c3b7475e4c7",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4dd73c7d148418f8ba58276158a0c8f"
          }
        },
        "3f6a82b2de4049c6b63b3d44c7650a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5f365cbf159e4e5ea5038eefd99443c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5974/5974 [14:51&lt;00:00, 11.41it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29ec6f2599bb48ccb36c3e40a9f5ca4c"
          }
        },
        "e4edd9bd234c463b8bcd0c3b7475e4c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4dd73c7d148418f8ba58276158a0c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f365cbf159e4e5ea5038eefd99443c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29ec6f2599bb48ccb36c3e40a9f5ca4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5bfff87f567846ff804f8181d40c55f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc5d6fb32df74e6a886f763c43369c28",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_88d69c268fcc4deca22c0430fff04531",
              "IPY_MODEL_45f388592b0f4acba9525af889f876f2"
            ]
          }
        },
        "fc5d6fb32df74e6a886f763c43369c28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "88d69c268fcc4deca22c0430fff04531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7886887deb5a4486acd0a07ffc489768",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_edbcc7479cbb4794a2b857fa3f2b7011"
          }
        },
        "45f388592b0f4acba9525af889f876f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_df05e933b5e748f1b8dbe8066dcc98f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5974/5974 [14:51&lt;00:00, 11.42it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e14a9b884c14d7a9f73be6fac342ea9"
          }
        },
        "7886887deb5a4486acd0a07ffc489768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "edbcc7479cbb4794a2b857fa3f2b7011": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df05e933b5e748f1b8dbe8066dcc98f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e14a9b884c14d7a9f73be6fac342ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e96879e04b9540898bfe4b0cd32aca6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ac29453ebe1d4b1e84bf28c4a4ad3e5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_086c5c69a17943dab5bc85ef964b1cda",
              "IPY_MODEL_1efef3d8d44f4a3d902fc1f228001480"
            ]
          }
        },
        "ac29453ebe1d4b1e84bf28c4a4ad3e5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "086c5c69a17943dab5bc85ef964b1cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_99a62e8d748d48faa3a1ee96c8095762",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_472eec65e672492492764f61b2b10304"
          }
        },
        "1efef3d8d44f4a3d902fc1f228001480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6b8b5354eb274ddaa0d2d7d599043075",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5974/5974 [14:59&lt;00:00, 11.27it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dbcdf2b93e984f569713e405e81c3376"
          }
        },
        "99a62e8d748d48faa3a1ee96c8095762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "472eec65e672492492764f61b2b10304": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b8b5354eb274ddaa0d2d7d599043075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dbcdf2b93e984f569713e405e81c3376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a147bc9ed6c5424a87c49f040795e7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3eaa92e7d8d044369259472e02a88f74",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0c7b3b5a27542cdacd2e89f707df900",
              "IPY_MODEL_8b0645d6dd254618aa101bffcbab68dd"
            ]
          }
        },
        "3eaa92e7d8d044369259472e02a88f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "c0c7b3b5a27542cdacd2e89f707df900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58cfc7a7e2654052b0dbea4ac1c36db0",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9309306fe8954a38859ac4604d721839"
          }
        },
        "8b0645d6dd254618aa101bffcbab68dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7742761c63c84d578cd35e76bde8108d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5974/5974 [14:55&lt;00:00, 10.79it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5cae93abb2e942bb8d4bb15150ca5cb5"
          }
        },
        "58cfc7a7e2654052b0dbea4ac1c36db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9309306fe8954a38859ac4604d721839": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7742761c63c84d578cd35e76bde8108d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5cae93abb2e942bb8d4bb15150ca5cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dfc8ebd9a37c4bec9304fc2338d670c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_92b9303f96674d3fb0d872a88ffa4b36",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_567666c030894f39bfe03732446ff9f3",
              "IPY_MODEL_e3e6736818664dee916735a18513446a"
            ]
          }
        },
        "92b9303f96674d3fb0d872a88ffa4b36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "567666c030894f39bfe03732446ff9f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5417c5d8875407281336bbd1c230886",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d06f0763948849e880804d53105d1ab8"
          }
        },
        "e3e6736818664dee916735a18513446a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0fb4e16339a41ffadabba0b143d10a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5974/5974 [14:56&lt;00:00, 11.30it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_500400ea7cb640b9ba5bbe0e4ec4a84d"
          }
        },
        "a5417c5d8875407281336bbd1c230886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d06f0763948849e880804d53105d1ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0fb4e16339a41ffadabba0b143d10a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "500400ea7cb640b9ba5bbe0e4ec4a84d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db382381a355414c93fd628bbad45849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_213c5dadf93347e283c422eef942f21b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bfc9d100c69841c3a365ec64272165fb",
              "IPY_MODEL_0f179e317fce4638be93189c1f765ffb"
            ]
          }
        },
        "213c5dadf93347e283c422eef942f21b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bfc9d100c69841c3a365ec64272165fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f04e2b5c811a42f0b320691c5ff63fef",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 642,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 642,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5402bd2c99d24a84aa706627688f2681"
          }
        },
        "0f179e317fce4638be93189c1f765ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_102645ef950240ba8d332b8228dc7bde",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 642/642 [00:22&lt;00:00, 27.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a01978041714664b45c74d00e77aa33"
          }
        },
        "f04e2b5c811a42f0b320691c5ff63fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5402bd2c99d24a84aa706627688f2681": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "102645ef950240ba8d332b8228dc7bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a01978041714664b45c74d00e77aa33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19935618b3ee463d8eb201df62b27c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e8db0544cc284092ae02bb6dfaf794fe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3654083f336f4c35866bb12156b16e58",
              "IPY_MODEL_a44a416c48ed427d8d2531fc9dd7b0cc"
            ]
          }
        },
        "e8db0544cc284092ae02bb6dfaf794fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3654083f336f4c35866bb12156b16e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cf9efe4b5bda4065af98d220511ba3e6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 711456784,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 711456784,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71daeb26c87d4f16b06940f259f63bcc"
          }
        },
        "a44a416c48ed427d8d2531fc9dd7b0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3826c788cb004f93aea811a84fc1276e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 711M/711M [00:22&lt;00:00, 31.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b0c1be0a96584e61b93a035136246e78"
          }
        },
        "cf9efe4b5bda4065af98d220511ba3e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71daeb26c87d4f16b06940f259f63bcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3826c788cb004f93aea811a84fc1276e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b0c1be0a96584e61b93a035136246e78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de7302a1df4b4aa18039e6ca145d0419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5681811defb148498287a230fa52c2d7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_becbf4b26d8d484cac0ff1933161aa23",
              "IPY_MODEL_86d6ef8a620c45279154807718c7e33b"
            ]
          }
        },
        "5681811defb148498287a230fa52c2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "becbf4b26d8d484cac0ff1933161aa23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_abfa6983648647ad85f731e8563da971",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1649718,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1649718,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7c22e87c748c4e959a2a768b95744943"
          }
        },
        "86d6ef8a620c45279154807718c7e33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32c0bae3b45f4edea98f196bff3a5b9a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.65M/1.65M [00:00&lt;00:00, 2.47MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ef5b8485edc485489c307f7e6d6240e"
          }
        },
        "abfa6983648647ad85f731e8563da971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7c22e87c748c4e959a2a768b95744943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32c0bae3b45f4edea98f196bff3a5b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ef5b8485edc485489c307f7e6d6240e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28561b422e1640c1920b6e262c432a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1402effb763d421da2a1c78bd798d12f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c7f7a38cd9dc4703af964bb62ef90190",
              "IPY_MODEL_81fa7fc9ec354facaf0d4e4af85a41ad"
            ]
          }
        },
        "1402effb763d421da2a1c78bd798d12f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7f7a38cd9dc4703af964bb62ef90190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_71c710c6c8dc469194fc3bdb123dfda7",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a05898621e5742129ffaa3f4ce62a70d"
          }
        },
        "81fa7fc9ec354facaf0d4e4af85a41ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7650804caf1e437eb494bf2551b5abfb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:00&lt;00:00, 371B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07e5a0bd593943b896bc513b3f8e8798"
          }
        },
        "71c710c6c8dc469194fc3bdb123dfda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a05898621e5742129ffaa3f4ce62a70d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7650804caf1e437eb494bf2551b5abfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07e5a0bd593943b896bc513b3f8e8798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7be9c6afb6b64c61abfa33141dee2fce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_62f5c56548da405bb6a9629e554f6fef",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5c4c41f64f3d4858be1d6a4537fdc5f5",
              "IPY_MODEL_78b322f83ba54e5db7beabe4ff8de1d6"
            ]
          }
        },
        "62f5c56548da405bb6a9629e554f6fef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c4c41f64f3d4858be1d6a4537fdc5f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d7f51724f3c748b48f9fe5c637f51971",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 24,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 24,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d5bb709cfe4440db051ecd932672a55"
          }
        },
        "78b322f83ba54e5db7beabe4ff8de1d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32f85a3d1f1840a1bcd1497617a7ab96",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 24.0/24.0 [00:00&lt;00:00, 192B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fae644234b404780a42317adce28a7ca"
          }
        },
        "d7f51724f3c748b48f9fe5c637f51971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d5bb709cfe4440db051ecd932672a55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32f85a3d1f1840a1bcd1497617a7ab96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fae644234b404780a42317adce28a7ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}