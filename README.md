
### Постановка задачи и результат:
## Задача:
Сравнительный анализ современных моделей, для классификации принадлежности вопроса к целевому каналу на основе исторических данных slack ods.ai.

## Результаты:

| Model               | pwROC-AUC score |
|:-------------------:|----------------:|
| USE_baseline        | 0,61            |
| USE_finetuned       | 0,985           |
| SBERT_finetuned     | 0,991           | 
| DeepPavlov ru-SBERT | **0.995**       |
| DeepPavlov mul-SBERT| 0.993           |
| TF IDF + LogReg     | 0.993           |
| tayga_skipgram_300  | 0.651           |


# Модели использованные в сравнительном анализе:
 - USE (https://tfhub.dev/google/universal-sentence-encoder-multilingual/3)
 - sbert (https://github.com/UKPLab/sentence-transformers)
 - DeepPavlov ru-SBERT
 - DeepPavlov mul-SBERT
 - Tf-Idf + logreg
 - tayga_upos_skipgram_300_2_2019
 
# Данные
дамп слака ods: https://opendatascience.slack.com/files/U04URBM8V/FSRD4NVLN/opendatascience_slack_export_mar_12_2015_-_jan_14_2020.zip

Из данного набора сообщений были выбраны все треды с допущением, что стартовое сообщение треда – вопрос, а остальные ответы. Далее, вопросы были отфильтрованы по наличию вопросительного знака в тексте, исключая ссылки.
Для задачи классификации были оставлены вопросы из каналов, содержащих более 100 тредов. 
Данные были разделены на три части: тренировочную, валидационную и тестовую части (train.parquet, val.parquet и test.parquet в папке data)

|         | Train      | Val       | Test      |
|---------|:----------:|:---------:|:---------:|
| Вопросы | 215054     | 71685     | 71685     |
| Слова   | 10,765,047 | 3,587,705 | 3,622,738 |
| Классы  | 79         | 79        | 79        |
 
# Метрика: 
  Pairwise macro average multiclass ROC AUC score (pwROC-AUC). https://link.springer.com/article/10.1023/A:1010920819831
   
# Baseline:
За baseline приняли результат полученный при помощи USE модели, примененной к тестовым данным, без дообучения на train данных из дампа slack ods.
pwROC_AUC = 0.61

### Вклад участников

- @Satel1ite основной контрибутор, автор всего кода проекта
- @vengodelsur вдохновитель проекта, автор изначальной идеи
- @yzelensky участие в обсуждении, формализация целей, организация
- @alexeysorokin89 куратор, советы
- @vnareyko единичные советы

### История проекта
Проект изначально задумывался как "автоответчик для ODS на основе современных моделей (до)обученных на данных дампа slack". 
Тред с описанием идеи в #ods_pet_projects https://opendatascience.slack.com/archives/CJW0A6U78/p1584557756041500 
Изначально предполагалось, решать задачу поиска вопросов-дубликатов. Привлекало наличие baseline оценки, для модели USE на основе данных quora. Однако, в ходе EDA (результаты доступны в папке plots репозитория), выяснилось что повторные вопросы в slack ods - это довольно большая редкость, и составить для проведения сравнительного анализа, достаточного объема размеченный датасет, вручную или полуавтоматически не представляется разумным. Кроме того, малое количество вопросов - дубликатов, подрывает исходную мотивацию к решению задачи. Было принято решение сменить постановку задачи, предерживаясь исходной мета-идеи: сравнительный анализ современных моделей на примере данных дампа ODS с адекватной метрикой выроженой одним числом. Остановились на задаче классификации принадлежности вопроса к каналу. Даже baseline решение показало очень высоке качество классификации. Дополнительный интерес, вызвали выявленные, систематичские ошибки в классификации сообщений из некоторых каналов, соответствующие интуитивным ожиданиям (в канал theory_and_practice задают вопросы относящиеся к cv, deep_learning, python - и это четко видно на матрице ошибок). Наличие таких систематических ошибок, натолкнуло на идею разработки потенциально полезного бота - "подсказывающего" правильный тематический канал.

### Результаты
Разработаны эверистики для выделения тредов вопросов из сырых данных дампа слака. Проведен exparatory анализ данных, включая интерактивную визуализацию tsne проекций средствами bokeh, с попыткой кластеризации. Построены матрицы ошибок принадлежности вопросов каналам. Выявлены соответствующие интуитивным ожиданиям систематические ошибки в классификации сообщений из заведомом "смешанных" каналов: #theory_and_practice, #random и т.п. Выделен репрезентативных набор каналов, для сразвнительной классификации. 
После дообучения моделей на данных дампа слака были получены следующие результаты:

| Model               | pwROC-AUC score |
|:-------------------:|----------------:|
| USE_baseline        | 0,61            |
| USE_finetuned       | 0,985           |
| SBERT_finetuned     | 0,991           | 
| DeepPavlov ru-SBERT | **0.995**       |
| DeepPavlov mul-SBERT| 0.993           |
| TF IDF + LogReg     | 0.993           |
| tayga_skipgram_300  | 0.651           |

Отсортированная нормированная тепловая карта матрицы ошибок для каждой из моделей находится в папке plots.
Модели DeepPavlov показали лучший результат по метрике, однако оказались наиболее "тяжелыми" для обучения и работы. Обучение не влезает в 16 GB видеопамяти, пришлось обрезать вопросы из обучающей выборки до 1024 символов.

ELMo занимает немногим меньше памяти, чем BERT из DeepPavlov, однако обучается на несколько порядков дольше и вылезает за лимиты Google Colabaratory, поэтому обучить эту модель в итоге не вышло. 

Метрика показывает близкие значения для разных моделей, однако карты ошибок различаются довольно значительно.

## Основные выводы
Дообучение моделей на предметных данных, существенно повышает результативность проанализированных моделях на задаче классификации. После дообучения как, более старая USE модель, так и более современная SBERT показывают высокие результаты в задаче классификации принадлежности вопроса к каналу, 0.985 и 0.991 по pairwise macro average multiclass ROC AUC score соответственно, что открывает возможности для практического применения моделей, например, для системы рекомендующей подходящий канал для задаваемого вопроса. Такие высокие показатели, также свидетельствуют о высоком качестве разделения (и разделимости) исходных данных. В случае большого числа ошибочно отнесенных к каналу вопросов, не удалось бы достичь высокого значения метрики, по причине ошибочного отнесения корректных случаев, к false positive.

Единственный канал, вызывающий затруднения у всех опробованных моделей _call_4_colaboration, почему именно он понять пока не удалось.

## Идеи по развитию
На основе проведенных работ, представляется возможным, разработать бота, подсказывающего тематический канал для вопросов задаваемых в "смешанных" каналах. Для того, чтобы уменьшить негатив от false positive срабатываний классификатора бота, можно поэкспериментировать с выдачей рекомендации не в виде стереотипического текста (что само по себе, может вызывать негатив) а в виде (ненавязчивых) иконок - реакций на текст вопроса.
Проверить вручную случаи ошибочной классификации (hard negative mining), на предмет определения типа вопросов, который вызывает сложности, или, выявления ошибок в ground truth разметке.

### Содержимое папок:
* data - данные из дампа слака, приведенные в табличный вид. Процесс подготовки данных можно увидеть в data_processing.ipynb
  * qa_pairs - грязные пары первое сообщение треда - ответ. Для каждого треда количество таких пар равняется количеству ответов в нём
  * qa_pairs_mod - выброшены пары с ответами без текста, с вопросами не содержащими знак вопроса (за исключением знаков вопроса в ссылках), а также пары из каналов с вакансиями
  * qa_clean - выбраны пары, у ответов которых есть одна из реакций: 'tnx', 'heavy_plus_sign', '+1', и не содержащих "спасибо"
  * qa_classif - из qa_pairs выброшены пары, не содержащие знаков вопроса и перенаправлений в другой канал и пары из каналов, содержащих менее 100 тредов
  * train, test, val - разделенная на три части qa_classif со стратификацией по классам.
* models_code - ноутбуки с кодом обучения моделей, а также архивы с исходниками, в некоторых случаях предобученными бинар моделей, которые можно загрузить в Google Colab и запустить несколькими командами:
```
!unzip achive_name.zip 
!pip install -r achive_name/requirements.txt
!wandb login
!python achive_name/run_colab_training.py
```
* modules - дополнительные модули для построения карты кластеров и проверки работы моделей
* plots - различные интерактивные карты кластеров, полученные в процессе изучения данных (файлы с расширением .html), тепловые карты матрицы ошибок (файлы с расширением .svg)
* tests - неструктурированный код, содержащий различные тесты и пробы, проводившиеся во время работы над проектом, вряд ли его кто-нибудь будет читать, но мало ли.

Ноутбуки clusters_example, evaluation_example использовались в той части работы, когда планировалось отбирать похожие вопросы. Впоследствии направленность проекта изменилась, но эти файлы остались.