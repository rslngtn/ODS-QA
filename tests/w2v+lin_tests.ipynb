{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import gensim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import re\n",
    "from ufal.udpipe import Model, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('models/w2v/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor(model.vectors)\n",
    "embedding = nn.Embedding.from_pretrained(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Embedding(249565, 300)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding.from_pretrained(torch.FloatTensor(gensim.models.KeyedVectors.load_word2vec_format('models/w2v/model.txt').vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetClass(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for smart batching, that is each batch is only padded to its longest sequence instead of padding all\n",
    "    sequences to the max length.\n",
    "    The SentenceBertEncoder.smart_batching_collate is required for this to work.\n",
    "    SmartBatchingDataset does *not* work without it.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: np.array, target: np.array, vocab: list):\n",
    "        \"\"\"\n",
    "        Create a new Dataset with the tokenized texts and the labels as Tensor\n",
    "        \"\"\"\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tok2id = {}\n",
    "        for i, token in enumerate(vocab):\n",
    "            self.tok2id.update({token: i})\n",
    "\n",
    "        self.features = self.make_tokens(features)\n",
    "        self.target = target\n",
    "\n",
    "    def make_tokens(self, texts):\n",
    "        tokens = []\n",
    "        for text in tqdm(texts, desc='Tokenizing...'):\n",
    "            encoded_text = self.tokenizer.encode(text)\n",
    "            #tokens to indexes\n",
    "            indexes = []\n",
    "            for token in encoded_text:\n",
    "                try:\n",
    "                    indexes.append(self.tok2id[token])\n",
    "                #map oov tokens\n",
    "                except KeyError:\n",
    "                    indexes.append(-1)\n",
    "\n",
    "            tokens.append(indexes)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Transforms a batch from a Dataset to a batch of tensors for the model\n",
    "        :param batch:\n",
    "            a batch from a Dataset\n",
    "        :return:\n",
    "            a batch of tensors for the model\n",
    "        \"\"\"\n",
    "        tokens, labels = [], []\n",
    "        for token, label in batch:\n",
    "            tokens.append(token)\n",
    "            labels.append(label)\n",
    "\n",
    "        max_len = 0\n",
    "        for token in tokens:\n",
    "            max_len = max(max_len, len(token))\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            padding_length = max_len - len(token)\n",
    "            if padding_length > 0:\n",
    "                token = token + ([-2] * padding_length)\n",
    "\n",
    "            tokens[i] = torch.tensor(token)\n",
    "\n",
    "        tokens = torch.stack(tokens)\n",
    "\n",
    "        return tokens, torch.argmax(torch.tensor(labels, dtype=torch.long), dim=1)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.features[item], self.target[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "class Tokenizer(): \n",
    "    def __init__(self):\n",
    "        udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "        udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "        if not os.path.isfile(udpipe_filename):\n",
    "            print('UDPipe model not found. Downloading...')\n",
    "            wget.download(udpipe_model_url)\n",
    "\n",
    "        print('\\nLoading the model...')\n",
    "        self.model = Model.load(udpipe_filename)\n",
    "        self.process_pipeline = Pipeline(self.model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "        \n",
    "    def num_replace(self, word):\n",
    "        newtoken = 'x' * len(word)\n",
    "        return newtoken\n",
    "\n",
    "    def clean_token(self, token, misc):\n",
    "        \"\"\"\n",
    "        :param token:  токен (строка)\n",
    "        :param misc:  содержимое поля \"MISC\" в CONLLU (строка)\n",
    "        :return: очищенный токен (строка)\n",
    "        \"\"\"\n",
    "        out_token = token.strip().replace(' ', '')\n",
    "        if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
    "            return None\n",
    "        return out_token\n",
    "\n",
    "\n",
    "    def clean_lemma(self, lemma, pos):\n",
    "        \"\"\"\n",
    "        :param lemma: лемма (строка)\n",
    "        :param pos: часть речи (строка)\n",
    "        :return: очищенная лемма (строка)\n",
    "        \"\"\"\n",
    "        out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
    "        if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
    "            return None\n",
    "        if pos != 'PUNCT':\n",
    "            if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
    "                out_lemma = ''.join(out_lemma[1:])\n",
    "            if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
    "                out_lemma = ''.join(out_lemma[:-1])\n",
    "            if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
    "                    or out_lemma.endswith('.'):\n",
    "                out_lemma = ''.join(out_lemma[:-1])\n",
    "        return out_lemma\n",
    "\n",
    "\n",
    "    def list_replace(self, search, replacement, text):\n",
    "        search = [el for el in search if el in text]\n",
    "        for c in search:\n",
    "            text = text.replace(c, replacement)\n",
    "        return text\n",
    "\n",
    "\n",
    "    def unify_sym(self, text):  # принимает строку в юникоде\n",
    "        text = self.list_replace \\\n",
    "            ('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
    "\n",
    "        text = self.list_replace \\\n",
    "            ('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
    "\n",
    "        text = self.list_replace('\\u2010\\u2011', '\\u002D', text)\n",
    "\n",
    "        text = self.list_replace \\\n",
    "                (\n",
    "                '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n",
    "                '\\u2002', text)\n",
    "\n",
    "        text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
    "        text = re.sub('\\t\\t', '\\t', text)\n",
    "\n",
    "        text = self.list_replace \\\n",
    "                (\n",
    "                '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n",
    "                '.', text)\n",
    "\n",
    "        text = self.list_replace('\\u2217', '\\u002A', text)\n",
    "\n",
    "        text = self.list_replace('…', '...', text)\n",
    "\n",
    "        text = self.list_replace('\\u2241\\u224B\\u2E2F\\u0483', '\\u223D', text)\n",
    "\n",
    "        text = self.list_replace('\\u00C4', 'A', text)  # латинская\n",
    "        text = self.list_replace('\\u00E4', 'a', text)\n",
    "        text = self.list_replace('\\u00CB', 'E', text)\n",
    "        text = self.list_replace('\\u00EB', 'e', text)\n",
    "        text = self.list_replace('\\u1E26', 'H', text)\n",
    "        text = self.list_replace('\\u1E27', 'h', text)\n",
    "        text = self.list_replace('\\u00CF', 'I', text)\n",
    "        text = self.list_replace('\\u00EF', 'i', text)\n",
    "        text = self.list_replace('\\u00D6', 'O', text)\n",
    "        text = self.list_replace('\\u00F6', 'o', text)\n",
    "        text = self.list_replace('\\u00DC', 'U', text)\n",
    "        text = self.list_replace('\\u00FC', 'u', text)\n",
    "        text = self.list_replace('\\u0178', 'Y', text)\n",
    "        text = self.list_replace('\\u00FF', 'y', text)\n",
    "        text = self.list_replace('\\u00DF', 's', text)\n",
    "        text = self.list_replace('\\u1E9E', 'S', text)\n",
    "\n",
    "        currencies = list \\\n",
    "                (\n",
    "                '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n",
    "            )\n",
    "\n",
    "        alphabet = list \\\n",
    "                (\n",
    "                '\\t\\n\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯ,.[]{}()=+-−*&^%$#@!?~;:0123456789§/\\|\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "\n",
    "        alphabet.append(\"'\")\n",
    "\n",
    "        allowed = set(currencies + alphabet)\n",
    "\n",
    "        cleaned_text = [sym for sym in text if sym in allowed]\n",
    "        cleaned_text = ''.join(cleaned_text)\n",
    "\n",
    "        return cleaned_text\n",
    "\n",
    "\n",
    "    def process(self, pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "        # Если частеречные тэги не нужны (например, их нет в модели), выставьте pos=False\n",
    "        # в этом случае на выход будут поданы только леммы\n",
    "        # По умолчанию знаки пунктуации вырезаются. Чтобы сохранить их, выставьте punct=True\n",
    "\n",
    "        entities = {'PROPN'}\n",
    "        named = False\n",
    "        memory = []\n",
    "        mem_case = None\n",
    "        mem_number = None\n",
    "        tagged_propn = []\n",
    "\n",
    "        # обрабатываем текст, получаем результат в формате conllu:\n",
    "        processed = pipeline.process(text)\n",
    "\n",
    "        # пропускаем строки со служебной информацией:\n",
    "        content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "        # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "        tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "        for t in tagged:\n",
    "            if len(t) != 10:\n",
    "                continue\n",
    "            (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "            token = self.clean_token(token, misc)\n",
    "            lemma = self.clean_lemma(lemma, pos)\n",
    "            if not lemma or not token:\n",
    "                continue\n",
    "            if pos in entities:\n",
    "                if '|' not in feats:\n",
    "                    tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                    continue\n",
    "                morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "                if 'Case' not in morph or 'Number' not in morph:\n",
    "                    tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                    continue\n",
    "                if not named:\n",
    "                    named = True\n",
    "                    mem_case = morph['Case']\n",
    "                    mem_number = morph['Number']\n",
    "                if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                    memory.append(lemma)\n",
    "                    if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                        named = False\n",
    "                        past_lemma = '::'.join(memory)\n",
    "                        memory = []\n",
    "                        tagged_propn.append(past_lemma + '_PROPN')\n",
    "                else:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN')\n",
    "                    tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                if not named:\n",
    "                    if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                        lemma = self.num_replace(token)\n",
    "                    tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                else:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN')\n",
    "                    tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "        if not keep_punct:\n",
    "            tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "        if not keep_pos:\n",
    "            tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "        return tagged_propn\n",
    "\n",
    "    def encode(self, inp):\n",
    "        res = self.unify_sym(inp.strip())\n",
    "        output = self.process(self.process_pipeline, text=res)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Этот скрипт принимает на вход необработанный русский текст \n",
    "(одно предложение на строку или один абзац на строку).\n",
    "Он токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
    "На выход подаётся последовательность разделенных пробелами лемм с частями речи \n",
    "(\"зеленый_NOUN трамвай_NOUN\").\n",
    "Их можно непосредственно использовать в моделях с RusVectōrēs (https://rusvectores.org).\n",
    "Примеры запуска:\n",
    "echo 'Мама мыла раму.' | python3 rus_preprocessing_udpipe.py\n",
    "zcat large_corpus.txt.gz | python3 rus_preprocessing_udpipe.py | gzip > processed_corpus.txt.gz'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nLoading the model...\n"
    }
   ],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['этот_DET',\n 'скрипт_NOUN',\n 'принимать_VERB',\n 'на_ADP',\n 'вход_NOUN',\n 'необработать_ADJ',\n 'русский_ADJ',\n 'текст_NOUN',\n 'один_ADJ',\n 'предложение_NOUN',\n 'на_ADP',\n 'строка_NOUN',\n 'или_CCONJ',\n 'один_NUM',\n 'абзац_NOUN',\n 'на_ADP',\n 'строка_NOUN',\n 'он_PRON',\n 'токенизируться_VERB',\n 'лемматизируться_VERB',\n 'и_CCONJ',\n 'размечаться_VERB',\n 'по_ADP',\n 'часть_NOUN',\n 'речь_NOUN',\n 'с_ADP',\n 'использование_NOUN',\n 'udpipe_PROPN',\n 'на_ADP',\n 'выход_NOUN',\n 'подаваться_VERB',\n 'последовательность_NOUN',\n 'разделять_VERB',\n 'пробелай_NOUN',\n 'лем_NOUN',\n 'с_ADP',\n 'часть_NOUN',\n 'речь_NOUN',\n 'зеленыйnoun_PROPN',\n 'трамвайnoun_PROPN',\n 'они_PRON',\n 'можно_ADV',\n 'непосредственно_ADV',\n 'использовать_VERB',\n 'в_ADP',\n 'модель_NOUN',\n 'с_ADP',\n 'rusvectrs_PROPN',\n 'https://rusvectores.org_X',\n 'пример_NOUN',\n 'запуск_NOUN',\n 'echo_X',\n \"'мам_PROPN\",\n 'мыть_VERB',\n \"раму.'_NOUN\",\n 'python3_NUM',\n 'ruspreprocessingudpipe.py_X',\n 'zcat_X',\n 'largecorpus.txt.gz_PROPN',\n 'python3_NUM',\n 'ruspreprocessingudpipe.py_ADJ',\n 'gzip_X',\n 'processedcorpus.txt.gz_PROPN']"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "tok.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet('data/val.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Tokenizing...:   1%|          | 1/101 [00:00<00:11,  8.66it/s]\nLoading the model...\nTokenizing...: 100%|██████████| 101/101 [00:03<00:00, 30.10it/s]\n"
    }
   ],
   "source": [
    "train_ds = DatasetClass(features=df_train.loc[:100, 'question'].to_numpy(), target=df_train.drop('question', axis=1).loc[:100, :].to_numpy(), \n",
    "                        vocab=model.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(train_ds, collate_fn=train_ds.collate_fn,\n",
    "                                     batch_size=2, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(tensor([[    -1,   5901,      2,    863,  11939,   1288,     -1,   2914,      2,\n           1382,    538,     -1,     -1,    876,    632,     -1,     -1,   1864,\n            445,   6713,   1707,   8092,   3556,  13220,   2315,    538,   4272,\n           7798,     -1,     -1,  43734,     -1,   1288,     -1,    905,     -1,\n           2315,  63059,     -1,  11738,     -1,     23,    702,     -1,     -1,\n          31673,   2664,   3277,     -1, 204119,     -1,     69,     -1,     25,\n          45452,    791,   1695,     -1,      9,   1288,     -1,  32984,     -1,\n             28,     -1,   4570,    245,  80258,    159,     -1,    489,  31673,\n          34651,     -1,     -1,     -1,   1695,     -1,   1288,     87,     12,\n           1273,     -1,     25,      0,    712,  25307,     -1,     -1,     56,\n              1,      2,    106,      9,   2914,     -1,    400,     -1,     -1,\n            101,    661,     -1,     -1,     -1,     -1,  31673,   1288,     -1,\n          10572,     -1,  16019,    863,     -1,  20914,     -1,    489,      1,\n              2,     -1,   2848,  22055,   1684,     -1,    863,     -1,    712,\n             -1,   1647,     -1,     89,   2848,     -1,     -1,     43,      1,\n             -1,     -1,    796,     -1,    257,     30,    863,     -1,     -1,\n             49,     54,    106,     34,    159,    710,    479,  38971],\n        [    -1,     -1,     -1,     -1,   3341,     -1,   8399,     73,   8335,\n             -1,      0,   1214,     -1,     -1,   7885,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2]]), tensor([53,  4]))\n"
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_ds[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[0]\n",
    "labels = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-1, -1, 702, -1, -1, -1, 1506, 2417, -1, 923, -1, 445, 51196, 923, -1, -1, -1, 32477]\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "tokens, labels = [], []\n",
    "for token, label in batch:\n",
    "    print(label)\n",
    "    tokens.append(token)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for token in tokens:\n",
    "    max_len = max(max_len, len(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "210"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[tensor([    -1,   5901,    179,   3228,    101,     -1,      0,     -1,     15,\n            -1,   1764,     -1, 105089,    101,     -1,    197,     -1,    631,\n             2,    863,  69553,     -1,     91,    671,   1011,     -1,   1976,\n            44,     -1,     10,  56739,     -1,     91,   1976,     -1,   1976,\n            -1,    745,     -1,   6280,     -1,   5917,    304,    323,     -1,\n            -1,     -1,    257,     -1,     89,     73,    596,     -1,   1538,\n            -1,     -1,     70,     -1,     -1,    138,     -1,   1976,    671,\n            -1,     50,    670,  74687,     -1,     66,   1976,   2997,    191,\n            -1,     -1,  21695,     -1, 217725,     -1,    380,     -1,   1377,\n            -1,    635,  15971,    686,     -1,     -1,  15971,     -1,     -1,\n          1976,      0,     -1,  17727,   2030,     -1,      6,     -1,     -1,\n          1542,     -1,   1976,    671,     -1,     50,    670,      1,    939,\n            -1,  44886,   2823,     -1,      1,      2,     -1,     -1,    778,\n            -1,   4181,     -1,    387,    191,     -1,   2030,   5862,     -1,\n         10117,     -1,   1976,  74687,     -1,     -1,   1999,     -1,     -1,\n            75,     -1,  35312,   9986,    548,   1999,  46262,    876,   3390,\n            -1,   5841,  18813,     -1,     -1,     89,  13376,  17056,   1976,\n            -1,  30836,     -1,   5395,    232,   3929,   2257,    671,     -1,\n           189,     -1,     53,     -1,   4902,    778,     -1,     -1,     -1,\n            -1,    966,     -1,  44620,  13376,   4214,   2340,     -1,  24600,\n            -1,     -1,     -1,     -1,     -1,      1,  10698,     -1,     -1,\n            70,    304,     -1,  27208,   6683,   5137,     40,     -1,      1,\n            -1,    525,     -1,   7073,    745,   1976,  19479,     -1,  19038,\n          1938,    940,     -1]), tensor([   -1,    -1,   702,    -1,    -1,    -1,  1506,  2417,    -1,   923,\n           -1,   445, 51196,   923,    -1,    -1,    -1, 32477,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,\n           -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2,    -2])]\n"
    }
   ],
   "source": [
    "for i, token in enumerate(tokens):\n",
    "    padding_length = max_len - len(token)\n",
    "    if padding_length > 0:\n",
    "        token = token + ([-2] * padding_length)\n",
    "\n",
    "    tokens[i] = torch.tensor(token)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.stack(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[    -1,   5901,    179,   3228,    101,     -1,      0,     -1,     15,\n             -1,   1764,     -1, 105089,    101,     -1,    197,     -1,    631,\n              2,    863,  69553,     -1,     91,    671,   1011,     -1,   1976,\n             44,     -1,     10,  56739,     -1,     91,   1976,     -1,   1976,\n             -1,    745,     -1,   6280,     -1,   5917,    304,    323,     -1,\n             -1,     -1,    257,     -1,     89,     73,    596,     -1,   1538,\n             -1,     -1,     70,     -1,     -1,    138,     -1,   1976,    671,\n             -1,     50,    670,  74687,     -1,     66,   1976,   2997,    191,\n             -1,     -1,  21695,     -1, 217725,     -1,    380,     -1,   1377,\n             -1,    635,  15971,    686,     -1,     -1,  15971,     -1,     -1,\n           1976,      0,     -1,  17727,   2030,     -1,      6,     -1,     -1,\n           1542,     -1,   1976,    671,     -1,     50,    670,      1,    939,\n             -1,  44886,   2823,     -1,      1,      2,     -1,     -1,    778,\n             -1,   4181,     -1,    387,    191,     -1,   2030,   5862,     -1,\n          10117,     -1,   1976,  74687,     -1,     -1,   1999,     -1,     -1,\n             75,     -1,  35312,   9986,    548,   1999,  46262,    876,   3390,\n             -1,   5841,  18813,     -1,     -1,     89,  13376,  17056,   1976,\n             -1,  30836,     -1,   5395,    232,   3929,   2257,    671,     -1,\n            189,     -1,     53,     -1,   4902,    778,     -1,     -1,     -1,\n             -1,    966,     -1,  44620,  13376,   4214,   2340,     -1,  24600,\n             -1,     -1,     -1,     -1,     -1,      1,  10698,     -1,     -1,\n             70,    304,     -1,  27208,   6683,   5137,     40,     -1,      1,\n             -1,    525,     -1,   7073,    745,   1976,  19479,     -1,  19038,\n           1938,    940,     -1],\n        [    -1,     -1,    702,     -1,     -1,     -1,   1506,   2417,     -1,\n            923,     -1,    445,  51196,    923,     -1,     -1,     -1,  32477,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,     -2,\n             -2,     -2,     -2]])"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(torch.nn.Module): \n",
    "    def __init__(self, vectors): \n",
    "        super().__init__() \n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        # vector for oov \n",
    "        self.dim = self.embedding.embedding_dim \n",
    "        self.oov = torch.nn.Parameter(data=torch.rand(1, self.dim)) \n",
    "        self.oov_index = -1\n",
    "        self.pad = torch.nn.Parameter(data=torch.zeros(1, self.dim))\n",
    "        self.pad.requires_grad = False\n",
    "        self.pad_index = -2\n",
    "\n",
    "    def forward(self, arr): \n",
    "        N, M = arr.shape\n",
    "        mask =  (arr==self.oov_index).long() \n",
    "        mask_ = mask.unsqueeze(dim=2).expand(-1, -1, self.dim).float() \n",
    "        pad_mask = (arr==self.pad_index).long()\n",
    "        pad_ = pad_mask.unsqueeze(dim=2).expand(-1, -1, self.dim).float()\n",
    "        embed = (1-mask_)*(1-pad_)*self.embedding((1-mask)*((1-pad_mask)*arr))\\\n",
    "                + mask_*(self.oov.expand((N, M, self.dim))) + pad_*(self.pad.expand((N, M, self.dim)))\n",
    "\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedding(model.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Этот скрипт принимает на вход необработанный русский текст ',\n '(одно предложение на строку или один абзац на строку).',\n 'Он токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.',\n 'На выход подаётся последовательность разделенных пробелами лемм с частями речи ',\n '(\"зеленый_NOUN трамвай_NOUN\").',\n 'Их можно непосредственно использовать в моделях с RusVectōrēs (https://rusvectores.org).',\n 'Примеры запуска:',\n \"echo 'Мама мыла раму.' | python3 rus_preprocessing_udpipe.py\",\n 'zcat large_corpus.txt.gz | python3 rus_preprocessing_udpipe.py | gzip > processed_corpus.txt.gz']"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "texts = text.split('\\n')\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Tokenizing...: 100%|██████████| 9/9 [00:00<00:00, 421.63it/s]\nLoading the model...\n\n"
    }
   ],
   "source": [
    "ds = DatasetClass(features=texts, target=np.eye(len(texts), 2), vocab=model.index2word)\n",
    "dl = torch.utils.data.DataLoader(ds, collate_fn=ds.collate_fn, batch_size=3, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    x = batch[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[   631,   9634,     -2,     -2,     -2,     -2],\n        [186581,     -1,   4487,     -1,     -1,     -1],\n        [    -1,     -1,     -1,     -1,     -1,     -1]])"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 0.1304,  0.0264, -0.4213,  ..., -0.0208,  0.4676, -0.1550],\n        [ 0.9763,  0.8876,  0.8292,  ...,  0.8711,  0.0260,  0.2194],\n        [ 0.0089,  0.0826, -0.2734,  ..., -0.0286,  0.6359, -0.3102],\n        [ 0.9763,  0.8876,  0.8292,  ...,  0.8711,  0.0260,  0.2194],\n        [ 0.9763,  0.8876,  0.8292,  ...,  0.8711,  0.0260,  0.2194],\n        [ 0.9763,  0.8876,  0.8292,  ...,  0.8711,  0.0260,  0.2194]],\n       grad_fn=<SelectBackward>)"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask =  (arr==emb.oov_index).long() \n",
    "mask_ = mask.unsqueeze(dim=1).float() \n",
    "pad_mask = (arr==emb.pad_index).long()\n",
    "pad_ = pad_mask.unsqueeze(dim=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3, 1, 6])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "mask_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[   631,   9634,     -2,     -2,     -2,     -2],\n        [186581,     -1,   4487,     -1,     -1,     -1],\n        [    -1,     -1,     -1,     -1,     -1,     -1]])"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1]])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0, 0, 1, 1, 1, 1],\n        [0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0]])"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[   631,   9634,      0,      0,      0,      0],\n        [186581,      0,   4487,      0,      0,      0],\n        [     0,      0,      0,      0,      0,      0]])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "((1-mask)*((1-pad_mask)*arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = emb.embedding((1-mask)*((1-pad_mask)*arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = arr.shape[0]\n",
    "M = arr.shape[1]\n",
    "B = emb.oov.expand((N, M, emb.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = emb.pad.expand((N, M, emb.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([3, 6, 300])"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "mask.unsqueeze(2).expand(-1, -1, 300).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 1.0027,  0.3766,  0.1932,  ...,  0.9070,  0.7093,  0.0137],\n         [ 1.0005,  0.4485,  1.1348,  ...,  0.7723,  0.6764,  0.0473],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998]],\n\n        [[ 0.8453,  0.5549,  0.1379,  ...,  0.7751,  1.0436,  0.0220],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.7237,  0.6111,  0.2858,  ...,  0.7673,  1.2119, -0.1332],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998]],\n\n        [[ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998],\n         [ 0.4351,  0.0441,  0.6284,  ...,  0.7290,  0.5911,  0.0998]]],\n       grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "A+B+C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (300) at non-singleton dimension 2",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5641294c0b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmask_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpad_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpad_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (300) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "(1-mask_)*(1-pad_)*emb.embedding((1-mask)*((1-pad_mask)*arr))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "ods-qa",
   "display_name": "ODS-QA"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}