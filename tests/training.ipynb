{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text # Not used directly but needed to import TF ops.\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dot, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"models/USE_mul3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>reactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lang_go</td>\n",
       "      <td>Народ, кто знает, есть ли в GO библиотека для ...</td>\n",
       "      <td>Шатать таблички в го это, как летать на дачу н...</td>\n",
       "      <td>[{'name': 'heavy_plus_sign', 'count': 1}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lang_go</td>\n",
       "      <td>Всем привет, нуждаюсь в помощи по составлению ...</td>\n",
       "      <td>Зачем эта простыня текста. Ее традиционно никт...</td>\n",
       "      <td>[{'name': 'tnx', 'count': 1}, {'name': 'heavy_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lang_go</td>\n",
       "      <td>Всем привет, нуждаюсь в помощи по составлению ...</td>\n",
       "      <td>&gt; в проект Data Science\\nвыглядит как bullshit...</td>\n",
       "      <td>[{'name': '+1', 'count': 1}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lang_go</td>\n",
       "      <td>Всем привет, нуждаюсь в помощи по составлению ...</td>\n",
       "      <td>&lt;@U64GCA997&gt; сильно лучше :notbad:</td>\n",
       "      <td>[{'name': 'tnx', 'count': 1}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lang_go</td>\n",
       "      <td>Всем привет, нуждаюсь в помощи по составлению ...</td>\n",
       "      <td>Что такое бустануть? Что за нужно уметь в них?...</td>\n",
       "      <td>[{'name': 'heavy_plus_sign', 'count': 1}, {'na...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   channel                                           question  \\\n",
       "0  lang_go  Народ, кто знает, есть ли в GO библиотека для ...   \n",
       "1  lang_go  Всем привет, нуждаюсь в помощи по составлению ...   \n",
       "2  lang_go  Всем привет, нуждаюсь в помощи по составлению ...   \n",
       "3  lang_go  Всем привет, нуждаюсь в помощи по составлению ...   \n",
       "4  lang_go  Всем привет, нуждаюсь в помощи по составлению ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Шатать таблички в го это, как летать на дачу н...   \n",
       "1  Зачем эта простыня текста. Ее традиционно никт...   \n",
       "2  > в проект Data Science\\nвыглядит как bullshit...   \n",
       "3                 <@U64GCA997> сильно лучше :notbad:   \n",
       "4  Что такое бустануть? Что за нужно уметь в них?...   \n",
       "\n",
       "                                           reactions  \n",
       "0          [{'name': 'heavy_plus_sign', 'count': 1}]  \n",
       "1  [{'name': 'tnx', 'count': 1}, {'name': 'heavy_...  \n",
       "2                       [{'name': '+1', 'count': 1}]  \n",
       "3                      [{'name': 'tnx', 'count': 1}]  \n",
       "4  [{'name': 'heavy_plus_sign', 'count': 1}, {'na...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('data/not_load/qa_short.parquet'); df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>q_id</th>\n",
       "      <th>ans_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Народ, кто знает, есть ли в GO библиотека для ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Всем привет, нуждаюсь в помощи по составлению ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1,2,3,4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>и еще вопрос, нет ли веяний, что этот проект п...</td>\n",
       "      <td>5</td>\n",
       "      <td>5,6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Кто-нибудь здесь проходил\\n&lt;http://coursera.or...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahh folks, anyone is able to submit an lstm+en...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  q_id  ans_ids\n",
       "0  Народ, кто знает, есть ли в GO библиотека для ...     0        0\n",
       "1  Всем привет, нуждаюсь в помощи по составлению ...     1  1,2,3,4\n",
       "2  и еще вопрос, нет ли веяний, что этот проект п...     5      5,6\n",
       "3  Кто-нибудь здесь проходил\\n<http://coursera.or...     7        7\n",
       "4  Ahh folks, anyone is able to submit an lstm+en...     8        8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = pd.read_parquet('data/not_load/qa_sim.parquet').loc[:, ['question', 'q_id', 'ans_ids']]; graph.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отбираем пары для отладки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Привет всем опять!\\n\\nУ меня есть конфиг, в ко...</td>\n",
       "      <td>Привет :slightly_smiling_face: Да, все верно. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>А есть у кого нибудь внятное объяснение с како...</td>\n",
       "      <td>хм. просто мне наврерное не нравится идея все ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>А есть у кого нибудь внятное объяснение с како...</td>\n",
       "      <td>но, как уже заметили можно начать и без “пайпл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>А есть у кого нибудь внятное объяснение с како...</td>\n",
       "      <td>к слову, если использовать кукикаттер и шаги +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Есть более подробный туториал как удалять данн...</td>\n",
       "      <td>Только не пользуйтесь `-c`, если у вас один ре...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "213  Привет всем опять!\\n\\nУ меня есть конфиг, в ко...   \n",
       "214  А есть у кого нибудь внятное объяснение с како...   \n",
       "215  А есть у кого нибудь внятное объяснение с како...   \n",
       "216  А есть у кого нибудь внятное объяснение с како...   \n",
       "217  Есть более подробный туториал как удалять данн...   \n",
       "\n",
       "                                                answer  \n",
       "213  Привет :slightly_smiling_face: Да, все верно. ...  \n",
       "214  хм. просто мне наврерное не нравится идея все ...  \n",
       "215  но, как уже заметили можно начать и без “пайпл...  \n",
       "216  к слову, если использовать кукикаттер и шаги +...  \n",
       "217  Только не пользуйтесь `-c`, если у вас один ре...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pairs = df.loc[213:217, ['question', 'answer']]; test_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>ans_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>214</td>\n",
       "      <td>214,215,216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>217</td>\n",
       "      <td>217,218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     q_id      ans_ids\n",
       "137   213          213\n",
       "138   214  214,215,216\n",
       "139   217      217,218"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph = graph.loc[graph['question'].isin(test_pairs['question']), ['q_id', 'ans_ids']]; test_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[213],\n",
       " [214, 215, 216],\n",
       " [214, 215, 216],\n",
       " [214, 215, 216],\n",
       " [217, 218],\n",
       " [217, 218]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = []\n",
    "q_ids = []\n",
    "for _, row in test_graph.iterrows():\n",
    "    el = eval(row['ans_ids'])\n",
    "    if isinstance(el, int):\n",
    "        similarity.append([el])\n",
    "        q_ids.append(row['q_id'])\n",
    "    else:\n",
    "        for i, answer in enumerate(el):\n",
    "            similarity.append(list(el))\n",
    "            q_ids.append(el[i])\n",
    "\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[213, 214, 215, 216, 217, 218]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sim_list(sim, true_ids):\n",
    "    \"\"\"\n",
    "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
    "    \"\"\"\n",
    "    max_len = max([len(x) for x in similarity])\n",
    "    mat = np.full((len(sim), max_len), -1)\n",
    "    for row, column in enumerate(sim):\n",
    "        mat[row, range(len(column))] = column\n",
    "\n",
    "    return np.hstack((mat, np.expand_dims(np.array(true_ids), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[213,  -1,  -1, 213],\n",
       "       [214, 215, 216, 214],\n",
       "       [214, 215, 216, 215],\n",
       "       [214, 215, 216, 216],\n",
       "       [217, 218,  -1, 217],\n",
       "       [217, 218,  -1, 218]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = pad_sim_list(similarity, q_ids); similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = len(q_ids)\n",
    "labels = np.zeros((shape, shape), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ids = similarity[:, -1]\n",
    "similarity = np.delete(similarity, -1, 1)\n",
    "for i, row in enumerate(similarity):\n",
    "    for el in row:\n",
    "        j = np.argwhere(true_ids == el)\n",
    "        if j != []:\n",
    "            j = np.argwhere(true_ids == el).item()\n",
    "        labels[i, j] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False, False],\n",
       "       [False,  True,  True,  True, False, False],\n",
       "       [False,  True,  True,  True, False, False],\n",
       "       [False,  True,  True,  True, False, False],\n",
       "       [False, False, False, False,  True,  True],\n",
       "       [False, False, False, False,  True,  True]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Computes the triplet loss with self-labeled positive pairs.\n",
    "    The loss encourages the positive distances (between a pair of embeddings\n",
    "    with the same labels) to be smaller than the minimum negative distance\n",
    "    among which are at least greater than the positive distance plus the\n",
    "    margin constant (called semi-hard negative) in the mini-batch.\n",
    "    If no such negative exists, uses the largest negative distance instead.\n",
    "    See: https://arxiv.org/abs/1503.03832.\n",
    "    We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n",
    "    [batch_size] of multi-class integer labels. And embeddings `y_pred` must be\n",
    "    2-D float `Tensor` of l2 normalized embedding vectors.\n",
    "    Args:\n",
    "      margin: Float, margin term in the loss definition. Default value is 1.0.\n",
    "      name: Optional name for the op.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, margin = 1.0, name = None, **kwargs\n",
    "    ):\n",
    "        super().__init__(name=name, reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, similarity, dot_product):\n",
    "        shape = dot_product.shape[0]\n",
    "        labels = np.zeros((shape, shape), dtype=bool)\n",
    "        true_ids = similarity[:, -1]\n",
    "        similarity = np.delete(similarity, -1, 1)\n",
    "        for i, row in enumerate(similarity):\n",
    "            for el in row:\n",
    "                j = np.argwhere(true_ids == el)\n",
    "                if j != []:\n",
    "                    j = np.argwhere(true_ids == el).item()\n",
    "                labels[i, j] = True\n",
    "\n",
    "        s_pos = dot_product*tf.cast(labels, tf.float32)\n",
    "        s_neg = dot_product*tf.cast(tf.math.logical_not(tf.cast(labels, tf.bool)), tf.float32)\n",
    "\n",
    "        loss = tf.maximum(0.0, self.margin - s_pos + s_neg)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"margin\": self.margin,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Computes the crossentopy loss with self-labeled positive pairs.\n",
    "    Args:\n",
    "      name: Optional name for the op.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, name = None, **kwargs\n",
    "    ):\n",
    "        super().__init__(name=name, reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.cross = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    def call(self, similarity, dot_product):\n",
    "        shape = dot_product.shape[0]\n",
    "        labels = np.zeros((shape, shape), dtype=bool)\n",
    "        true_ids = similarity[:, -1]\n",
    "        similarity = np.delete(similarity, -1, 1)\n",
    "        for i, row in enumerate(similarity):\n",
    "            for el in row:\n",
    "                j = np.argwhere(true_ids == el)\n",
    "                if j != []:\n",
    "                    j = np.argwhere(true_ids == el).item()\n",
    "                labels[i, j] = True\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "\n",
    "        loss = self.cross(labels, dot_product)\n",
    "        return loss\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим модель, получающую эмбеддинги вопросов и ответов и строящую матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = Input(shape=(1,), dtype=tf.string, name='Question')\n",
    "answer = Input(shape=(1,), dtype=tf.string, name='Answer')\n",
    "use = hub.KerasLayer(path_to_model, trainable=True, name='USE')\n",
    "#encode questions and answers\n",
    "q_emb = use(tf.squeeze(tf.cast(question, tf.string)))\n",
    "a_emb = use(tf.squeeze(tf.cast(answer, tf.string)))\n",
    "#apply cosine similarity function\n",
    "dist_matrix = 1 - tf.matmul(tf.math.l2_normalize(q_emb, axis=1), tf.math.l2_normalize(tf.transpose(a_emb), axis=0))\n",
    "model = Model(inputs=[question, answer], outputs=dist_matrix)\n",
    "model.compile(loss=TripletLoss(), optimizer=opt, run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Question (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Answer (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [None]               0           Question[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_1 (TensorFl [None]               0           Answer[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "USE (KerasLayer)                (None, 512)          68927232    tf_op_layer_Squeeze[0][0]        \n",
      "                                                                 tf_op_layer_Squeeze_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose (TensorFl [(512, None)]        0           USE[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_l2_normalize/Square [(None, 512)]        0           USE[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_l2_normalize_1/Squa [(512, None)]        0           tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_l2_normalize/Sum (T [(None, 1)]          0           tf_op_layer_l2_normalize/Square[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_l2_normalize_1/Sum  [(1, None)]          0           tf_op_layer_l2_normalize_1/Square\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_l2_normalize/Maximu [(None, 1)]          0           tf_op_layer_l2_normalize/Sum[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_l2_normalize_1/Maxi [(1, None)]          0           tf_op_layer_l2_normalize_1/Sum[0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_l2_normalize/Rsqrt  [(None, 1)]          0           tf_op_layer_l2_normalize/Maximum[\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_l2_normalize_1/Rsqr [(1, None)]          0           tf_op_layer_l2_normalize_1/Maximu\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_l2_normalize (Tenso [(None, 512)]        0           USE[0][0]                        \n",
      "                                                                 tf_op_layer_l2_normalize/Rsqrt[0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_l2_normalize_1 (Ten [(512, None)]        0           tf_op_layer_transpose[0][0]      \n",
      "                                                                 tf_op_layer_l2_normalize_1/Rsqrt[\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul (TensorFlowO [(None, None)]       0           tf_op_layer_l2_normalize[0][0]   \n",
      "                                                                 tf_op_layer_l2_normalize_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub (TensorFlowOpLa [(None, None)]       0           tf_op_layer_MatMul[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 68,927,232\n",
      "Trainable params: 68,927,232\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"Question:0\", shape=(None, 1), dtype=string) for input (None, 1), but it was re-called on a Tensor with incompatible shape (5,).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"Answer:0\", shape=(None, 1), dtype=string) for input (None, 1), but it was re-called on a Tensor with incompatible shape (5,).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[0.9058144 , 0.8884254 , 0.7597515 , 0.9857815 , 0.7766231 ],\n",
       "       [1.0161836 , 0.82771003, 0.58489823, 0.9291676 , 0.7595733 ],\n",
       "       [1.0161836 , 0.82771003, 0.58489823, 0.9291676 , 0.7595733 ],\n",
       "       [1.0161836 , 0.82771003, 0.58489823, 0.9291676 , 0.7595733 ],\n",
       "       [1.0161837 , 0.82771003, 0.5848982 , 0.9291676 , 0.7595732 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([test_pairs['question'].to_numpy(), test_pairs['answer'].to_numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "similarity = []\n",
    "true_ids = []\n",
    "for _, row in graph.iterrows():\n",
    "    el = eval(row['ans_ids'])\n",
    "    if isinstance(el, int):\n",
    "        similarity.append([el])\n",
    "        true_ids.append(row['q_id'])\n",
    "    else:\n",
    "        for i, answer in enumerate(el):\n",
    "            similarity.append(list(el))\n",
    "            true_ids.append(el[i])\n",
    "\n",
    "print(len(similarity) == len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sim_list(sim, true_ids):\n",
    "    \"\"\"\n",
    "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
    "    \"\"\"\n",
    "    max_len = max([len(x) for x in similarity])\n",
    "    mat = np.full((len(sim), max_len), -1)\n",
    "    for row, column in enumerate(sim):\n",
    "        mat[row, range(len(column))] = column\n",
    "\n",
    "    return np.hstack((mat, np.expand_dims(np.array(true_ids), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49394 samples\n",
      "Epoch 1/4\n",
      "49394/49394 [==============================] - 2336s 47ms/sample - loss: 1.1131\n",
      "Epoch 2/4\n",
      "49394/49394 [==============================] - 1669s 34ms/sample - loss: 1.0004\n",
      "Epoch 3/4\n",
      "49394/49394 [==============================] - 1661s 34ms/sample - loss: 1.0001\n",
      "Epoch 4/4\n",
      "49394/49394 [==============================] - 1660s 34ms/sample - loss: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train_hist = model.fit([df['question'].to_numpy(), df['answer'].to_numpy()], \n",
    "                       pad_sim_list(similarity, true_ids),\n",
    "                       batch_size=64,\n",
    "                       epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим метрику после пробного обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 328/328 [01:13<00:00,  4.48it/s]\n",
      "Calculating embeddings: 100%|██████████| 771/771 [01:42<00:00,  7.55it/s]\n",
      "Searching for top k texts for all inputs: 100%|██████████| 21018/21018 [13:22<00:00, 26.20it/s]\n",
      "0.0017150234291494165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.evaluation import evaluate\n",
    "\n",
    "use = model.layers[4]\n",
    "questions = df['question'].unique()\n",
    "answers = df['answer']\n",
    "similarity = []\n",
    "for st in graph['ans_ids'].to_numpy():\n",
    "    el = eval(st)\n",
    "    if isinstance(el, int):\n",
    "        similarity.append([el])\n",
    "    else:\n",
    "        similarity.append(list(el))\n",
    "\n",
    "map10 = evaluate(use, questions, answers, similarity)\n",
    "print('')\n",
    "print(map10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем с другим лоссом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49394 samples\n",
      "Epoch 1/4\n",
      "49394/49394 [==============================] - 1724s 35ms/sample - loss: 4.2306\n",
      "Epoch 2/4\n",
      "49394/49394 [==============================] - 1694s 34ms/sample - loss: 4.1869\n",
      "Epoch 3/4\n",
      "49394/49394 [==============================] - 1700s 34ms/sample - loss: 4.1953\n",
      "Epoch 4/4\n",
      "49394/49394 [==============================] - 2025s 41ms/sample - loss: 4.1880\n"
     ]
    }
   ],
   "source": [
    "from modules.training import train\n",
    "\n",
    "model, train_hist = train(path_to_model, CrossEntropyLoss(), opt, \n",
    "                          [df['question'].to_numpy(), df['answer'].to_numpy()],\n",
    "                          pad_sim_list(similarity, true_ids),\n",
    "                          batch_size=64,\n",
    "                          epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим метрику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 328/328 [01:18<00:00,  4.16it/s]\n",
      "Calculating embeddings: 100%|██████████| 771/771 [01:49<00:00,  7.04it/s]\n",
      "Searching for top k texts for all inputs: 100%|██████████| 21018/21018 [12:24<00:00, 28.24it/s]\n",
      "0.00013569141122508077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.evaluation import evaluate\n",
    "\n",
    "use = model.layers[4]\n",
    "questions = df['question'].unique()\n",
    "answers = df['answer']\n",
    "similarity = []\n",
    "for st in graph['ans_ids'].to_numpy():\n",
    "    el = eval(st)\n",
    "    if isinstance(el, int):\n",
    "        similarity.append([el])\n",
    "    else:\n",
    "        similarity.append(list(el))\n",
    "\n",
    "map10 = evaluate(use, questions, answers, similarity)\n",
    "print('')\n",
    "print(map10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пробуем обучать SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modules.training import SBERT, SentencesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/not_load/qa_short.parquet')\n",
    "graph = pd.read_parquet('data/not_load/qa_sim.parquet').loc[:, ['question', 'q_id', 'ans_ids']]\n",
    "df['question'] = df['question'].str.lower()\n",
    "df['answer'] = df['answer'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "model = SBERT('distiluse-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = df.loc[:, ['question', 'answer']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "similarity = []\n",
    "true_ids = []\n",
    "for _, row in graph.iterrows():\n",
    "    el = eval(row['ans_ids'])\n",
    "    if isinstance(el, int):\n",
    "        similarity.append([el])\n",
    "        true_ids.append(row['q_id'])\n",
    "    else:\n",
    "        for i, answer in enumerate(el):\n",
    "            similarity.append(list(el))\n",
    "            true_ids.append(el[i])\n",
    "\n",
    "print(len(similarity) == len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sim_list(sim, true_ids):\n",
    "    \"\"\"\n",
    "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
    "    \"\"\"\n",
    "    max_len = max([len(x) for x in similarity])\n",
    "    mat = np.full((len(sim), max_len), -1)\n",
    "    for row, column in enumerate(sim):\n",
    "        mat[row, range(len(column))] = column\n",
    "\n",
    "    return np.hstack((mat, np.expand_dims(np.array(true_ids), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert dataset: 100%|██████████| 49394/49394 [01:16<00:00, 642.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences 0 longer than max_sequence_length: 13931\n",
      "Sentences 1 longer than max_sequence_length: 4296\n"
     ]
    }
   ],
   "source": [
    "ds = SentencesDataset(train_set, pad_sim_list(similarity, true_ids), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = DataLoader(ds, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilarityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, seq_a, seq_b, label):\n",
    "        loss_fct = torch.nn.CosineEmbeddingLoss()\n",
    "        device = seq_a.device\n",
    "        y = torch.tensor([1]*seq_a.size(0)).to(device)\n",
    "\n",
    "        loss = loss_fct(seq_a, seq_b, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleNegativesRankingLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        super(MultipleNegativesRankingLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, seq_a, seq_b, similarity):\n",
    "        scores = torch.matmul(seq_a, seq_b.t())\n",
    "        \n",
    "        shape = scores.shape[0]\n",
    "        labels = np.zeros((shape, shape), dtype=bool)\n",
    "        similarity = similarity.detach().cpu().numpy()\n",
    "        true_ids = similarity[:, -1]\n",
    "        similarity = np.delete(similarity, -1, 1)\n",
    "        for i, row in enumerate(similarity):\n",
    "            for el in row:\n",
    "                j = np.argwhere(true_ids == el)\n",
    "                if j != []:\n",
    "                    j = np.argwhere(true_ids == el).item()\n",
    "                labels[i, j] = True\n",
    "        labels = torch.tensor(labels).to(scores.device)\n",
    "        \n",
    "        s_pos = scores*labels\n",
    "        s_neg = scores*(~labels)\n",
    "        \n",
    "        loss = torch.clamp(self.margin - s_pos + s_neg, min=0.0)\n",
    "        loss = torch.mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.0463: 100%|██████████| 3088/3088 [21:18<00:00,  2.42it/s]\n",
      "Epoch 2 Loss 0.000241: 100%|██████████| 3088/3088 [21:38<00:00,  2.38it/s]\n",
      "Epoch 3 Loss 3.39e-05: 100%|██████████| 3088/3088 [21:39<00:00,  2.38it/s]\n",
      "Epoch 4 Loss 6.27e-06: 100%|██████████| 3088/3088 [21:40<00:00,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "train_objective = (tdl, CosineSimilarityLoss())\n",
    "model.fit(train_objective, epoches=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.evaluation import evaluate\n",
    "\n",
    "questions = df['question'].unique()\n",
    "answers = df['answer'].to_numpy()\n",
    "similarity = []\n",
    "for st in graph['ans_ids'].to_numpy():\n",
    "    el = eval(st)\n",
    "    if isinstance(el, int):\n",
    "        similarity.append([el])\n",
    "    else:\n",
    "        similarity.append(list(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 328/328 [03:26<00:00,  1.59it/s]\n",
      "Calculating embeddings: 100%|██████████| 771/771 [07:17<00:00,  1.76it/s]\n",
      "Searching for top k texts for all inputs: 100%|██████████| 21018/21018 [14:49<00:00, 23.63it/s]\n",
      "0.0005444130528581951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "map10 = evaluate(model, questions, answers, similarity, model_type='sbert', batch_size=64)\n",
    "print('')\n",
    "print(map10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем триплет лосс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/3088 [00:00<?, ?it/s]E:\\Programs\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "Epoch 1 Loss 0.361: 100%|██████████| 3088/3088 [22:12<00:00,  2.32it/s]\n",
      "Epoch 2 Loss 0.198: 100%|██████████| 3088/3088 [22:37<00:00,  2.27it/s]\n",
      "Epoch 3 Loss 0.18: 100%|██████████| 3088/3088 [22:41<00:00,  2.27it/s] \n",
      "Epoch 4 Loss 0.162: 100%|██████████| 3088/3088 [22:40<00:00,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train_objective = (tdl, MultipleNegativesRankingLoss())\n",
    "model.fit(train_objective, epoches=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Calculating embeddings: 100%|██████████| 328/328 [03:22<00:00,  1.62it/s]\nCalculating embeddings: 100%|██████████| 771/771 [05:52<00:00,  2.18it/s]\nSearching for top k texts for all inputs: 100%|██████████| 21018/21018 [12:12<00:00, 28.69it/s]\n8.815195440944797e-05\n\n"
    }
   ],
   "source": [
    "from modules.evaluation import evaluate\n",
    "\n",
    "questions = df['question'].unique()\n",
    "answers = df['answer'].to_numpy()\n",
    "similarity = []\n",
    "for st in graph['ans_ids'].to_numpy():\n",
    "    el = eval(st)\n",
    "    if isinstance(el, int):\n",
    "        similarity.append([el])\n",
    "    else:\n",
    "        similarity.append(list(el))\n",
    "        \n",
    "map10 = evaluate(model, questions, answers, similarity, model_type='sbert', batch_size=64)\n",
    "print('')\n",
    "print(map10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заготовка тренировочного велосипеда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "batch_size = 2\n",
    "q = test_pairs['question'].to_numpy()\n",
    "a = test_pairs['answer'].to_numpy()\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((q, a, pad_sim_list(similarity)))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"Question:0\", shape=(None, 1), dtype=string) for input (None, 1), but it was re-called on a Tensor with incompatible shape (2,).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"Answer:0\", shape=(None, 1), dtype=string) for input (None, 1), but it was re-called on a Tensor with incompatible shape (2,).\n",
      "tf.Tensor(\n",
      "[[0.6475699  0.2517897 ]\n",
      " [0.6475699  0.25178975]], shape=(2, 2), dtype=float32)\n",
      "<__main__.TripletLoss object at 0x7fd8b0148cd0>\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"Question:0\", shape=(None, 1), dtype=string) for input (None, 1), but it was re-called on a Tensor with incompatible shape (2,).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"Answer:0\", shape=(None, 1), dtype=string) for input (None, 1), but it was re-called on a Tensor with incompatible shape (2,).\n",
      "tf.Tensor(\n",
      "[[1.258095   0.        ]\n",
      " [0.34368467 0.6485029 ]], shape=(2, 2), dtype=float32)\n",
      "<__main__.TripletLoss object at 0x7fd8b0148cd0>\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"Question:0\", shape=(None, 1), dtype=string) for input (None, 1), but it was re-called on a Tensor with incompatible shape (1,).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"Answer:0\", shape=(None, 1), dtype=string) for input (None, 1), but it was re-called on a Tensor with incompatible shape (1,).\n",
      "tf.Tensor([[1.1938863]], shape=(1, 1), dtype=float32)\n",
      "<__main__.TripletLoss object at 0x7fd8b0148cd0>\n"
     ]
    }
   ],
   "source": [
    "trl = TripletLoss()\n",
    "for batch in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model((batch[0], batch[1]))\n",
    "      loss_value = trl(batch[2], logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text # Not used directly but needed to import TF ops.\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dot, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   channel                                           question  \\\n0  lang_go  Народ, кто знает, есть ли в GO библиотека для ...   \n1  lang_go  Народ, кто знает, есть ли в GO библиотека для ...   \n2  lang_go  Народ, кто знает, есть ли в GO библиотека для ...   \n3  lang_go  Народ, кто знает, есть ли в GO библиотека для ...   \n4  lang_go  Народ, кто знает, есть ли в GO библиотека для ...   \n\n                                              answer  \\\n0  В python. Аналог pandas есть только в R, но та...   \n1  Шатать таблички в го это, как летать на дачу н...   \n2                                           В Julia.   \n3  Вообще есть такая поделка - <https://github.co...   \n4  Ну и arrow есть, правда пока там мало что можн...   \n\n                                   reactions  \n0                                        nan  \n1  [{'name': 'heavy_plus_sign', 'count': 1}]  \n2                                        nan  \n3                                        nan  \n4                                        nan  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>channel</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>reactions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>lang_go</td>\n      <td>Народ, кто знает, есть ли в GO библиотека для ...</td>\n      <td>В python. Аналог pandas есть только в R, но та...</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>lang_go</td>\n      <td>Народ, кто знает, есть ли в GO библиотека для ...</td>\n      <td>Шатать таблички в го это, как летать на дачу н...</td>\n      <td>[{'name': 'heavy_plus_sign', 'count': 1}]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>lang_go</td>\n      <td>Народ, кто знает, есть ли в GO библиотека для ...</td>\n      <td>В Julia.</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>lang_go</td>\n      <td>Народ, кто знает, есть ли в GO библиотека для ...</td>\n      <td>Вообще есть такая поделка - &lt;https://github.co...</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>lang_go</td>\n      <td>Народ, кто знает, есть ли в GO библиотека для ...</td>\n      <td>Ну и arrow есть, правда пока там мало что можн...</td>\n      <td>nan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "path_to_model = \"models/USE_mul3\"\n",
    "df = pd.read_parquet('data/qa_classif.parquet'); df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 398292/398292 [00:32<00:00, 12403.68it/s]\n\nОтветов выбросить 38244, количество записей станет 360048\n"
    }
   ],
   "source": [
    "indxs = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if any(['proj' in row['channel'], 'meetings' in row['channel']]):\n",
    "        indxs.append(i)\n",
    "\n",
    "print(f'\\nОтветов выбросить {len(df.loc[indxs])}, количество записей станет {len(df)-len(df.loc[indxs])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index=indxs, axis=0, inplace=True)\n",
    "df.drop(index=[df[df['channel'] == 'ods_challenges'].index[0],\n",
    "               df[df['channel'] == '_top_jobs'].index[0],\n",
    "               df[df['channel'] == 'welcome'].index[0]], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "career                    65991\n_random_talks             40323\ndeep_learning             32524\ntheory_and_practice       27765\nmlcourse_ai_news          20659\nlang_python               20642\nnlp                       16848\nhardware                  14236\nkaggle_crackers            8038\nedu_courses                6640\nods_sport                  6377\nmlcourse_ai                5685\nmltrainings_beginners      5658\ndevops                     5622\nbig_data                   5385\ncv                         5355\n_random_b                  5333\ndatasets                   4058\nblockchain                 3658\ngnomiki                    3373\nsatellite_imaging          3145\nclass_dl_cmu               2908\nlang_r                     2728\nvisualization              2494\ntrading                    1990\nclass_cs231n               1961\nedu_books                  1938\nml_pipeline                1817\ntool_catalyst              1680\n_jobs                      1624\nods_habr                   1574\nmlcourse_ai_rus            1481\nmath                       1480\nedu_academy                1435\ninteresting_links          1399\ndlcourse_ai                1315\nreinforcement_learning     1310\nrecommender_systems        1278\ndata_fest                  1132\nsequences_series           1087\n_call_4_collaboration      1084\nsberbank_contest           1082\ntool_albumentations        1047\nconference                 1039\nwelcome                    1006\ncloud                       899\ntool_dvc                    880\nself_driving                862\nmedicine                    820\nods_resume_mastering        802\nbusiness                    702\ngis                         660\naudio_and_speech            619\nbayesian                    603\nlang_cpp                    599\ndeephack                    517\nclass_fastai                489\nnetwork_analysis            486\nrandomcoffee                394\narticle_essence             350\ntool_catboost               298\nds_process                  289\nclass_coursera_aml          258\nlang_scala                  251\nods_travel                  247\ndata_breakfast              209\nclass_udacity_pytorch       201\nbioinformatics              198\n_random_politics            191\nods_sport_sf                188\nods_policy_prior            177\nmltrainings_live            148\nwriting_n_presenting        145\nedu_shad_exams              141\nods_merch                   135\nsecurity                    127\npicsartaihack               119\nlang_go                     113\nlooking_for_a_job           103\nName: channel, dtype: int64\n"
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df['channel'].value_counts()[df['channel'].value_counts() > 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = df['channel'].value_counts()[df['channel'].value_counts() <= 100].index\n",
    "df.drop(index=df[df['channel'].isin(to_drop)].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.get_dummies(df['channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {}\n",
    "total = df['channel'].nunique()\n",
    "for i, num in enumerate(df['channel'].value_counts().sort_index()):\n",
    "    class_weights.update({i: total/num})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/label_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(class_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['question'], df_labels, test_size=0.2, shuffle=True, stratify=df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, shuffle=True, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1, ignore_index=True); train.columns = ['question', *df_labels.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.concat([X_val, y_val], axis=1, ignore_index=True); val.columns = ['question', *df_labels.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([X_test, y_test], axis=1, ignore_index=True); test.columns = ['question', *df_labels.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet('data/train.parquet', compression='brotli', index=False)\n",
    "val.to_parquet('data/val.parquet', compression='brotli', index=False)\n",
    "test.to_parquet('data/test.parquet', compression='brotli', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0,  \n",
    "                                                  name='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc', verbose=1,\n",
    "                                                  patience=4, mode='max', restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('models/USE_classif.h5', monitor='val_auc', verbose=1, \n",
    "                             save_best_only=True, save_weights_only=False, mode='max', save_freq='epoch')\n",
    "reduse_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.1, patience=2, verbose=1, mode='max',\n",
    "                              min_delta=0.001, cooldown=1, min_lr=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.distribute import distribution_strategy_context as distribute_ctx\n",
    "from tensorflow.python.eager import context, def_function\n",
    "from tensorflow.python.framework import constant_op, tensor_spec, tensor_shape, ops, dtypes\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.engine import base_layer\n",
    "from tensorflow.python.keras.engine import base_layer_utils\n",
    "from tensorflow.python.keras.losses import categorical_crossentropy\n",
    "from tensorflow.python.keras.utils import metrics_utils\n",
    "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
    "from tensorflow.python.keras.utils.generic_utils import serialize_keras_object\n",
    "from tensorflow.python.keras.utils.generic_utils import to_list\n",
    "from tensorflow.python.keras.utils.tf_utils import is_tensor_or_variable\n",
    "from tensorflow.python.ops import array_ops, weights_broadcast_ops, confusion_matrix, check_ops, control_flow_ops, math_ops, init_ops, nn\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "from tensorflow.python.ops.losses import util as tf_losses_utils\n",
    "from tensorflow.python.training.tracking import base as trackable\n",
    "from tensorflow.python.util import nest, tf_inspect\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from tensorflow.tools.docs import doc_controls\n",
    "\n",
    "class AUC(tf.keras.metrics.Metric):\n",
    "  \"\"\"Computes the approximate AUC (Area under the curve) via a Riemann sum.\n",
    "  This metric creates four local variables, `true_positives`, `true_negatives`,\n",
    "  `false_positives` and `false_negatives` that are used to compute the AUC.\n",
    "  To discretize the AUC curve, a linearly spaced set of thresholds is used to\n",
    "  compute pairs of recall and precision values. The area under the ROC-curve is\n",
    "  therefore computed using the height of the recall values by the false positive\n",
    "  rate, while the area under the PR-curve is the computed using the height of\n",
    "  the precision values by the recall.\n",
    "  This value is ultimately returned as `auc`, an idempotent operation that\n",
    "  computes the area under a discretized curve of precision versus recall values\n",
    "  (computed using the aforementioned variables). The `num_thresholds` variable\n",
    "  controls the degree of discretization with larger numbers of thresholds more\n",
    "  closely approximating the true AUC. The quality of the approximation may vary\n",
    "  dramatically depending on `num_thresholds`. The `thresholds` parameter can be\n",
    "  used to manually specify thresholds which split the predictions more evenly.\n",
    "  For best results, `predictions` should be distributed approximately uniformly\n",
    "  in the range [0, 1] and not peaked around 0 or 1. The quality of the AUC\n",
    "  approximation may be poor if this is not the case. Setting `summation_method`\n",
    "  to 'minoring' or 'majoring' can help quantify the error in the approximation\n",
    "  by providing lower or upper bound estimate of the AUC.\n",
    "  If `sample_weight` is `None`, weights default to 1.\n",
    "  Use `sample_weight` of 0 to mask values.\n",
    "  Usage:\n",
    "  >>> m = tf.keras.metrics.AUC(num_thresholds=3)\n",
    "  >>> _ = m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n",
    "  >>> # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n",
    "  >>> # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n",
    "  >>> # recall = [1, 0.5, 0], fp_rate = [1, 0, 0]\n",
    "  >>> # auc = ((((1+0.5)/2)*(1-0))+ (((0.5+0)/2)*(0-0))) = 0.75\n",
    "  >>> m.result().numpy()\n",
    "  0.75\n",
    "  >>> m.reset_states()\n",
    "  >>> _ = m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n",
    "  ...                    sample_weight=[1, 0, 0, 1])\n",
    "  >>> m.result().numpy()\n",
    "  1.0\n",
    "  Usage with tf.keras API:\n",
    "  ```python\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.AUC()])\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_thresholds=200,\n",
    "               curve='ROC',\n",
    "               summation_method='interpolation',\n",
    "               name=None,\n",
    "               dtype=None,\n",
    "               thresholds=None,\n",
    "               multi_label=False,\n",
    "               label_weights=None):\n",
    "    \"\"\"Creates an `AUC` instance.\n",
    "    Args:\n",
    "      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n",
    "        use when discretizing the roc curve. Values must be > 1.\n",
    "      curve: (Optional) Specifies the name of the curve to be computed, 'ROC'\n",
    "        [default] or 'PR' for the Precision-Recall-curve.\n",
    "      summation_method: (Optional) Specifies the Riemann summation method used\n",
    "        (https://en.wikipedia.org/wiki/Riemann_sum): 'interpolation' [default],\n",
    "          applies mid-point summation scheme for `ROC`. For PR-AUC, interpolates\n",
    "          (true/false) positives but not the ratio that is precision (see Davis\n",
    "          & Goadrich 2006 for details); 'minoring' that applies left summation\n",
    "          for increasing intervals and right summation for decreasing intervals;\n",
    "          'majoring' that does the opposite.\n",
    "      name: (Optional) string name of the metric instance.\n",
    "      dtype: (Optional) data type of the metric result.\n",
    "      thresholds: (Optional) A list of floating point values to use as the\n",
    "        thresholds for discretizing the curve. If set, the `num_thresholds`\n",
    "        parameter is ignored. Values should be in [0, 1]. Endpoint thresholds\n",
    "        equal to {-epsilon, 1+epsilon} for a small positive epsilon value will\n",
    "        be automatically included with these to correctly handle predictions\n",
    "        equal to exactly 0 or 1.\n",
    "      multi_label: boolean indicating whether multilabel data should be\n",
    "        treated as such, wherein AUC is computed separately for each label and\n",
    "        then averaged across labels, or (when False) if the data should be\n",
    "        flattened into a single label before AUC computation. In the latter\n",
    "        case, when multilabel data is passed to AUC, each label-prediction pair\n",
    "        is treated as an individual data point. Should be set to False for\n",
    "        multi-class data.\n",
    "      label_weights: (optional) list, array, or tensor of non-negative weights\n",
    "        used to compute AUCs for multilabel data. When `multi_label` is True,\n",
    "        the weights are applied to the individual label AUCs when they are\n",
    "        averaged to produce the multi-label AUC. When it's False, they are used\n",
    "        to weight the individual label predictions in computing the confusion\n",
    "        matrix on the flattened data. Note that this is unlike class_weights in\n",
    "        that class_weights weights the example depending on the value of its\n",
    "        label, whereas label_weights depends only on the index of that label\n",
    "        before flattening; therefore `label_weights` should not be used for\n",
    "        multi-class data.\n",
    "    \"\"\"\n",
    "    # Validate configurations.\n",
    "    if isinstance(curve, metrics_utils.AUCCurve) and curve not in list(\n",
    "        metrics_utils.AUCCurve):\n",
    "      raise ValueError('Invalid curve: \"{}\". Valid options are: \"{}\"'.format(\n",
    "          curve, list(metrics_utils.AUCCurve)))\n",
    "    if isinstance(\n",
    "        summation_method,\n",
    "        metrics_utils.AUCSummationMethod) and summation_method not in list(\n",
    "            metrics_utils.AUCSummationMethod):\n",
    "      raise ValueError(\n",
    "          'Invalid summation method: \"{}\". Valid options are: \"{}\"'.format(\n",
    "              summation_method, list(metrics_utils.AUCSummationMethod)))\n",
    "\n",
    "    # Update properties.\n",
    "    if thresholds is not None:\n",
    "      # If specified, use the supplied thresholds.\n",
    "      self.num_thresholds = len(thresholds) + 2\n",
    "      thresholds = sorted(thresholds)\n",
    "    else:\n",
    "      if num_thresholds <= 1:\n",
    "        raise ValueError('`num_thresholds` must be > 1.')\n",
    "\n",
    "      # Otherwise, linearly interpolate (num_thresholds - 2) thresholds in\n",
    "      # (0, 1).\n",
    "      self.num_thresholds = num_thresholds\n",
    "      thresholds = [(i + 1) * 1.0 / (num_thresholds - 1)\n",
    "                    for i in range(num_thresholds - 2)]\n",
    "\n",
    "    # Add an endpoint \"threshold\" below zero and above one for either\n",
    "    # threshold method to account for floating point imprecisions.\n",
    "    self.thresholds = [0.0 - K.epsilon()] + thresholds + [1.0 + K.epsilon()]\n",
    "\n",
    "    if isinstance(curve, metrics_utils.AUCCurve):\n",
    "      self.curve = curve\n",
    "    else:\n",
    "      self.curve = metrics_utils.AUCCurve.from_str(curve)\n",
    "    if isinstance(summation_method, metrics_utils.AUCSummationMethod):\n",
    "      self.summation_method = summation_method\n",
    "    else:\n",
    "      self.summation_method = metrics_utils.AUCSummationMethod.from_str(\n",
    "          summation_method)\n",
    "    super(AUC, self).__init__(name=name, dtype=dtype)\n",
    "\n",
    "    # Handle multilabel arguments.\n",
    "    self.multi_label = multi_label\n",
    "    if label_weights is not None:\n",
    "      label_weights = constant_op.constant(label_weights, dtype=self.dtype)\n",
    "      checks = [\n",
    "          check_ops.assert_non_negative(\n",
    "              label_weights,\n",
    "              message='All values of `label_weights` must be non-negative.')\n",
    "      ]\n",
    "      self.label_weights = control_flow_ops.with_dependencies(\n",
    "          checks, label_weights)\n",
    "\n",
    "    else:\n",
    "      self.label_weights = None\n",
    "\n",
    "    self._built = False\n",
    "    if self.multi_label:\n",
    "      self._num_labels = None\n",
    "    else:\n",
    "      self._build(None)\n",
    "\n",
    "  def _build(self, shape):\n",
    "    \"\"\"Initialize TP, FP, TN, and FN tensors, given the shape of the data.\"\"\"\n",
    "    if self.multi_label:\n",
    "      if shape.ndims != 2:\n",
    "        raise ValueError('`y_true` must have rank=2 when `multi_label` is '\n",
    "                         'True. Found rank %s.' % shape.ndims)\n",
    "      self._num_labels = shape[1]\n",
    "      variable_shape = tensor_shape.TensorShape(\n",
    "          [tensor_shape.Dimension(self.num_thresholds), self._num_labels])\n",
    "\n",
    "    else:\n",
    "      variable_shape = tensor_shape.TensorShape(\n",
    "          [tensor_shape.Dimension(self.num_thresholds)])\n",
    "    self._build_input_shape = shape\n",
    "    # Create metric variables\n",
    "    self.true_positives = self.add_weight(\n",
    "        'true_positives',\n",
    "        shape=variable_shape,\n",
    "        initializer=init_ops.zeros_initializer)\n",
    "    self.true_negatives = self.add_weight(\n",
    "        'true_negatives',\n",
    "        shape=variable_shape,\n",
    "        initializer=init_ops.zeros_initializer)\n",
    "    self.false_positives = self.add_weight(\n",
    "        'false_positives',\n",
    "        shape=variable_shape,\n",
    "        initializer=init_ops.zeros_initializer)\n",
    "    self.false_negatives = self.add_weight(\n",
    "        'false_negatives',\n",
    "        shape=variable_shape,\n",
    "        initializer=init_ops.zeros_initializer)\n",
    "\n",
    "    if self.multi_label:\n",
    "      with ops.init_scope():\n",
    "        # This should only be necessary for handling v1 behavior. In v2, AUC\n",
    "        # should be initialized outside of any tf.functions, and therefore in\n",
    "        # eager mode.\n",
    "        if not context.executing_eagerly():\n",
    "          K._initialize_variables(K._get_session())  # pylint: disable=protected-access\n",
    "\n",
    "    self._built = True\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    \"\"\"Accumulates confusion matrix statistics.\n",
    "    Args:\n",
    "      y_true: The ground truth values.\n",
    "      y_pred: The predicted values.\n",
    "      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
    "        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
    "        be broadcastable to `y_true`.\n",
    "    Returns:\n",
    "      Update op.\n",
    "    \"\"\"\n",
    "    deps = []\n",
    "    if not self._built:\n",
    "      self._build(tensor_shape.TensorShape(y_pred.shape))\n",
    "\n",
    "    if self.multi_label or (self.label_weights is not None):\n",
    "      # y_true should have shape (number of examples, number of labels).\n",
    "      shapes = [\n",
    "          (y_true, ('N', 'L'))\n",
    "      ]\n",
    "      if self.multi_label:\n",
    "        # TP, TN, FP, and FN should all have shape\n",
    "        # (number of thresholds, number of labels).\n",
    "        shapes.extend([(self.true_positives, ('T', 'L')),\n",
    "                       (self.true_negatives, ('T', 'L')),\n",
    "                       (self.false_positives, ('T', 'L')),\n",
    "                       (self.false_negatives, ('T', 'L'))])\n",
    "      if self.label_weights is not None:\n",
    "        # label_weights should be of length equal to the number of labels.\n",
    "        shapes.append((self.label_weights, ('L',)))\n",
    "      deps = [\n",
    "          check_ops.assert_shapes(\n",
    "              shapes, message='Number of labels is not consistent.')\n",
    "      ]\n",
    "\n",
    "    # Only forward label_weights to update_confusion_matrix_variables when\n",
    "    # multi_label is False. Otherwise the averaging of individual label AUCs is\n",
    "    # handled in AUC.result\n",
    "    label_weights = None if self.multi_label else self.label_weights\n",
    "    metrics_utils.update_confusion_matrix_variables(\n",
    "          {\n",
    "              metrics_utils.ConfusionMatrix.TRUE_POSITIVES:\n",
    "                  self.true_positives,\n",
    "              metrics_utils.ConfusionMatrix.TRUE_NEGATIVES:\n",
    "                  self.true_negatives,\n",
    "              metrics_utils.ConfusionMatrix.FALSE_POSITIVES:\n",
    "                  self.false_positives,\n",
    "              metrics_utils.ConfusionMatrix.FALSE_NEGATIVES:\n",
    "                  self.false_negatives,\n",
    "          },\n",
    "          y_true,\n",
    "          y_pred,\n",
    "          self.thresholds,\n",
    "          sample_weight=sample_weight,\n",
    "          multi_label=self.multi_label,\n",
    "          label_weights=label_weights)\n",
    "\n",
    "  def interpolate_pr_auc(self):\n",
    "    \"\"\"Interpolation formula inspired by section 4 of Davis & Goadrich 2006.\n",
    "    https://www.biostat.wisc.edu/~page/rocpr.pdf\n",
    "    Note here we derive & use a closed formula not present in the paper\n",
    "    as follows:\n",
    "      Precision = TP / (TP + FP) = TP / P\n",
    "    Modeling all of TP (true positive), FP (false positive) and their sum\n",
    "    P = TP + FP (predicted positive) as varying linearly within each interval\n",
    "    [A, B] between successive thresholds, we get\n",
    "      Precision slope = dTP / dP\n",
    "                      = (TP_B - TP_A) / (P_B - P_A)\n",
    "                      = (TP - TP_A) / (P - P_A)\n",
    "      Precision = (TP_A + slope * (P - P_A)) / P\n",
    "    The area within the interval is (slope / total_pos_weight) times\n",
    "      int_A^B{Precision.dP} = int_A^B{(TP_A + slope * (P - P_A)) * dP / P}\n",
    "      int_A^B{Precision.dP} = int_A^B{slope * dP + intercept * dP / P}\n",
    "    where intercept = TP_A - slope * P_A = TP_B - slope * P_B, resulting in\n",
    "      int_A^B{Precision.dP} = TP_B - TP_A + intercept * log(P_B / P_A)\n",
    "    Bringing back the factor (slope / total_pos_weight) we'd put aside, we get\n",
    "      slope * [dTP + intercept *  log(P_B / P_A)] / total_pos_weight\n",
    "    where dTP == TP_B - TP_A.\n",
    "    Note that when P_A == 0 the above calculation simplifies into\n",
    "      int_A^B{Precision.dTP} = int_A^B{slope * dTP} = slope * (TP_B - TP_A)\n",
    "    which is really equivalent to imputing constant precision throughout the\n",
    "    first bucket having >0 true positives.\n",
    "    Returns:\n",
    "      pr_auc: an approximation of the area under the P-R curve.\n",
    "    \"\"\"\n",
    "    dtp = self.true_positives[:self.num_thresholds -\n",
    "                              1] - self.true_positives[1:]\n",
    "    p = self.true_positives + self.false_positives\n",
    "    dp = p[:self.num_thresholds - 1] - p[1:]\n",
    "    prec_slope = math_ops.div_no_nan(\n",
    "        dtp, math_ops.maximum(dp, 0), name='prec_slope')\n",
    "    intercept = self.true_positives[1:] - math_ops.multiply(prec_slope, p[1:])\n",
    "\n",
    "    safe_p_ratio = array_ops.where(\n",
    "        math_ops.logical_and(p[:self.num_thresholds - 1] > 0, p[1:] > 0),\n",
    "        math_ops.div_no_nan(\n",
    "            p[:self.num_thresholds - 1],\n",
    "            math_ops.maximum(p[1:], 0),\n",
    "            name='recall_relative_ratio'),\n",
    "        array_ops.ones_like(p[1:]))\n",
    "\n",
    "    pr_auc_increment = math_ops.div_no_nan(\n",
    "        prec_slope * (dtp + intercept * math_ops.log(safe_p_ratio)),\n",
    "        math_ops.maximum(self.true_positives[1:] + self.false_negatives[1:], 0),\n",
    "        name='pr_auc_increment')\n",
    "\n",
    "    if self.multi_label:\n",
    "      by_label_auc = math_ops.reduce_sum(\n",
    "          pr_auc_increment, name=self.name + '_by_label', axis=0)\n",
    "      if self.label_weights is None:\n",
    "        # Evenly weighted average of the label AUCs.\n",
    "        return math_ops.reduce_mean(by_label_auc, name=self.name)\n",
    "      else:\n",
    "        # Weighted average of the label AUCs.\n",
    "        return math_ops.div_no_nan(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.multiply(by_label_auc, self.label_weights)),\n",
    "            math_ops.reduce_sum(self.label_weights),\n",
    "            name=self.name)\n",
    "    else:\n",
    "      return math_ops.reduce_sum(pr_auc_increment, name='interpolate_pr_auc')\n",
    "\n",
    "  def result(self):\n",
    "    if (self.curve == metrics_utils.AUCCurve.PR and\n",
    "        self.summation_method == metrics_utils.AUCSummationMethod.INTERPOLATION\n",
    "       ):\n",
    "      # This use case is different and is handled separately.\n",
    "      return self.interpolate_pr_auc()\n",
    "\n",
    "    # Set `x` and `y` values for the curves based on `curve` config.\n",
    "    recall = math_ops.div_no_nan(self.true_positives,\n",
    "                                 self.true_positives + self.false_negatives)\n",
    "    if self.curve == metrics_utils.AUCCurve.ROC:\n",
    "      fp_rate = math_ops.div_no_nan(self.false_positives,\n",
    "                                    self.false_positives + self.true_negatives)\n",
    "      x = fp_rate\n",
    "      y = recall\n",
    "    else:  # curve == 'PR'.\n",
    "      precision = math_ops.div_no_nan(\n",
    "          self.true_positives, self.true_positives + self.false_positives)\n",
    "      x = recall\n",
    "      y = precision\n",
    "\n",
    "    # Find the rectangle heights based on `summation_method`.\n",
    "    if self.summation_method == metrics_utils.AUCSummationMethod.INTERPOLATION:\n",
    "      # Note: the case ('PR', 'interpolation') has been handled above.\n",
    "      heights = (y[:self.num_thresholds - 1] + y[1:]) / 2.\n",
    "    elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING:\n",
    "      heights = math_ops.minimum(y[:self.num_thresholds - 1], y[1:])\n",
    "    else:  # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:\n",
    "      heights = math_ops.maximum(y[:self.num_thresholds - 1], y[1:])\n",
    "\n",
    "    # Sum up the areas of all the rectangles.\n",
    "    if self.multi_label:\n",
    "      riemann_terms = math_ops.multiply(x[:self.num_thresholds - 1] - x[1:],\n",
    "                                        heights)\n",
    "      by_label_auc = math_ops.reduce_sum(\n",
    "          riemann_terms, name=self.name + '_by_label', axis=0)\n",
    "\n",
    "      if self.label_weights is None:\n",
    "        # Unweighted average of the label AUCs.\n",
    "        return math_ops.reduce_mean(by_label_auc, name=self.name)\n",
    "      else:\n",
    "        # Weighted average of the label AUCs.\n",
    "        return math_ops.div_no_nan(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.multiply(by_label_auc, self.label_weights)),\n",
    "            math_ops.reduce_sum(self.label_weights),\n",
    "            name=self.name)\n",
    "    else:\n",
    "      return math_ops.reduce_sum(\n",
    "          math_ops.multiply(x[:self.num_thresholds - 1] - x[1:], heights),\n",
    "          name=self.name)\n",
    "\n",
    "  def reset_states(self):\n",
    "    if self.multi_label:\n",
    "      K.batch_set_value([(v, np.zeros((self.num_thresholds, self._num_labels)))\n",
    "                         for v in self.variables])\n",
    "    else:\n",
    "      K.batch_set_value([\n",
    "          (v, np.zeros((self.num_thresholds,))) for v in self.variables\n",
    "      ])\n",
    "\n",
    "  def get_config(self):\n",
    "    if is_tensor_or_variable(self.label_weights):\n",
    "      label_weights = K.eval(self.label_weights)\n",
    "    else:\n",
    "      label_weights = self.label_weights\n",
    "    config = {\n",
    "        'num_thresholds': self.num_thresholds,\n",
    "        'curve': self.curve.value,\n",
    "        'summation_method': self.summation_method.value,\n",
    "        # We remove the endpoint thresholds as an inverse of how the thresholds\n",
    "        # were initialized. This ensures that a metric initialized from this\n",
    "        # config has the same thresholds.\n",
    "        'thresholds': self.thresholds[1:-1],\n",
    "        'multi_label': self.multi_label,\n",
    "        'label_weights': label_weights\n",
    "    }\n",
    "    base_config = super(AUC, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "           AUC(name='auc', multi_label=True, label_weights=list(class_weights.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = Input(shape=(1,), dtype=tf.string, name='Question')\n",
    "use = hub.KerasLayer(path_to_model, trainable=True, name='USE')\n",
    "#encode questions\n",
    "q_emb = use(tf.squeeze(tf.cast(question, tf.string)))\n",
    "#dense layer\n",
    "dropout = tf.keras.layers.Dropout(0.2)(q_emb)\n",
    "preds = Dense(total, activation='softmax', name='Dense')(dropout)\n",
    "model = Model(inputs=question, outputs=preds)\n",
    "model.compile(loss=loss_fn, optimizer=opt, run_eagerly=True, metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nQuestion (InputLayer)        [(None, 1)]               0         \n_________________________________________________________________\ntf_op_layer_Squeeze (TensorF [None]                    0         \n_________________________________________________________________\nUSE (KerasLayer)             (None, 512)               68927232  \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\nDense (Dense)                (None, 249)               127737    \n=================================================================\nTotal params: 69,054,969\nTrainable params: 69,054,969\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим, что без обучение выходит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "79659/79659 [==============================] - 348s 4ms/sample - loss: 5.5318 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 19994408.0000 - fn: 79659.0000 - accuracy: 0.0025 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.4848\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[5.531824594546119,\n 0.0,\n 0.0,\n 19994408.0,\n 79659.0,\n 0.0024981482,\n 0.0,\n 0.0,\n 0.48478246]"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nTrain on 238973 samples, validate on 79658 samples\nEpoch 1/20\nWARNING:tensorflow:From /home/satellite/.ODS_QA/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n238592/238973 [============================>.] - ETA: 5s - loss: 0.8637 - accuracy: 0.0085 - auc: 0.5000\nEpoch 00001: val_auc improved from -inf to 0.46601, saving model to models/USE_classif.h5\n238973/238973 [==============================] - 3627s 15ms/sample - loss: 0.8638 - accuracy: 0.0085 - auc: 0.5000 - val_loss: 0.8581 - val_accuracy: 0.0149 - val_auc: 0.4660\nEpoch 2/20\n  7168/238973 [..............................] - ETA: 53:55 - loss: 0.8164 - accuracy: 0.0128 - auc: 0.0976"
    }
   ],
   "source": [
    "train_hist = model.fit(X_train, y_train,\n",
    "                       batch_size=512,\n",
    "                       epochs=20,\n",
    "                       validation_data=(X_val, y_val),\n",
    "                       callbacks=[early_stopping, checkpoint, reduse_lr],\n",
    "                       class_weight=class_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ODS-QA",
   "language": "python",
   "name": "ods-qa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}