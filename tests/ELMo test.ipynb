{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"models/ELMo/options.json\"\n",
    "weight_file = \"models/ELMo/model.hdf5\"\n",
    "\n",
    "# Compute two different representation for each token.\n",
    "# Each representation is a linear weighted combination for the\n",
    "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n",
    "elmo = Elmo(options_file, weight_file, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use batch_to_ids to convert sentences to character ids\n",
    "sentences = [\"Привет, мир!\", \"Привет, мир!\"]\n",
    "character_ids = batch_to_ids(sentences)\n",
    "\n",
    "embeddings = elmo(character_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[tensor([[[-0.3252,  0.1442, -0.0000,  ..., -0.0000, -0.0624,  0.1152],\n          [-0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n          [-0.0000, -0.4533, -1.5221,  ..., -0.0000,  0.0000, -0.0000],\n          ...,\n          [-1.0777,  0.0000, -1.7911,  ..., -0.1676,  0.5619, -0.5261],\n          [-0.0000,  0.0000, -2.1429,  ...,  0.0000,  0.0000, -0.2931],\n          [-0.6248,  0.6789, -1.4321,  ..., -0.0000,  0.0000, -0.4447]],\n \n         [[-0.0000,  0.1442, -0.2452,  ..., -0.0000, -0.0624,  0.0000],\n          [-0.0000,  0.9385, -1.2484,  ..., -0.8439,  0.0000,  0.3235],\n          [-0.0000, -0.4533, -0.0000,  ..., -0.0000,  0.0000, -0.1018],\n          ...,\n          [-0.0000,  0.3273, -0.0000,  ..., -0.1676,  0.5619, -0.5261],\n          [-0.0000,  1.1391, -0.0000,  ...,  0.0000,  0.0000, -0.2931],\n          [-0.6248,  0.6789, -1.4321,  ..., -0.7854,  0.0000, -0.0000]]],\n        grad_fn=<MulBackward0>),\n tensor([[[-0.0000,  0.1442, -0.2452,  ..., -0.9485, -0.0624,  0.0000],\n          [-0.0000,  0.9385, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n          [-0.8669, -0.0000, -1.5221,  ..., -0.0000,  0.0519, -0.1018],\n          ...,\n          [-1.0777,  0.0000, -1.7911,  ..., -0.1676,  0.0000, -0.0000],\n          [-0.0000,  0.0000, -0.0000,  ...,  0.1275,  0.4512, -0.0000],\n          [-0.6248,  0.0000, -1.4321,  ..., -0.0000,  0.2342, -0.4447]],\n \n         [[-0.0000,  0.1442, -0.2452,  ..., -0.0000, -0.0624,  0.1152],\n          [-0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.3235],\n          [-0.0000, -0.0000, -1.5221,  ..., -0.4313,  0.0519, -0.0000],\n          ...,\n          [-1.0777,  0.3273, -0.0000,  ..., -0.1676,  0.0000, -0.5261],\n          [-0.0000,  0.0000, -2.1429,  ...,  0.1275,  0.0000, -0.0000],\n          [-0.0000,  0.0000, -0.0000,  ..., -0.7854,  0.2342, -0.4447]]],\n        grad_fn=<MulBackward0>)]"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "embeddings['elmo_representations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_intro = \"\"\"\n",
    "Extensive experiments demonstrate that ELMo representations work extremely well in practice.\n",
    "We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis.\n",
    "The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions.\n",
    "For tasks where direct comparisons are possible, ELMo outperforms CoVe (McCann et al., 2017), which computes contextualized representations using a neural machine translation encoder.\n",
    "Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM.\n",
    "Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "898"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "len(elmo_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 898, 50])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "batch_to_ids([elmo_intro]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetClass(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for smart batching, that is each batch is only padded to its longest sequence instead of padding all\n",
    "    sequences to the max length.\n",
    "    The SentenceBertEncoder.smart_batching_collate is required for this to work.\n",
    "    SmartBatchingDataset does *not* work without it.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: np.array, target: np.array):\n",
    "        \"\"\"\n",
    "        Create a new SentencesDataset with the tokenized texts and the labels as Tensor\n",
    "        \"\"\"\n",
    "        self.tokenizer = batch_to_ids\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model\n",
    "        :param batch:\n",
    "            a batch from a SmartBatchingDataset\n",
    "        :return:\n",
    "            a batch of tensors for the model\n",
    "        \"\"\"\n",
    "        features, labels = [], []\n",
    "        for feature, label in batch:\n",
    "            features.append(feature)\n",
    "            labels.append(label)\n",
    "\n",
    "        return self.tokenizer(features), torch.argmax(torch.tensor(labels, dtype=torch.long), dim=1)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.features[item], self.target[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/val.parquet')\n",
    "\n",
    "ds = DatasetClass(features=df['question'].to_numpy(), target=df.drop('question', axis=1).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(ds, collate_fn=ds.collate_fn,\n",
    "                                     batch_size=2, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([2, 481, 1024])\n"
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    #print(batch[0])\n",
    "    #print(batch[1])\n",
    "    print(elmo(batch[0])['elmo_representations'][0].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([2, 139, 1024])"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "elmo(x)['elmo_representations'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1024"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "elmo._elmo_lstm.get_output_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "def get_config(workdir=None, num_classes=None, weigths=0, batch_size=16):\n",
    "    return Namespace(num_epochs = 5,\n",
    "                    batch_size = batch_size,\n",
    "                    dropout_prob=0.1,\n",
    "                    num_classes=num_classes,\n",
    "                    lr=3e-5,\n",
    "                    weigths=torch.tensor(weigths),\n",
    "                    options_file = \"models/ELMo/options.json\",\n",
    "                    weight_file = \"models/ELMo/model.hdf5\",\n",
    "                    train_file_path = workdir+'data/train.parquet',\n",
    "                    val_file_path = workdir+'data/val.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoPooler(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        nn.init.xavier_normal(self.dense.weight)\n",
    "        nn.init.normal_(self.dense.bias, std=0.3)\n",
    "\n",
    "    def forward(self, elmo_repr):\n",
    "        # pass elmo_repr and mean pool all tokens\n",
    "        first_token_tensor = self.pool(elmo_repr.permute(0, 2, 1)).squeeze()\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelClass(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(ModelClass, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        #define model layers\n",
    "        self.elmo = Elmo(hparams.options_file, hparams.weight_file, num_output_representations=1, requires_grad=True, \n",
    "                         do_layer_norm=True)\n",
    "        self.pooler = ElmoPooler(self.elmo._elmo_lstm.get_output_dim())\n",
    "        self.drop = nn.Dropout(hparams.dropout_prob)\n",
    "        self.lin = nn.Linear(self.elmo._elmo_lstm.get_output_dim(), hparams.num_classes)\n",
    "        #define loss, metric and softmax\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=hparams.weigths, ignore_index=0) #ignoring PAD index\n",
    "        self.auc = roc_auc_score\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #get sentence embeddings\n",
    "        embs = self.pooler(self.elmo(input)['elmo_representations'][0])\n",
    "        logits = nn.functional.leaky_relu(self.lin(self.drop(embs)))\n",
    "\n",
    "        print(logits.size())\n",
    "        return logits\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        df_train = pd.read_parquet(self.hparams.train_file_path)\n",
    "        df_val = pd.read_parquet(self.hparams.val_file_path)\n",
    "\n",
    "        self.train_ds = DatasetClass(features=df_train['question'].to_numpy(), target=df_train.drop('question', axis=1).to_numpy())\n",
    "        self.val_ds = DatasetClass(features=df_val['question'].to_numpy(), target=df_val.drop('question', axis=1).to_numpy())\n",
    "\n",
    "        self.num_train_steps = int(len(self.train_ds) / self.hparams.batch_size * self.hparams.num_epochs)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(self.train_ds, collate_fn=self.train_ds.collate_fn,\n",
    "                                             batch_size=self.hparams.batch_size,\n",
    "                                             num_workers=4, shuffle=True)\n",
    "        return loader\n",
    "     \n",
    "    def val_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(self.val_ds, collate_fn=self.train_ds.collate_fn,\n",
    "                                             batch_size=self.hparams.batch_size,\n",
    "                                             num_workers=4)      \n",
    "        return loader\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        \n",
    "        logs = {'train_loss': loss}\n",
    "        \n",
    "        return {'loss': logs['train_loss'], 'log': logs}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "\n",
    "        logs = {'train_epoch_loss': avg_loss}\n",
    "        return {'log': logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        \n",
    "        preds = self(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        \n",
    "        logs = {'val_loss': loss}\n",
    "        self.logger.experiment.log(logs)\n",
    "        \n",
    "        return {'val_loss': logs['val_loss'], 'labels': y, 'preds': preds}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "\n",
    "        label = torch.cat([x['labels'] for x in outputs], dim=0)\n",
    "        preds = torch.cat([x['preds'] for x in outputs], dim=0)\n",
    "        label = np.eye(self.hparams.num_classes, dtype=np.int)[label.cpu().numpy()]\n",
    "        try:\n",
    "          auc = torch.tensor(self.auc(label, self.soft(preds).cpu(), average='macro', multi_class='ovo'))\n",
    "        except ValueError:\n",
    "          auc = torch.tensor(0)\n",
    "\n",
    "        logs = {'val_epoch_loss': avg_loss, 'val_epoch_auc': auc}\n",
    "        return {'val_epoch_auc': logs['val_epoch_auc'], 'log': logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_parameters, lr=self.hparams.lr)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps=0, \n",
    "                                                    num_training_steps=self.num_train_steps)\n",
    "        \n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def predict(self, texts, batch_size=32):\n",
    "        ds = DatasetClass(features=texts, target=np.zeros(len(texts)))\n",
    "        loader = torch.utils.data.DataLoader(ds, collate_fn=ds.collate_fn, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.to(device)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, total=len(loader), desc=f\"Predicting on {device}\"):\n",
    "                #move to proper device\n",
    "                features = batch[0].to(device)\n",
    "\n",
    "                preds.append(self.soft(self(features)))\n",
    "            \n",
    "            preds = torch.cat(preds, dim=0)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = ''\n",
    "if workdir is not '':\n",
    "    workdir += '/'\n",
    "with open(workdir + 'data/label_weights.pkl', 'rb') as f:\n",
    "    class_weights = pickle.load(f)\n",
    "config = get_config(workdir, num_classes=len(class_weights), weigths=list(class_weights.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Namespace(batch_size=16, dropout_prob=0.1, lr=3e-05, num_classes=79, num_epochs=5, options_file='models/ELMo/options.json', train_file_path='data/train.parquet', val_file_path='data/val.parquet', weight_file='models/ELMo/model.hdf5', weigths=tensor([0.0729, 0.0486, 0.0148, 0.4136, 0.0020, 0.2257, 0.1276, 0.1310, 0.0147,\n        0.3990, 0.0216, 0.1125, 0.0012, 0.3062, 0.0403, 0.0272, 0.1616, 0.3930,\n        0.0879, 0.0760, 0.0148, 0.3780, 0.0698, 0.0195, 0.0024, 0.1528, 0.0141,\n        0.0601, 0.2734, 0.0551, 0.0408, 0.0119, 0.5603, 0.1197, 0.0234, 0.0055,\n        0.0565, 0.0098, 0.1319, 0.6991, 0.0038, 0.0290, 0.3147, 0.7670, 0.0534,\n        0.0963, 0.0435, 0.0139, 0.0038, 0.0533, 0.0140, 0.5338, 0.1626, 0.0047,\n        0.0502, 0.5852, 0.4463, 0.0985, 0.0124, 0.4202, 0.3198, 0.6639, 0.2005,\n        0.0618, 0.0603, 0.0251, 0.0730, 0.6220, 0.0916, 0.0727, 0.0028, 0.0755,\n        0.0470, 0.2651, 0.0898, 0.0397, 0.0317, 0.0785, 0.5448]))"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='ODS_QA'\n",
    "run_name='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/satellite/Documents/python_workdir/netology_learning/deep-nlp-spring-2020/ODS-QA/wandb/run-20200605_162214-2avyajhq/wandb-history.jsonl'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-ba9beefbddd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                             mode='max')\n\u001b[0;32m---> 10\u001b[0;31m checkpoint_callback = ModelCheckpoint(filepath=wandb_logger.experiment.dir+'/'+run_name+'_{epoch}-{val_epoch_auc:.2f}', \n\u001b[0m\u001b[1;32m     11\u001b[0m                                     \u001b[0msave_top_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                     monitor='val_epoch_auc', mode='max')\n",
      "\u001b[0;32m~/.ODS_QA/lib/python3.7/site-packages/pytorch_lightning/loggers/wandb.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_project\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_anonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mreinit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'allow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 group=self._group)\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;31m# save checkpoints in wandb dir to upload on W&B servers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ODS_QA/lib/python3.7/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, name, notes, id, magic, anonymous, config_exclude_keys, config_include_keys, save_code)\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_global_run_stack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mtermwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"If you want to track multiple runs concurrently in wandb you should use multi-processing not threads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ODS_QA/lib/python3.7/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(exit_code)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_global_run_stack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0m_global_run_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ODS_QA/lib/python3.7/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36mclose_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ODS_QA/lib/python3.7/site-packages/wandb/history.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ODS_QA/lib/python3.7/site-packages/wandb/history.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# for resuming.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/satellite/Documents/python_workdir/netology_learning/deep-nlp-spring-2020/ODS-QA/wandb/run-20200605_162214-2avyajhq/wandb-history.jsonl'"
     ]
    }
   ],
   "source": [
    "model = ModelClass(config)\n",
    "\n",
    "wandb_logger = WandbLogger(project=project_name, name=run_name)\n",
    "stop_patience = 2\n",
    "early_stopping = EarlyStopping(monitor='val_epoch_auc',\n",
    "                            min_delta=0,\n",
    "                            patience=stop_patience,\n",
    "                            verbose=True,\n",
    "                            mode='max')\n",
    "checkpoint_callback = ModelCheckpoint(filepath=wandb_logger.experiment.dir+'/'+run_name+'_{epoch}-{val_epoch_auc:.2f}', \n",
    "                                    save_top_k=1, verbose= True,\n",
    "                                    monitor='val_epoch_auc', mode='max')\n",
    "trainer = Trainer(gpus=1, early_stopping_callback=early_stopping, checkpoint_callback=checkpoint_callback, logger=wandb_logger, \n",
    "                    max_epochs=config.num_epochs, auto_scale_batch_size='binsearch')\n",
    "trainer.fit(model)\n",
    "for file in os.listdir(wandb_logger.experiment.dir):\n",
    "    if file.endswith(\".ckpt\"):\n",
    "        ckpt_path = os.path.join(wandb_logger.experiment.dir, file)\n",
    "model.load_from_checkpoint(ckpt_path)\n",
    "return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "for obj in gc.get_objects():\n",
    "    if torch.is_tensor(obj):\n",
    "        del obj\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "ods-qa",
   "display_name": "ODS-QA"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}